<!DOCTYPE html>
<html lang="en"><head>
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-93c6896557d5f42838538b14f49b9a4e.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Trent Nelson">
  <meta name="dcterms.date" content="2025-11-08">
  <title>Trent Nelson – PyTorch and Python Free-Threading</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #f8f8f2; background-color: #333333; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #f8f8f2; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #ff79c6; } /* Annotation */
    code span.at { color: #ffe4b5; } /* Attribute */
    code span.bn { color: #bd93f9; } /* BaseN */
    code span.bu { color: #dda0dd; } /* BuiltIn */
    code span.cf { color: #ff79c6; } /* ControlFlow */
    code span.ch { color: #f1fa8c; } /* Char */
    code span.cn { color: #bd93f9; font-weight: bold; } /* Constant */
    code span.co { color: #7ec699; } /* Comment */
    code span.cv { color: #8be9fd; } /* CommentVar */
    code span.do { color: #ffb86c; } /* Documentation */
    code span.dt { color: #afeeee; } /* DataType */
    code span.dv { color: #bd93f9; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #b0e0e6; } /* Extension */
    code span.fl { color: #bd93f9; } /* Float */
    code span.fu { color: #87ceeb; } /* Function */
    code span.im { color: #ff79c6; } /* Import */
    code span.in { color: #f1fa8c; } /* Information */
    code span.kw { color: #ff79c6; } /* Keyword */
    code span.op { color: #ffefd5; } /* Operator */
    code span.ot { color: #c4ed54; } /* Other */
    code span.pp { color: #ffe4b5; } /* Preprocessor */
    code span.re { color: #8be9fd; } /* RegionMarker */
    code span.sc { color: #ff79c6; } /* SpecialChar */
    code span.ss { color: #f1fa8c; } /* SpecialString */
    code span.st { color: #db7093; } /* String */
    code span.va { color: #98fb98; } /* Variable */
    code span.vs { color: #ff6347; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-b034ff0ead49eadb7d97b9150cea9e89.css">
  <link rel="stylesheet" href="../../styles.css">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-8RPR4XKPEW"></script>

  <script type="text/javascript">

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-8RPR4XKPEW', { 'anonymize_ip': false});
  </script>
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-bg-video-ctrl/bg-video-ctrl.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="PyTorch and Python Free-Threading – Trent Nelson">
<meta property="og:description" content="Unlocking multi-threaded parallel inference in PyTorch PyData Seattle, November 8, 2025">
<meta property="og:image" content="https://trent.me/decks/pytorch-and-python-free-threading/images/pytorch-and-python-free-threading.png">
<meta property="og:site_name" content="Trent Nelson">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1024">
<meta name="twitter:title" content="PyTorch and Python Free-Threading – Trent Nelson">
<meta name="twitter:description" content="Unlocking multi-threaded parallel inference in PyTorch PyData Seattle, November 8, 2025">
<meta name="twitter:image" content="https://trent.me/decks/pytorch-and-python-free-threading/images/pytorch-and-python-free-threading.png">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1024">
<meta name="twitter:card" content="summary_large_image">
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="../../images/background.jpg" data-background-size="cover" class="quarto-title-block center">
  <h1 class="title">PyTorch and Python Free-Threading</h1>
  <p class="subtitle">Unlocking multi-threaded parallel inference in PyTorch PyData Seattle, November 8, 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Trent Nelson 
</div>
</div>
</div>

  <p class="date">2025-11-08</p>
</section>
<section>
<section id="introduction" class="title-slide slide level1 center">
<h1>Introduction</h1>

</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<ul>
<li><p>PyTorch is a popular deep learning framework widely used for building and deploying machine learning models.</p></li>
<li><p>Python’s Global Interpreter Lock (GIL) can limit the performance of multi-threaded applications, including those using PyTorch.</p></li>
</ul>
</section>
<section id="tips-for-interacting-with-this-deck" class="slide level2">
<h2>Tips For Interacting With This Deck</h2>
<ul>
<li>Intent of this deck is to be useful as a reference you can come back to.</li>
<li><a href="https://trentn.nvidia.com/decks/git-carpentry">https://trentn.nvidia.com/decks/git-carpentry</a>
<ul>
<li><a href="https://gitlab-master.nvidia.com/trentn/website/-/blob/main/decks/git-carpentry/index.qmd">GitLab source for this deck</a></li>
</ul></li>
<li><code>m</code>: Side Menu with TOC</li>
<li><code>o</code>: Overview Mode
<ul>
<li>This deck uses vertical and horizontal orientation, so overview mode is helpful for quickly navigating to the area you’re interested in.</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="pre-requisites" class="title-slide slide level1 center">
<h1>Pre-Requisites</h1>

</section>
<section id="conda-environment" class="slide level2">
<h2>Conda Environment</h2>
<pre><code>conda create -n py314t python=3.14 python-freethreading \
    nodejs pip tqdm flake8 rust requests \
    -c conda-forge</code></pre>
</section>
<section id="docker" class="slide level2">
<h2>Docker</h2>
<pre><code>docker run ...
</code></pre>
</section></section>
<section>
<section id="python-free-threading" class="title-slide slide level1 center">
<h1>Python Free-Threading</h1>

</section>
<section id="what-is-python-free-threading" class="slide level2">
<h2>What is Python Free-Threading?</h2>
<p>Python 3.13, released in October 2024, is the first version of Python to introduce support for a “no-GIL” free-threaded mode, per PEP-703 Making the Global Interpreter Lock Optional in CPython, unlocking the ability for multiple Python threads to run simultaneously.</p>
<p>This allows, for the first time since the language’s inception in December 1989, a single Python process to saturate all CPU cores in parallel with pure Python code (i.e.&nbsp;not farming out to extension modules written in C, C++, or, more recently, Rust).</p>
<ul>
<li><p><a href="https://peps.python.org/pep-0703/">PEP-703 Making the Global Interpreter Lock Optional in CPython</a></p></li>
<li><p>PEP-703: <a href="https://peps.python.org/pep-0703/">https://peps.python.org/pep-0703/</a></p></li>
</ul>
</section>
<section id="installing-python-free-threading" class="slide level2">
<h2>Installing Python Free-Threading</h2>
<ul>
<li>Via Conda:</li>
</ul>
<pre><code>conda create -n py314t python=3.14 python-freethreading -c conda-forge</code></pre>
</section></section>
<section>
<section id="pytorch-llm-crash-course" class="title-slide slide level1 center">
<h1>PyTorch &amp; LLM Crash Course</h1>

</section>
<section id="neural-networks-from-zero-to-hero" class="slide level2">
<h2>Neural Networks: From Zero to Hero</h2>
<ul>
<li>My prior LLM and PyTorch experience: zero.</li>
<li><a href="https://karpathy.ai">Andrej Karpathy</a>’s YouTube Series on deep neural networks and LLMs.</li>
<li>Nearly 20 hours of content across 10 videos.</li>
<li>Took me easily double that to actually absorb it.</li>
</ul>
<aside class="notes">
<p>My involvement with PyTorch and Large Language Models (LLMs) started around late November last year, 2024. Going in, I knew nothing about PyTorch, nor deep neural networks, nor LLMs—other than having enjoyed using LLMs thoroughly the past couple of years. I had never trained an AI model of any kind. I did have a bit of NumPy and data science exposure up my sleeve, plus general familiarity with Python.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside class="notes">
<p>Thanks to <a href="https://karpathy.ai/">Andrej Karpathy</a>’s phenomenal YouTube series on deep neural networks and LLMs titled <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Neural Networks: From Zero to Hero</a>, over the course of about 3 weeks or so I went from zero to… well I wouldn’t necessarily say <em>hero</em>—perhaps zero to <em>not-completley-clueless</em> is more apropos.</p>
<p>Andrej’s content is a fantastic resource to learn everything you need to know to understand how modern LLMs work from the ground-up. It’s not a short series—there are 19 hours, 21 minutes and two seconds of content across ten videos—and you’ll probably spend double that if you <em>really</em> want to properly absorb the content.</p>
<p>None of the work presented in this post would have been possible had I not invested the time in Andrej’s series. If you’re reading this Andrej, thanks, and keep up the brilliant work!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="training-gpt-2-124m-locally" class="slide level2">
<h2>Training GPT-2 (124M) Locally</h2>
<p>Equipped with my new knowledge about LLMs, PyTorch, and, thanks to Andrej’s final video in the series titled <a href="https://www.youtube.com/watch?v=l8pRSuU81PU&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=10&amp;t=1286s&amp;pp=iAQB">Let’s reproduce GPT-2 (124M)</a> and the accompanying <a href="https://github.com/karpathy/build-nanogpt">build-nanogpt</a> Github repo, I was able to train a local GPT-2 model via PyTorch, from scratch, using the <a href="https://huggingface.co/rhysjones/gpt2-124M-edu-fineweb-10B">edu_fineweb10B</a> dataset.</p>
</section></section>
<section>
<section id="pytorch-gpt-2-implementation" class="title-slide slide level1 center">
<h1>PyTorch GPT-2 Implementation</h1>

</section>
<section id="initial-implementation" class="slide level2">
<h2>Initial Implementation</h2>
<p>The code below roughly corresponds to my first version of the code in the commit <a href="https://github.com/tpn/parallelopedia/blob/3ed4fe60a767a12b31fca183fed00fef43c65827/src/parallelopedia/gpt2.py">3ed4fe6: Add gpt2.py</a>, with some formatting and style tweaks to ensure the code is viewable on mobile devices without requiring horizontal scrolling.</p>
<p>We’ll revise this code later in the post, but for now, it’s a good starting point to get a feel for how we can use PyTorch to load a GPT-2 model checkpoint, tokenize some input text, and generate some output text.</p>
</section>
<section id="code" class="slide level2">
<h2>Code</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/trent/src/website/decks/pytorch-and-python-free-threading/gpt2-v1.ipynb" data-notebook-title="gpt2_v1.ipynb" data-notebook-cellid="cell-gpt2-v1-setup">
<div id="gpt2-v1-setup" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="co"># gpt2_v1.ipynb</span></span>
<span id="cb4-2"><a></a></span>
<span id="cb4-3"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-4"><a></a><span class="co"># Imports</span></span>
<span id="cb4-5"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-6"><a></a><span class="im">import</span> dataclasses</span>
<span id="cb4-7"><a></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb4-8"><a></a><span class="im">import</span> logging</span>
<span id="cb4-9"><a></a><span class="im">import</span> os</span>
<span id="cb4-10"><a></a><span class="im">from</span> os.path <span class="im">import</span> join, exists</span>
<span id="cb4-11"><a></a><span class="im">import</span> subprocess</span>
<span id="cb4-12"><a></a><span class="im">import</span> sys</span>
<span id="cb4-13"><a></a><span class="im">import</span> textwrap</span>
<span id="cb4-14"><a></a><span class="im">from</span> textwrap <span class="im">import</span> wrap</span>
<span id="cb4-15"><a></a><span class="im">import</span> time</span>
<span id="cb4-16"><a></a></span>
<span id="cb4-17"><a></a><span class="im">import</span> tiktoken</span>
<span id="cb4-18"><a></a><span class="im">import</span> torch</span>
<span id="cb4-19"><a></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-20"><a></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb4-21"><a></a></span>
<span id="cb4-22"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-23"><a></a><span class="co"># Helper Timer Class</span></span>
<span id="cb4-24"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-25"><a></a><span class="kw">class</span> ElapsedTimer:</span>
<span id="cb4-26"><a></a>    <span class="co">"""</span></span>
<span id="cb4-27"><a></a><span class="co">    Context manager and reusable timer to measure elapsed time.</span></span>
<span id="cb4-28"><a></a></span>
<span id="cb4-29"><a></a><span class="co">    Example:</span></span>
<span id="cb4-30"><a></a><span class="co">        timer = elapsed_timer()</span></span>
<span id="cb4-31"><a></a><span class="co">        with timer:</span></span>
<span id="cb4-32"><a></a><span class="co">            do_something()</span></span>
<span id="cb4-33"><a></a><span class="co">        print(f'Elapsed: {timer.elapsed:.3f}')</span></span>
<span id="cb4-34"><a></a></span>
<span id="cb4-35"><a></a><span class="co">        # Re-enterable:</span></span>
<span id="cb4-36"><a></a><span class="co">        with timer:</span></span>
<span id="cb4-37"><a></a><span class="co">            do_something_else()</span></span>
<span id="cb4-38"><a></a><span class="co">        print(f'Elapsed: {timer.elapsed:.3f}')</span></span>
<span id="cb4-39"><a></a><span class="co">    """</span></span>
<span id="cb4-40"><a></a></span>
<span id="cb4-41"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-42"><a></a>        <span class="va">self</span>.start <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-43"><a></a>        <span class="va">self</span>._elapsed <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-44"><a></a></span>
<span id="cb4-45"><a></a>    <span class="kw">def</span> <span class="fu">__enter__</span>(<span class="va">self</span>):</span>
<span id="cb4-46"><a></a>        <span class="va">self</span>.start <span class="op">=</span> time.perf_counter()</span>
<span id="cb4-47"><a></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb4-48"><a></a></span>
<span id="cb4-49"><a></a>    <span class="kw">def</span> <span class="fu">__exit__</span>(<span class="va">self</span>, exc_type, exc_value, traceback):</span>
<span id="cb4-50"><a></a>        <span class="va">self</span>._elapsed <span class="op">=</span> time.perf_counter() <span class="op">-</span> <span class="va">self</span>.start</span>
<span id="cb4-51"><a></a></span>
<span id="cb4-52"><a></a>    <span class="at">@property</span></span>
<span id="cb4-53"><a></a>    <span class="kw">def</span> elapsed(<span class="va">self</span>):</span>
<span id="cb4-54"><a></a>        <span class="co">"""</span></span>
<span id="cb4-55"><a></a><span class="co">        Return the elapsed time for the most recent context.</span></span>
<span id="cb4-56"><a></a><span class="co">        """</span></span>
<span id="cb4-57"><a></a>        <span class="cf">if</span> <span class="va">self</span>._elapsed <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-58"><a></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Timer has not been used in a context yet."</span>)</span>
<span id="cb4-59"><a></a>        <span class="cf">return</span> <span class="va">self</span>._elapsed</span>
<span id="cb4-60"><a></a></span>
<span id="cb4-61"><a></a></span>
<span id="cb4-62"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-63"><a></a><span class="co"># Globals</span></span>
<span id="cb4-64"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-65"><a></a></span>
<span id="cb4-66"><a></a>LOG_LEVEL <span class="op">=</span> <span class="st">'DEBUG'</span></span>
<span id="cb4-67"><a></a>PARALLELOPEDIA_ROOT <span class="op">=</span> os.environ[<span class="st">'PARALLELOPEDIA_ROOT'</span>]</span>
<span id="cb4-68"><a></a>PARALLELOPEDIA_DATA_DIR <span class="op">=</span> join(PARALLELOPEDIA_ROOT, <span class="st">'data'</span>)</span>
<span id="cb4-69"><a></a>MODEL_CHECKPOINT <span class="op">=</span> join(</span>
<span id="cb4-70"><a></a>    PARALLELOPEDIA_DATA_DIR,</span>
<span id="cb4-71"><a></a>    <span class="st">'model_19072.pt'</span>,</span>
<span id="cb4-72"><a></a>)</span>
<span id="cb4-73"><a></a>MODEL_DOWNLOAD_URL <span class="op">=</span> (</span>
<span id="cb4-74"><a></a>    <span class="st">"https://huggingface.co/datasets/trentnelson/"</span></span>
<span id="cb4-75"><a></a>    <span class="st">"parallelopedia-data-gpt2/resolve/main/model_19072.pt"</span></span>
<span id="cb4-76"><a></a>)</span>
<span id="cb4-77"><a></a></span>
<span id="cb4-78"><a></a><span class="co"># Download the model from huggingface if necessary.</span></span>
<span id="cb4-79"><a></a>os.makedirs(PARALLELOPEDIA_DATA_DIR, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-80"><a></a></span>
<span id="cb4-81"><a></a><span class="cf">if</span> <span class="kw">not</span> exists(MODEL_CHECKPOINT):</span>
<span id="cb4-82"><a></a>    <span class="bu">print</span>(<span class="ss">f'Downloading </span><span class="sc">{</span>MODEL_DOWNLOAD_URL<span class="sc">}</span><span class="ss"> via wget '</span></span>
<span id="cb4-83"><a></a>          <span class="st">'this might take a while...'</span>)</span>
<span id="cb4-84"><a></a>    args <span class="op">=</span> [</span>
<span id="cb4-85"><a></a>        <span class="st">"wget"</span>,</span>
<span id="cb4-86"><a></a>        <span class="st">"--quiet"</span>,</span>
<span id="cb4-87"><a></a>        MODEL_DOWNLOAD_URL,</span>
<span id="cb4-88"><a></a>        <span class="st">"-P"</span>,</span>
<span id="cb4-89"><a></a>        PARALLELOPEDIA_DATA_DIR,</span>
<span id="cb4-90"><a></a>    ]</span>
<span id="cb4-91"><a></a>    timer <span class="op">=</span> ElapsedTimer()</span>
<span id="cb4-92"><a></a>    <span class="cf">with</span> timer:</span>
<span id="cb4-93"><a></a>        subprocess.run(args, check<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-94"><a></a>    <span class="bu">print</span>(<span class="ss">f'Downloaded model in </span><span class="sc">{</span>timer<span class="sc">.</span>elapsed<span class="sc">:.3f}</span><span class="ss"> seconds.'</span>)</span>
<span id="cb4-95"><a></a>    </span>
<span id="cb4-96"><a></a><span class="cf">assert</span> exists(MODEL_CHECKPOINT), <span class="st">"Missing checkpoint."</span></span>
<span id="cb4-97"><a></a></span>
<span id="cb4-98"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-99"><a></a><span class="co"># Logging</span></span>
<span id="cb4-100"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-101"><a></a><span class="co"># N.B. We redirect logs to sys.stdout in order for Quarto to pick</span></span>
<span id="cb4-102"><a></a><span class="co">#      them up and include them in rendering the output.</span></span>
<span id="cb4-103"><a></a>logging.basicConfig(</span>
<span id="cb4-104"><a></a>    level<span class="op">=</span><span class="bu">getattr</span>(logging, LOG_LEVEL),</span>
<span id="cb4-105"><a></a>    <span class="bu">format</span><span class="op">=</span><span class="st">'</span><span class="sc">%(asctime)s</span><span class="st"> - </span><span class="sc">%(levelname)s</span><span class="st"> - </span><span class="sc">%(message)s</span><span class="st">'</span>,</span>
<span id="cb4-106"><a></a>    stream<span class="op">=</span>sys.stdout</span>
<span id="cb4-107"><a></a>)</span>
<span id="cb4-108"><a></a></span>
<span id="cb4-109"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-110"><a></a><span class="co"># Setup</span></span>
<span id="cb4-111"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-112"><a></a></span>
<span id="cb4-113"><a></a><span class="co"># Use bfloat16 for matmul precision where possible.</span></span>
<span id="cb4-114"><a></a>torch.set_float32_matmul_precision(<span class="st">'high'</span>)</span>
<span id="cb4-115"><a></a></span>
<span id="cb4-116"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-117"><a></a><span class="co"># GPT2 PyTorch Model Components</span></span>
<span id="cb4-118"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-119"><a></a></span>
<span id="cb4-120"><a></a><span class="co"># Now define the classes making up our GPT2 implementation.</span></span>
<span id="cb4-121"><a></a><span class="co"># These map directly to the components introduced by the</span></span>
<span id="cb4-122"><a></a><span class="co"># now-seminal 2017 "Attention Is All You Need" paper.</span></span>
<span id="cb4-123"><a></a></span>
<span id="cb4-124"><a></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb4-125"><a></a>    <span class="co">"""</span></span>
<span id="cb4-126"><a></a><span class="co">    Causal self-attention for the GPT2 model.</span></span>
<span id="cb4-127"><a></a><span class="co">    """</span></span>
<span id="cb4-128"><a></a></span>
<span id="cb4-129"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb4-130"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-131"><a></a>        <span class="cf">assert</span> config.n_embd <span class="op">%</span> config.n_head <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb4-132"><a></a>        <span class="co"># Key, query, value projections for all heads, but in a batch.</span></span>
<span id="cb4-133"><a></a>        <span class="va">self</span>.c_attn <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">3</span> <span class="op">*</span> config.n_embd)</span>
<span id="cb4-134"><a></a></span>
<span id="cb4-135"><a></a>        <span class="co"># Output projection.</span></span>
<span id="cb4-136"><a></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(config.n_embd, config.n_embd)</span>
<span id="cb4-137"><a></a>        <span class="va">self</span>.c_proj.NANOGPT_SCALE_INIT <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-138"><a></a></span>
<span id="cb4-139"><a></a>        <span class="co"># Regularization.</span></span>
<span id="cb4-140"><a></a>        <span class="va">self</span>.n_head <span class="op">=</span> config.n_head</span>
<span id="cb4-141"><a></a>        <span class="va">self</span>.n_embd <span class="op">=</span> config.n_embd</span>
<span id="cb4-142"><a></a></span>
<span id="cb4-143"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-144"><a></a>        <span class="co"># Batch size, sequence length, embedding dimensionality.</span></span>
<span id="cb4-145"><a></a>        B, T, C <span class="op">=</span> (x.size())</span>
<span id="cb4-146"><a></a></span>
<span id="cb4-147"><a></a>        <span class="co"># Calculate query, key, values for all heads in</span></span>
<span id="cb4-148"><a></a>        <span class="co"># batch and move head forward to be the batch dim.</span></span>
<span id="cb4-149"><a></a>        <span class="co">#</span></span>
<span id="cb4-150"><a></a>        <span class="co"># N.B. nh is "number of heads", hs is "head size",</span></span>
<span id="cb4-151"><a></a>        <span class="co">#      and C (number of channels) is nh * hs.</span></span>
<span id="cb4-152"><a></a>        <span class="co">#      E.g. in GPT-2 (124M), n_head=12, hs=64, so</span></span>
<span id="cb4-153"><a></a>        <span class="co">#      nh*hs=C=768 channels in the Transformer.</span></span>
<span id="cb4-154"><a></a>        qkv <span class="op">=</span> <span class="va">self</span>.c_attn(x)</span>
<span id="cb4-155"><a></a>        q, k, v <span class="op">=</span> qkv.split(<span class="va">self</span>.n_embd, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-156"><a></a></span>
<span id="cb4-157"><a></a>        head_dim <span class="op">=</span> C <span class="op">//</span> <span class="va">self</span>.n_head</span>
<span id="cb4-158"><a></a></span>
<span id="cb4-159"><a></a>        <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb4-160"><a></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_head, head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-161"><a></a></span>
<span id="cb4-162"><a></a>        <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb4-163"><a></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_head, head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-164"><a></a></span>
<span id="cb4-165"><a></a>        <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb4-166"><a></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_head, head_dim).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-167"><a></a></span>
<span id="cb4-168"><a></a>        <span class="co"># Flash attention.</span></span>
<span id="cb4-169"><a></a>        y <span class="op">=</span> F.scaled_dot_product_attention(q, k, v, is_causal<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-170"><a></a></span>
<span id="cb4-171"><a></a>        <span class="co"># Re-assemble all head outputs side by side.</span></span>
<span id="cb4-172"><a></a>        y <span class="op">=</span> (y.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C))</span>
<span id="cb4-173"><a></a></span>
<span id="cb4-174"><a></a>        <span class="co"># Output projection.</span></span>
<span id="cb4-175"><a></a>        y <span class="op">=</span> <span class="va">self</span>.c_proj(y)</span>
<span id="cb4-176"><a></a>        <span class="cf">return</span> y</span>
<span id="cb4-177"><a></a></span>
<span id="cb4-178"><a></a></span>
<span id="cb4-179"><a></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb4-180"><a></a>    <span class="co">"""</span></span>
<span id="cb4-181"><a></a><span class="co">    Multi-layer perceptron for the GPT2 model.</span></span>
<span id="cb4-182"><a></a><span class="co">    """</span></span>
<span id="cb4-183"><a></a></span>
<span id="cb4-184"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb4-185"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-186"><a></a>        <span class="va">self</span>.c_fc <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">4</span> <span class="op">*</span> config.n_embd)</span>
<span id="cb4-187"><a></a>        <span class="va">self</span>.gelu <span class="op">=</span> nn.GELU(approximate<span class="op">=</span><span class="st">'tanh'</span>)</span>
<span id="cb4-188"><a></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(<span class="dv">4</span> <span class="op">*</span> config.n_embd, config.n_embd)</span>
<span id="cb4-189"><a></a>        <span class="va">self</span>.c_proj.NANOGPT_SCALE_INIT <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-190"><a></a></span>
<span id="cb4-191"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-192"><a></a>        x <span class="op">=</span> <span class="va">self</span>.c_fc(x)</span>
<span id="cb4-193"><a></a>        x <span class="op">=</span> <span class="va">self</span>.gelu(x)</span>
<span id="cb4-194"><a></a>        x <span class="op">=</span> <span class="va">self</span>.c_proj(x)</span>
<span id="cb4-195"><a></a>        <span class="cf">return</span> x</span>
<span id="cb4-196"><a></a></span>
<span id="cb4-197"><a></a></span>
<span id="cb4-198"><a></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb4-199"><a></a>    <span class="co">"""</span></span>
<span id="cb4-200"><a></a><span class="co">    Transformer block for the GPT2 model.</span></span>
<span id="cb4-201"><a></a><span class="co">    """</span></span>
<span id="cb4-202"><a></a></span>
<span id="cb4-203"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb4-204"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-205"><a></a>        <span class="va">self</span>.ln_1 <span class="op">=</span> nn.LayerNorm(config.n_embd)</span>
<span id="cb4-206"><a></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(config)</span>
<span id="cb4-207"><a></a>        <span class="va">self</span>.ln_2 <span class="op">=</span> nn.LayerNorm(config.n_embd)</span>
<span id="cb4-208"><a></a>        <span class="va">self</span>.mlp <span class="op">=</span> MLP(config)</span>
<span id="cb4-209"><a></a></span>
<span id="cb4-210"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-211"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln_1(x))</span>
<span id="cb4-212"><a></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.ln_2(x))</span>
<span id="cb4-213"><a></a>        <span class="cf">return</span> x</span>
<span id="cb4-214"><a></a></span>
<span id="cb4-215"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-216"><a></a><span class="co"># GPT2 Supporting Classes</span></span>
<span id="cb4-217"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-218"><a></a></span>
<span id="cb4-219"><a></a><span class="co"># N.B. These differ slightly from Andrej's classes in</span></span>
<span id="cb4-220"><a></a><span class="co">#      `train_gpt2.py`.  `GPTCheckpoint` is a helper</span></span>
<span id="cb4-221"><a></a><span class="co">#      class I wrote that has no analog in the former.</span></span>
<span id="cb4-222"><a></a></span>
<span id="cb4-223"><a></a><span class="at">@dataclass</span></span>
<span id="cb4-224"><a></a><span class="kw">class</span> GPTConfig:</span>
<span id="cb4-225"><a></a>    <span class="co">"""</span></span>
<span id="cb4-226"><a></a><span class="co">    Configuration class for GPT model.</span></span>
<span id="cb4-227"><a></a></span>
<span id="cb4-228"><a></a><span class="co">    Attributes:</span></span>
<span id="cb4-229"><a></a></span>
<span id="cb4-230"><a></a><span class="co">        block_size (int): Maximum sequence length.</span></span>
<span id="cb4-231"><a></a></span>
<span id="cb4-232"><a></a><span class="co">        vocab_size (int): Number of tokens.  GPT2 from</span></span>
<span id="cb4-233"><a></a><span class="co">            huggingface has a vocab size of 50257, which</span></span>
<span id="cb4-234"><a></a><span class="co">            includes 50,000 BPE merges, 256 byte tokens,</span></span>
<span id="cb4-235"><a></a><span class="co">            and 1 &lt;|endoftext|&gt; token.  However, Andrej</span></span>
<span id="cb4-236"><a></a><span class="co">            Karpathy's `build-nanogpt/train_gpt2.py`</span></span>
<span id="cb4-237"><a></a><span class="co">            uses a vocab size of 50304.  I vaguely recall</span></span>
<span id="cb4-238"><a></a><span class="co">            the explanation for this discrepancy as a local</span></span>
<span id="cb4-239"><a></a><span class="co">            optimization to yield better alignment sizes,</span></span>
<span id="cb4-240"><a></a><span class="co">            but I'm not 100% certain.</span></span>
<span id="cb4-241"><a></a></span>
<span id="cb4-242"><a></a><span class="co">            The local GPT2 training that we did on</span></span>
<span id="cb4-243"><a></a><span class="co">            edu_fineweb10b used 50304, so we will use</span></span>
<span id="cb4-244"><a></a><span class="co">            that here.</span></span>
<span id="cb4-245"><a></a></span>
<span id="cb4-246"><a></a><span class="co">        n_layer (int): Number of layers.</span></span>
<span id="cb4-247"><a></a></span>
<span id="cb4-248"><a></a><span class="co">        n_head (int): Number of attention heads.</span></span>
<span id="cb4-249"><a></a></span>
<span id="cb4-250"><a></a><span class="co">        n_embd (int): Embedding dimension.</span></span>
<span id="cb4-251"><a></a><span class="co">    """</span></span>
<span id="cb4-252"><a></a>    block_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb4-253"><a></a>    vocab_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50304</span></span>
<span id="cb4-254"><a></a>    n_layer: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb4-255"><a></a>    n_head: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb4-256"><a></a>    n_embd: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb4-257"><a></a></span>
<span id="cb4-258"><a></a></span>
<span id="cb4-259"><a></a></span>
<span id="cb4-260"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-261"><a></a><span class="co"># GPT2 Model Implementation</span></span>
<span id="cb4-262"><a></a><span class="co"># ===================================================================</span></span>
<span id="cb4-263"><a></a></span>
<span id="cb4-264"><a></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb4-265"><a></a></span>
<span id="cb4-266"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config, device):</span>
<span id="cb4-267"><a></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-268"><a></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb4-269"><a></a>        <span class="va">self</span>.device <span class="op">=</span> device</span>
<span id="cb4-270"><a></a>        <span class="va">self</span>.manual_seed <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb4-271"><a></a></span>
<span id="cb4-272"><a></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.ModuleDict(</span>
<span id="cb4-273"><a></a>            <span class="bu">dict</span>(</span>
<span id="cb4-274"><a></a>                wte<span class="op">=</span>nn.Embedding(config.vocab_size, config.n_embd),</span>
<span id="cb4-275"><a></a>                wpe<span class="op">=</span>nn.Embedding(config.block_size, config.n_embd),</span>
<span id="cb4-276"><a></a>                h<span class="op">=</span>nn.ModuleList(</span>
<span id="cb4-277"><a></a>                    [Block(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.n_layer)]</span>
<span id="cb4-278"><a></a>                ),</span>
<span id="cb4-279"><a></a>                ln_f<span class="op">=</span>nn.LayerNorm(config.n_embd),</span>
<span id="cb4-280"><a></a>            )</span>
<span id="cb4-281"><a></a>        )</span>
<span id="cb4-282"><a></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(</span>
<span id="cb4-283"><a></a>            config.n_embd, config.vocab_size, bias<span class="op">=</span><span class="va">False</span></span>
<span id="cb4-284"><a></a>        )</span>
<span id="cb4-285"><a></a></span>
<span id="cb4-286"><a></a>        <span class="va">self</span>.transformer.wte.weight <span class="op">=</span> <span class="va">self</span>.lm_head.weight</span>
<span id="cb4-287"><a></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb4-288"><a></a></span>
<span id="cb4-289"><a></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb4-290"><a></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb4-291"><a></a>            std <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb4-292"><a></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(module, <span class="st">"NANOGPT_SCALE_INIT"</span>):</span>
<span id="cb4-293"><a></a>                std <span class="op">*=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.config.n_layer) <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb4-294"><a></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span>std)</span>
<span id="cb4-295"><a></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-296"><a></a>                torch.nn.init.zeros_(module.bias)</span>
<span id="cb4-297"><a></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb4-298"><a></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb4-299"><a></a></span>
<span id="cb4-300"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-301"><a></a>        <span class="co">"""</span></span>
<span id="cb4-302"><a></a><span class="co">        Forward pass of the GPT model.</span></span>
<span id="cb4-303"><a></a></span>
<span id="cb4-304"><a></a><span class="co">        Args:</span></span>
<span id="cb4-305"><a></a><span class="co">            idx (torch.Tensor): Supplies the input tensor of shape</span></span>
<span id="cb4-306"><a></a><span class="co">                (B, T).</span></span>
<span id="cb4-307"><a></a><span class="co">            targets (torch.Tensor): Optionally supplies the target</span></span>
<span id="cb4-308"><a></a><span class="co">                tensor of shape (B, T) for computing the loss.</span></span>
<span id="cb4-309"><a></a></span>
<span id="cb4-310"><a></a><span class="co">        """</span></span>
<span id="cb4-311"><a></a>        (B, T) <span class="op">=</span> idx.size()</span>
<span id="cb4-312"><a></a></span>
<span id="cb4-313"><a></a>        <span class="co"># Forward the token and position embeddings.</span></span>
<span id="cb4-314"><a></a></span>
<span id="cb4-315"><a></a>        <span class="co"># Shape (T)</span></span>
<span id="cb4-316"><a></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, T, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>idx.device)</span>
<span id="cb4-317"><a></a></span>
<span id="cb4-318"><a></a>        <span class="co"># Position embeddings of shape (T, n_embd).</span></span>
<span id="cb4-319"><a></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.transformer.wpe(pos)</span>
<span id="cb4-320"><a></a></span>
<span id="cb4-321"><a></a>        <span class="co"># Token embeddings of shape (B, T, n_embd).</span></span>
<span id="cb4-322"><a></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.transformer.wte(idx)</span>
<span id="cb4-323"><a></a></span>
<span id="cb4-324"><a></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb</span>
<span id="cb4-325"><a></a></span>
<span id="cb4-326"><a></a>        <span class="co"># Forward the blocks of the transformer.</span></span>
<span id="cb4-327"><a></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb4-328"><a></a>            x <span class="op">=</span> block(x)</span>
<span id="cb4-329"><a></a></span>
<span id="cb4-330"><a></a>        <span class="co"># Forward the final layernorm and the classifier.</span></span>
<span id="cb4-331"><a></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.ln_f(x)</span>
<span id="cb4-332"><a></a></span>
<span id="cb4-333"><a></a>        <span class="co"># (B, T, vocab_size)</span></span>
<span id="cb4-334"><a></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb4-335"><a></a></span>
<span id="cb4-336"><a></a>        loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-337"><a></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-338"><a></a>            loss <span class="op">=</span> F.cross_entropy(</span>
<span id="cb4-339"><a></a>                logits.view(<span class="op">-</span><span class="dv">1</span>, logits.size(<span class="op">-</span><span class="dv">1</span>)), targets.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-340"><a></a>            )</span>
<span id="cb4-341"><a></a></span>
<span id="cb4-342"><a></a>        <span class="cf">return</span> (logits, loss)</span>
<span id="cb4-343"><a></a></span>
<span id="cb4-344"><a></a>    <span class="at">@classmethod</span></span>
<span id="cb4-345"><a></a>    <span class="kw">def</span> from_local_pretrained(</span>
<span id="cb4-346"><a></a>        cls, model_path: <span class="bu">str</span>, map_location: <span class="bu">str</span> <span class="op">=</span> <span class="st">"cuda"</span></span>
<span id="cb4-347"><a></a>    ):</span>
<span id="cb4-348"><a></a>        <span class="co">"""</span></span>
<span id="cb4-349"><a></a><span class="co">        Load a model from a local checkpoint.</span></span>
<span id="cb4-350"><a></a></span>
<span id="cb4-351"><a></a><span class="co">        N.B. This is a new method based off GPT.from_pretrained</span></span>
<span id="cb4-352"><a></a><span class="co">             in Andrej Karpathy's train_gpt2.py.</span></span>
<span id="cb4-353"><a></a></span>
<span id="cb4-354"><a></a><span class="co">        Args:</span></span>
<span id="cb4-355"><a></a><span class="co">            cls (type): Supplies the class type.</span></span>
<span id="cb4-356"><a></a></span>
<span id="cb4-357"><a></a><span class="co">            model_path (str): Supplies the path to the model</span></span>
<span id="cb4-358"><a></a><span class="co">                checkpoint.</span></span>
<span id="cb4-359"><a></a></span>
<span id="cb4-360"><a></a><span class="co">            map_location (str): Supplies the device to which</span></span>
<span id="cb4-361"><a></a><span class="co">                the model will be mapped.</span></span>
<span id="cb4-362"><a></a><span class="co">        """</span></span>
<span id="cb4-363"><a></a>        <span class="cf">with</span> torch.serialization.safe_globals([GPTConfig]):</span>
<span id="cb4-364"><a></a>            checkpoint <span class="op">=</span> torch.load(</span>
<span id="cb4-365"><a></a>                model_path,</span>
<span id="cb4-366"><a></a>                map_location<span class="op">=</span>map_location,</span>
<span id="cb4-367"><a></a>            )</span>
<span id="cb4-368"><a></a></span>
<span id="cb4-369"><a></a>        config <span class="op">=</span> checkpoint[<span class="st">"config"</span>]</span>
<span id="cb4-370"><a></a>        config <span class="op">=</span> GPTConfig(<span class="op">**</span>checkpoint[<span class="st">"config"</span>])</span>
<span id="cb4-371"><a></a>        model <span class="op">=</span> cls(config, device<span class="op">=</span>map_location)</span>
<span id="cb4-372"><a></a>        model.load_state_dict(checkpoint[<span class="st">"model"</span>])</span>
<span id="cb4-373"><a></a>        model.<span class="bu">eval</span>()</span>
<span id="cb4-374"><a></a></span>
<span id="cb4-375"><a></a>        msg <span class="op">=</span> (</span>
<span id="cb4-376"><a></a>            <span class="ss">f"Loaded model from step </span><span class="sc">{</span>checkpoint[<span class="st">'step'</span>]<span class="sc">}</span><span class="ss">, "</span></span>
<span id="cb4-377"><a></a>            <span class="ss">f"val_loss </span><span class="sc">{</span>checkpoint[<span class="st">'val_loss'</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-378"><a></a>        )</span>
<span id="cb4-379"><a></a>        logging.info(msg)</span>
<span id="cb4-380"><a></a>        <span class="cf">return</span> model</span>
<span id="cb4-381"><a></a></span>
<span id="cb4-382"><a></a>    <span class="kw">def</span> generate(</span>
<span id="cb4-383"><a></a>        <span class="va">self</span>, text: <span class="bu">str</span>, max_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1024</span>, top_k: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>,</span>
<span id="cb4-384"><a></a>        seed: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-385"><a></a>    ) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb4-386"><a></a>        <span class="co">"""</span></span>
<span id="cb4-387"><a></a><span class="co">        Generate text from the model.</span></span>
<span id="cb4-388"><a></a></span>
<span id="cb4-389"><a></a><span class="co">        N.B. This is a new method based off the generation code</span></span>
<span id="cb4-390"><a></a><span class="co">             present in Andrej Karpathy's train_gpt2.py.</span></span>
<span id="cb4-391"><a></a></span>
<span id="cb4-392"><a></a><span class="co">        Args:</span></span>
<span id="cb4-393"><a></a></span>
<span id="cb4-394"><a></a><span class="co">            text (str): Supplies the prompt.</span></span>
<span id="cb4-395"><a></a></span>
<span id="cb4-396"><a></a><span class="co">            max_length (int): Supplies the maximum total length,</span></span>
<span id="cb4-397"><a></a><span class="co">                including prompt.</span></span>
<span id="cb4-398"><a></a></span>
<span id="cb4-399"><a></a><span class="co">            top_k (int): Supplies the number of tokens to consider</span></span>
<span id="cb4-400"><a></a><span class="co">                at each generation step.</span></span>
<span id="cb4-401"><a></a></span>
<span id="cb4-402"><a></a><span class="co">            seed (int): Optionally supplies the manual seed to use</span></span>
<span id="cb4-403"><a></a><span class="co">                for the generator.  If None, the model's manual</span></span>
<span id="cb4-404"><a></a><span class="co">                seed will be used.</span></span>
<span id="cb4-405"><a></a></span>
<span id="cb4-406"><a></a><span class="co">        Returns:</span></span>
<span id="cb4-407"><a></a></span>
<span id="cb4-408"><a></a><span class="co">            str: The generated text (including the initial prompt).</span></span>
<span id="cb4-409"><a></a><span class="co">        """</span></span>
<span id="cb4-410"><a></a>        <span class="va">self</span>.<span class="bu">eval</span>()</span>
<span id="cb4-411"><a></a>        device <span class="op">=</span> <span class="va">self</span>.device</span>
<span id="cb4-412"><a></a>        <span class="co"># Obtain our GPT2 tokenizer, and resolve the stop token.</span></span>
<span id="cb4-413"><a></a>        enc <span class="op">=</span> tiktoken.get_encoding(<span class="st">"gpt2"</span>)</span>
<span id="cb4-414"><a></a>        stop_string <span class="op">=</span> <span class="st">'&lt;|endoftext|&gt;'</span></span>
<span id="cb4-415"><a></a>        stop_token <span class="op">=</span> enc.n_vocab <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb4-416"><a></a>        actual <span class="op">=</span> enc.decode([stop_token])</span>
<span id="cb4-417"><a></a>        <span class="cf">assert</span> actual <span class="op">==</span> stop_string, (</span>
<span id="cb4-418"><a></a>            <span class="ss">f"expected </span><span class="sc">{</span>stop_string<span class="sc">}</span><span class="ss">, got </span><span class="sc">{</span>actual<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-419"><a></a>        )</span>
<span id="cb4-420"><a></a></span>
<span id="cb4-421"><a></a>        <span class="co"># Encode the prompt.</span></span>
<span id="cb4-422"><a></a>        tokens <span class="op">=</span> enc.encode(text)</span>
<span id="cb4-423"><a></a>        x <span class="op">=</span> torch.tensor(</span>
<span id="cb4-424"><a></a>            tokens, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device</span>
<span id="cb4-425"><a></a>        ).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb4-426"><a></a></span>
<span id="cb4-427"><a></a>        <span class="co"># Create a random generator for reproducibility.</span></span>
<span id="cb4-428"><a></a>        <span class="cf">if</span> seed <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-429"><a></a>            seed <span class="op">=</span> <span class="va">self</span>.manual_seed</span>
<span id="cb4-430"><a></a>        sample_rng <span class="op">=</span> torch.Generator(device<span class="op">=</span>device)</span>
<span id="cb4-431"><a></a>        sample_rng.manual_seed(seed)</span>
<span id="cb4-432"><a></a></span>
<span id="cb4-433"><a></a>        <span class="co"># Generate tokens up to our max length, or until we hit the</span></span>
<span id="cb4-434"><a></a>        <span class="co"># stop token.</span></span>
<span id="cb4-435"><a></a>        start <span class="op">=</span> time.perf_counter()</span>
<span id="cb4-436"><a></a>        count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-437"><a></a>        <span class="cf">while</span> x.size(<span class="dv">1</span>) <span class="op">&lt;</span> max_length:</span>
<span id="cb4-438"><a></a>            count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-439"><a></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-440"><a></a>                <span class="co"># Forward pass, ignoring the returned loss.</span></span>
<span id="cb4-441"><a></a>                (logits, _) <span class="op">=</span> <span class="va">self</span>(x)</span>
<span id="cb4-442"><a></a></span>
<span id="cb4-443"><a></a>            <span class="co"># Take the logits at the last time-step (shape:</span></span>
<span id="cb4-444"><a></a>            <span class="co"># (1, vocab_size)).</span></span>
<span id="cb4-445"><a></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb4-446"><a></a></span>
<span id="cb4-447"><a></a>            <span class="co"># Convert to probabilities.</span></span>
<span id="cb4-448"><a></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-449"><a></a></span>
<span id="cb4-450"><a></a>            <span class="co"># Top-k sampling.</span></span>
<span id="cb4-451"><a></a>            topk_probs, topk_indices <span class="op">=</span> torch.topk(</span>
<span id="cb4-452"><a></a>                probs, k<span class="op">=</span>top_k, dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb4-453"><a></a>            )</span>
<span id="cb4-454"><a></a></span>
<span id="cb4-455"><a></a>            <span class="co"># Sample the next token.</span></span>
<span id="cb4-456"><a></a>            next_idx <span class="op">=</span> torch.multinomial(</span>
<span id="cb4-457"><a></a>                topk_probs, num_samples<span class="op">=</span><span class="dv">1</span>, generator<span class="op">=</span>sample_rng</span>
<span id="cb4-458"><a></a>            )</span>
<span id="cb4-459"><a></a>            next_token <span class="op">=</span> torch.gather(topk_indices, <span class="op">-</span><span class="dv">1</span>, next_idx)</span>
<span id="cb4-460"><a></a></span>
<span id="cb4-461"><a></a>            <span class="co"># If the next token is the stop token, we're done.</span></span>
<span id="cb4-462"><a></a>            <span class="cf">if</span> next_token.item() <span class="op">==</span> stop_token:</span>
<span id="cb4-463"><a></a>                <span class="cf">break</span></span>
<span id="cb4-464"><a></a></span>
<span id="cb4-465"><a></a>            <span class="co"># Otherwise, append the token to the current sequence</span></span>
<span id="cb4-466"><a></a>            <span class="co"># and continue generation.</span></span>
<span id="cb4-467"><a></a>            x <span class="op">=</span> torch.cat((x, next_token), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-468"><a></a></span>
<span id="cb4-469"><a></a>        end <span class="op">=</span> time.perf_counter()</span>
<span id="cb4-470"><a></a>        elapsed <span class="op">=</span> end <span class="op">-</span> start</span>
<span id="cb4-471"><a></a>        tokens_per_sec <span class="op">=</span> <span class="bu">float</span>(count) <span class="op">/</span> elapsed</span>
<span id="cb4-472"><a></a></span>
<span id="cb4-473"><a></a>        msg <span class="op">=</span> (</span>
<span id="cb4-474"><a></a>            <span class="ss">f'Generated </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss"> tokens in </span><span class="sc">{</span>elapsed<span class="sc">:.2f}</span><span class="ss"> seconds '</span></span>
<span id="cb4-475"><a></a>            <span class="ss">f'(</span><span class="sc">{</span>tokens_per_sec<span class="sc">:.2f}</span><span class="ss"> tokens/sec)'</span></span>
<span id="cb4-476"><a></a>        )</span>
<span id="cb4-477"><a></a>        logging.debug(msg)</span>
<span id="cb4-478"><a></a></span>
<span id="cb4-479"><a></a>        <span class="co"># Decode the output tokens and return the generated text,</span></span>
<span id="cb4-480"><a></a>        <span class="co"># including the initial prompt.</span></span>
<span id="cb4-481"><a></a>        output_tokens <span class="op">=</span> x[<span class="dv">0</span>].tolist()</span>
<span id="cb4-482"><a></a>        <span class="cf">return</span> enc.decode(output_tokens)</span>
<span id="cb4-483"><a></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
</div>
</section>
<section id="loading-the-model" class="slide level2">
<h2>Loading the Model</h2>
<p>With the code above executed in a preceding Jupyter Notebook cell, we can load the model as follows:</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/trent/src/website/decks/pytorch-and-python-free-threading/gpt2-v1.ipynb" data-notebook-title="gpt2_v1.ipynb" data-notebook-cellid="cell-gpt2-v1-load-model">
<div id="cell-gpt2-v1-load-model" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>model <span class="op">=</span> GPT.from_local_pretrained(</span>
<span id="cb5-2"><a></a>    MODEL_CHECKPOINT,</span>
<span id="cb5-3"><a></a>    map_location<span class="op">=</span><span class="st">'cuda'</span>,</span>
<span id="cb5-4"><a></a>)</span>
<span id="cb5-5"><a></a>model.to(<span class="st">'cuda'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>2025-02-09 15:26:39,136 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795</code></pre>
</div>
<div id="gpt2-v1-load-model" class="cell-output cell-output-display" data-execution_count="2">
<pre><code>GPT(
  (transformer): ModuleDict(
    (wte): Embedding(50304, 768)
    (wpe): Embedding(1024, 768)
    (h): ModuleList(
      (0-11): 12 x Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): CausalSelfAttention(
          (c_attn): Linear(in_features=768, out_features=2304, bias=True)
          (c_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): GELU(approximate='tanh')
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50304, bias=False)
)</code></pre>
</div>
</div>
</div>
</section>
<section id="generating-text" class="slide level2">
<h2>Generating Text</h2>
<div class="desktop-only">
<div class="quarto-embed-nb-cell" data-notebook="/home/trent/src/website/decks/pytorch-and-python-free-threading/gpt2-v1.ipynb" data-notebook-title="gpt2_v1.ipynb" data-notebook-cellid="cell-gpt2-v1-generate-1-desktop">
<div id="gpt2-v1-generate-1-desktop" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>prompt <span class="op">=</span> <span class="st">"Albert Einstein's Theory of Relativity stated that"</span></span>
<span id="cb8-2"><a></a>result <span class="op">=</span> model.generate(prompt, seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-3"><a></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> textwrap.fill(result, width<span class="op">=</span><span class="dv">105</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>2025-02-09 15:26:40,464 - DEBUG - Generated 79 tokens in 0.81 seconds (98.10 tokens/sec)

Albert Einstein's Theory of Relativity stated that the speed of light was approximately 10 000 of
parsecs, whereas quantum physicists have suggested that, as we move further into the universe, the
universe might grow older. The new experiment, conducted by researchers at the University of New Jersey,
New York, and the University of California, Berkeley shows that photons travelling at the speed of light
will be around 30 to 65 kilometres per second.</code></pre>
</div>
</div>
</div>
</div>
<div class="tablet-only">
<div class="quarto-embed-nb-cell" data-notebook="/home/trent/src/website/decks/pytorch-and-python-free-threading/gpt2-v1.ipynb" data-notebook-title="gpt2_v1.ipynb" data-notebook-cellid="cell-gpt2-v1-generate-1-tablet">
<div id="gpt2-v1-generate-1-tablet" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>prompt <span class="op">=</span> <span class="st">"Albert Einstein's Theory of Relativity stated that"</span></span>
<span id="cb10-2"><a></a>result <span class="op">=</span> model.generate(prompt, seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-3"><a></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> textwrap.fill(result, width<span class="op">=</span><span class="dv">58</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>2025-02-09 15:26:41,014 - DEBUG - Generated 79 tokens in 0.54 seconds (145.49 tokens/sec)

Albert Einstein's Theory of Relativity stated that the
speed of light was approximately 10 000 of parsecs,
whereas quantum physicists have suggested that, as we move
further into the universe, the universe might grow older.
The new experiment, conducted by researchers at the
University of New Jersey, New York, and the University of
California, Berkeley shows that photons travelling at the
speed of light will be around 30 to 65 kilometres per
second.</code></pre>
</div>
</div>
</div>
</div>
<div class="phone-only">
<div class="quarto-embed-nb-cell" data-notebook="/home/trent/src/website/decks/pytorch-and-python-free-threading/gpt2-v1.ipynb" data-notebook-title="gpt2_v1.ipynb" data-notebook-cellid="cell-gpt2-v1-generate-1-phone">
<div id="gpt2-v1-generate-1-phone" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>prompt <span class="op">=</span> <span class="st">"Albert Einstein's Theory of Relativity stated that"</span></span>
<span id="cb12-2"><a></a>result <span class="op">=</span> model.generate(prompt, seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-3"><a></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> textwrap.fill(result, width<span class="op">=</span><span class="dv">45</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>2025-02-09 15:26:41,565 - DEBUG - Generated 79 tokens in 0.54 seconds (145.05 tokens/sec)

Albert Einstein's Theory of Relativity stated
that the speed of light was approximately 10
000 of parsecs, whereas quantum physicists
have suggested that, as we move further into
the universe, the universe might grow older.
The new experiment, conducted by researchers
at the University of New Jersey, New York,
and the University of California, Berkeley
shows that photons travelling at the speed of
light will be around 30 to 65 kilometres per
second.</code></pre>
</div>
</div>
</div>
</div>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../static/images/nvidia-logo.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-bg-video-ctrl/bg-video-ctrl.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: '75%',

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealBgVideoCtrl, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/trent\.me");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <input type="hidden" id="giscus-base-theme" value="dark">
    <input type="hidden" id="giscus-alt-theme" value="dark">
    <script>
      function loadGiscus() {
        // Function to get the theme based on body class
        const getTheme = () => {
          let baseTheme = document.getElementById('giscus-base-theme').value;
          let altTheme = document.getElementById('giscus-alt-theme').value;
          return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
        };
        const script = document.createElement("script");
        script.src = "https://giscus.app/client.js";
        script.async = true;
        script.dataset.repo = "tpn/website";
        script.dataset.repoId = "MDEwOlJlcG9zaXRvcnkxMjg2ODc3NTQ=";
        script.dataset.category = "Announcements";
        script.dataset.categoryId = "DIC_kwDOB6ueis4Cj53i";
        script.dataset.mapping = "pathname";
        script.dataset.reactionsEnabled = "1";
        script.dataset.emitMetadata = "0";
        script.dataset.inputPosition = "top";
        script.dataset.theme = getTheme();
        script.dataset.lang = "en";
        script.crossOrigin = "anonymous";
        // Append the script to the desired div instead of at the end of the body
        document.getElementById("quarto-content").appendChild(script);
      }
      loadGiscus();
    </script>
    

</body></html>