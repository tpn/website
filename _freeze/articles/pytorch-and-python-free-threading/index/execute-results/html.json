{
  "hash": "d2a2a9e636476d525e13bac228fc7ccf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PyTorch and Python Free-Threading\"\nsubtitle: |\n    Unlocking multi-threaded parallel inference on PyTorch models\ncategories:\n  - PyTorch\n  - Python\n  - Free-Threading\n  - No-GIL\n  - LLM\n  - GPT2\nauthor: \"Trent Nelson\"\ndate: 02/13/2025\n#draft: true\n#draft-mode: visible\nimage: \"images/pytorch-and-python-free-threading.png\"\ndescription: |\n  This post examines multi-threaded parallel inference on PyTorch models\n  using the new *No-GIL*, free-threaded version of Python.  Using a simple\n  124M parameter GPT2 model that we train from scratch, we explore the novel\n  new territory unlocked by free-threaded Python: parallel PyTorch model\n  inference, where multiple threads, unimpeded by the Python GIL, attempt to\n  generate text from a transformer-based model in parallel.\njupyter:\n  kernelspec:\n    display_name: \"Python 3.13t (Free-Threaded)\"\n    language: python\n    name: py313t\nexecute:\n  echo: true\n  eval: true\n  output: true\n  error: true\n  cache: true\n  freeze: auto\nlightbox:\n  match: auto\ncomments: false\nformat:\n  html:\n    grid:\n      gutter-width: 1rem\nnotebook-view:\n  - notebook: torch-compile-analysis.ipynb\n    title: \"Data Visualization: torch.compile\"\n  - notebook: wrk-analysis.ipynb\n    title: \"Data Visualization: wrk\"\n  - notebook: gpt2-v1.ipynb\n    title: \"GPT2 v1\"\n---\n\n\n\n\nThis post is sponsored in part by [Meta](https://meta.com) in\ncollaboration with [Quansight](https://quansight.com) and\n[OpenTeams](https://openteams.com).\n\n::: {.callout-tip}\n\nToggle the little radio button in the navigation bar at the top to switch\nbetween native *Light Mode* and *Dark Mode* themes.\n\n:::\n\n# Introduction\n\nPython 3.13, released in October 2024, is the first version of Python to\nintroduce support for a \"no-GIL\" *free-threaded* mode, per\n[PEP-703 Making the Global Interpreter Lock Optional in CPython](\nhttps://peps.python.org/pep-0703/), unlocking the ability for multiple Python\nthreads to run simultaneously.\n\nThis allows, for the first time since the language's inception in December\n1989, a single Python process to saturate all CPU cores in parallel with\npure Python code (i.e. not farming out to extension modules written in C, C++,\nor, more recently, Rust).\n\nA handful of the [motivations](https://peps.python.org/pep-0703/#motivation)\ncaptured in that PEP opine on how the GIL impedes Python AI workflows,\nparticularly as it relates to GPU programming.\n\nThis blog post explores what can be done with [PyTorch](https://pytorch.org)\nnow with the new free-threaded version of Python, specifically focusing on\nrun-time inference on transformer-based generative models.\n\n::: {.callout-note}\n\nI didn't focus on how *training* PyTorch models might look in the new\nfree-threaded Python world for a couple of reasons.\n\nPrimarily, training is\na lot more complex if you're involving multiple nodes---as gradients need\nto be carefully synchronized at critical points, for example---and is well\noutside the scope of a simple blog post.\n\nAdditionally, there's already a huge body of existing work tackling\nmulti-node training in PyTorch by way of the [Distributed Data Parallel](\nhttps://pytorch.org/tutorials/intermediate/ddp_tutorial.html),\n`multiprocessing`-based facilities exposed by [`torch.distributed`](\nhttps://pytorch.org/tutorials/intermediate/dist_tuto.html).\n\nWhereas, on the flip side, no one has really explored what parallel\ninference might look like in a single-threaded Python because the GIL has\nprevented that from even being an option until now.\n\n:::\n\n# Getting Started\n\nAll of this work was done on Linux (Ubuntu 22.04) with Python 3.13t, PyTorch\n2.6, and CUDA 12.6.\n\nFull source code is provided to everything captured in this post.  It is\nworth noting that in a few cases, I am rolling my own solutions for things\nthat have existing solutions in the broader Python ecosystem.  For example,\nin the tail end of the post, I leverage a multi-threaded `asyncio`-based\nHTTP server I wrote instead of using existing solutions like\n[FastAPI](https://fastapi.tiangolo.com/).\n\nThe reason for this is that, as free-threaded Python is still in its\ninfancy, a lot of packages do not work with it yet, especially those that\nrely on Cython, C or C++ code, or Rust.\n\nIn fact, Rust dependencies are particularly problematic due to the\nproliferation of Python projects leveraging [PyO3](\nhttps://github.com/PyO3/pyo3) (Rust bindings for Python), especially\nprominent projects such as [TikToken](https://github.com/openai/tiktoken)\nand [Pydantic](https://docs.pydantic.dev/latest/), upon which a lot of the\nPython AI ecosystem is built.  PyO3 only recently grew support for\nfree-threaded Python in their 0.23.3 release, which came out in December,\n2024, and many dependent projects are yet to update to it.\n\nThus, this post and its supporting code should not be considered the state\nof the art for production deployments---its primary goal is exploratory in\nnature, and minimizing the number of moving pieces in the stack helps\nachieve this goal.\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Environment Setup Details\"}\n\n## Environments\n\nIt is fiddly getting the environments set up in support of this post.\nAgain, this is due to the infancy of free-threaded Python.  So I apologize\nin advance for how long this environment setup section is.\n\nI reference two `conda` environments in this post: a Python 3.13\nfree-threaded one named `py313t`, and a normal, not-free-threaded Python\n3.13 one named `py313`.\n\nThe primary motivation behind the second `py313` environment is it allows\nus to install Jupyter Lab, which, at the time of writing, still isn't\ncompatible with a Python free-threaded installation.  However, we can still\nregister a free-threaded Python kernel with Jupyter, which is all we really\ncare about when running the code in this post in a free-threaded environment.\n\nDetails on creating the `conda` environments follow.\n\n### Free-Threaded 3.13 Env (py313t)\n\nI use `conda` to create the Python 3.13 free-threaded environment plus\ninitial dependencies, activate it, then install the remaining dependencies\nvia pip, as follows:\n\n```bash\nconda create -n py313t python=3.13 python-freethreading \\\n    nodejs pip tqdm flake8 rust requests \\\n        -c conda-forge\nconda activate py313t\npip install numpy setuptools_rust regex safetensors\n```\n\n`nodejs` is required for the UI component we'll introduce later.  `regex`,\n`rust`, and `setuptools_rust` are needed for `tiktoken`, described next.\nFinally, `numpy` is for `torch`, which we install later, too.\n\n#### TikToken\n\n[TikToken](https://github.com/openai/tiktoken) is a fast BPE tokenizer from\nOpenAI that is used extensively in the emerging Python LLM landscape.  At the\ntime of writing, the latest TikToken release was [0.8.0](\nhttps://github.com/openai/tiktoken/releases/tag/0.8.0), which was built\nagainst PyO3 0.22.2, which isn't compatible with free-threaded Python.\n\nThankfully, it was trivial to get a local installation of `tiktoken` working\nby cloning the Github repo, bumping the PyO3 version in `Cargo.toml`, then\nrebuilding and installing.\n\n::: {.callout-note}\n\nThis is a perfect example of the type of fiddling around I wanted to avoid\nby not depending on any external packages other than the bare necessities,\nsuch as PyTorch.  I made an exception for `tiktoken` because a) it's arguably\nan equally-important part of the LLM stack as `torch`, and b) it thankfully\nwasn't *too* difficult getting a compatible version of `tiktoken` installed\nlocally.\n\n:::\n\nClone the tiktoken git repo and cd into it as follows:\n\n```bash\ngit clone https://github.com/openai/tiktoken\ncd tiktoken\n```\n\nEdit the `Cargo.toml` file and change the `pyo3` dependency version to at\nleast 0.23.3^[I used 0.23.3, as that was the latest version available at the\ntime, however, 0.23.4 has since been released, so you could try that too.]:\n\n```diff\ndiff --git a/Cargo.toml b/Cargo.toml\nindex 2eed0c1..6be5f63 100644\n--- a/Cargo.toml\n+++ b/Cargo.toml\n@@ -9,7 +9,7 @@ name = \"_tiktoken\"\n crate-type = [\"cdylib\"]\n \n [dependencies]\n-pyo3 = { version = \"0.22.2\", default-features = false, features = [\"extension-module\", \"macros\"] }\n+pyo3 = { version = \"0.23.3\", default-features = false, features = [\"extension-module\", \"macros\"] }\n \n # tiktoken dependencies\n fancy-regex = \"0.13.0\"\n```\n\nWith this patch applied, and the `py313t` conda environment active (with\n`rust` and `setuptools_rust` installed):\n\n```bash\npython setup.py build\npython setup.py install\n```\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Show Build & Install Output\"}\n\n\n```bash\n(py313t) {pytorch} [12.6] [trent@dgx/ttypts/3(~s/tiktoken)%] python setup.py build\nrunning build\nrunning build_py\ncopying tiktoken/_educational.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/__init__.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/registry.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/model.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/load.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/core.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken_ext/openai_public.py -> build/lib.linux-x86_64-cpython-313t/tiktoken_ext\nrunning egg_info\nwriting tiktoken.egg-info/PKG-INFO\nwriting dependency_links to tiktoken.egg-info/dependency_links.txt\nwriting requirements to tiktoken.egg-info/requires.txt\nwriting top-level names to tiktoken.egg-info/top_level.txt\nreading manifest file 'tiktoken.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwarning: no files found matching 'Makefile'\nadding license file 'LICENSE'\nwriting manifest file 'tiktoken.egg-info/SOURCES.txt'\ncopying tiktoken/py.typed -> build/lib.linux-x86_64-cpython-313t/tiktoken\nrunning build_ext\nrunning build_rust\ncargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --\n   Compiling target-lexicon v0.12.16\n   Compiling once_cell v1.20.2\n   Compiling proc-macro2 v1.0.92\n   Compiling unicode-ident v1.0.14\n   Compiling memchr v2.7.4\n   Compiling regex-syntax v0.8.5\n   Compiling libc v0.2.169\n   Compiling autocfg v1.4.0\n   Compiling heck v0.5.0\n   Compiling bit-vec v0.6.3\n   Compiling unindent v0.2.3\n   Compiling indoc v2.0.5\n   Compiling cfg-if v1.0.0\n   Compiling rustc-hash v1.1.0\n     Running `rustc --crate-name build_script_build --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/target-lexicon-0.12.16/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arch_zkasm\", \"default\", \"serde\", \"serde_support\", \"std\"))' -C metadata=826df8be4fa9ef21 -C extra-filename=-826df8be4fa9ef21 --out-dir /home/trent/src/tiktoken/target/release/build/target-lexicon-826df8be4fa9ef21 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name once_cell --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/once_cell-1.20.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"race\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"atomic-polyfill\", \"critical-section\", \"default\", \"parking_lot\", \"portable-atomic\", \"race\", \"std\", \"unstable\"))' -C metadata=6870d82906744d65 -C extra-filename=-6870d82906744d65 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.92/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=4463b02a2f05d75c -C extra-filename=-4463b02a2f05d75c --out-dir /home/trent/src/tiktoken/target/release/build/proc-macro2-4463b02a2f05d75c -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name unicode_ident --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-ident-1.0.14/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=17278dafa5f26de9 -C extra-filename=-17278dafa5f26de9 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name memchr --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memchr-2.7.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"compiler_builtins\", \"core\", \"default\", \"libc\", \"logging\", \"rustc-dep-of-std\", \"std\", \"use_std\"))' -C metadata=25fa9792dd9399b0 -C extra-filename=-25fa9792dd9399b0 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name regex_syntax --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-syntax-0.8.5/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arbitrary\", \"default\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\"))' -C metadata=66f570e05dbe3825 -C extra-filename=-66f570e05dbe3825 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.169/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=42fd5088387abf7a -C extra-filename=-42fd5088387abf7a --out-dir /home/trent/src/tiktoken/target/release/build/libc-42fd5088387abf7a -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name autocfg --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/autocfg-1.4.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b8e4c5d316ce5bfb -C extra-filename=-b8e4c5d316ce5bfb --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name bit_vec --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bit-vec-0.6.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"serde\", \"serde_no_std\", \"serde_std\", \"std\"))' -C metadata=de2a0d1e2ef2490a -C extra-filename=-de2a0d1e2ef2490a --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name heck --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/heck-0.5.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=5e22b1dffa7f4255 -C extra-filename=-5e22b1dffa7f4255 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name once_cell --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/once_cell-1.20.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"race\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"atomic-polyfill\", \"critical-section\", \"default\", \"parking_lot\", \"portable-atomic\", \"race\", \"std\", \"unstable\"))' -C metadata=05003007543cfa87 -C extra-filename=-05003007543cfa87 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name unindent --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unindent-0.2.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=d53e2a0385a47a80 -C extra-filename=-d53e2a0385a47a80 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name indoc --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indoc-2.0.5/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b4e94d9ecbd21a39 -C extra-filename=-b4e94d9ecbd21a39 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro --cap-lints allow`\n     Running `rustc --crate-name cfg_if --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cfg-if-1.0.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"rustc-dep-of-std\"))' -C metadata=e4ded2c19830fbdd -C extra-filename=-e4ded2c19830fbdd --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n     Running `rustc --crate-name rustc_hash --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustc-hash-1.1.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=b006f4d81e95dfaf -C extra-filename=-b006f4d81e95dfaf --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n   Compiling bit-set v0.5.3\n     Running `rustc --crate-name bit_set --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bit-set-0.5.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=40d90f83eb5bab57 -C extra-filename=-40d90f83eb5bab57 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern bit_vec=/home/trent/src/tiktoken/target/release/deps/libbit_vec-de2a0d1e2ef2490a.rmeta --cap-lints allow`\n     Running `/home/trent/src/tiktoken/target/release/build/libc-42fd5088387abf7a/build-script-build`\n     Running `/home/trent/src/tiktoken/target/release/build/proc-macro2-4463b02a2f05d75c/build-script-build`\n   Compiling memoffset v0.9.1\n     Running `rustc --crate-name build_script_build --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memoffset-0.9.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"unstable_const\", \"unstable_offset_of\"))' -C metadata=679ebbc3261d6845 -C extra-filename=-679ebbc3261d6845 --out-dir /home/trent/src/tiktoken/target/release/build/memoffset-679ebbc3261d6845 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern autocfg=/home/trent/src/tiktoken/target/release/deps/libautocfg-b8e4c5d316ce5bfb.rlib --cap-lints allow`\n     Running `rustc --crate-name libc --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.169/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=6b0fbe5bd5ba9d30 -C extra-filename=-6b0fbe5bd5ba9d30 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow --cfg freebsd11 --cfg libc_const_extern_fn --check-cfg 'cfg(emscripten_new_stat_abi)' --check-cfg 'cfg(espidf_time32)' --check-cfg 'cfg(freebsd10)' --check-cfg 'cfg(freebsd11)' --check-cfg 'cfg(freebsd12)' --check-cfg 'cfg(freebsd13)' --check-cfg 'cfg(freebsd14)' --check-cfg 'cfg(freebsd15)' --check-cfg 'cfg(libc_const_extern_fn)' --check-cfg 'cfg(libc_deny_warnings)' --check-cfg 'cfg(libc_thread_local)' --check-cfg 'cfg(libc_ctest)' --check-cfg 'cfg(target_os,values(\"switch\",\"aix\",\"ohos\",\"hurd\",\"rtems\",\"visionos\",\"nuttx\"))' --check-cfg 'cfg(target_env,values(\"illumos\",\"wasi\",\"aix\",\"ohos\"))' --check-cfg 'cfg(target_arch,values(\"loongarch64\",\"mips32r6\",\"mips64r6\",\"csky\"))'`\n     Running `rustc --crate-name proc_macro2 --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.92/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=4c69c42d9df03375 -C extra-filename=-4c69c42d9df03375 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern unicode_ident=/home/trent/src/tiktoken/target/release/deps/libunicode_ident-17278dafa5f26de9.rmeta --cap-lints allow --cfg wrap_proc_macro --check-cfg 'cfg(fuzzing)' --check-cfg 'cfg(no_is_available)' --check-cfg 'cfg(no_literal_byte_character)' --check-cfg 'cfg(no_literal_c_string)' --check-cfg 'cfg(no_source_text)' --check-cfg 'cfg(proc_macro_span)' --check-cfg 'cfg(procmacro2_backtrace)' --check-cfg 'cfg(procmacro2_nightly_testing)' --check-cfg 'cfg(procmacro2_semver_exempt)' --check-cfg 'cfg(randomize_layout)' --check-cfg 'cfg(span_locations)' --check-cfg 'cfg(super_unstable)' --check-cfg 'cfg(wrap_proc_macro)'`\n     Running `/home/trent/src/tiktoken/target/release/build/target-lexicon-826df8be4fa9ef21/build-script-build`\n     Running `rustc --crate-name target_lexicon --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/target-lexicon-0.12.16/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arch_zkasm\", \"default\", \"serde\", \"serde_support\", \"std\"))' -C metadata=a879275207ec599a -C extra-filename=-a879275207ec599a --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow --cfg 'feature=\"rust_1_40\"'`\n     Running `/home/trent/src/tiktoken/target/release/build/memoffset-679ebbc3261d6845/build-script-build`\n     Running `rustc --crate-name memoffset --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memoffset-0.9.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"unstable_const\", \"unstable_offset_of\"))' -C metadata=65fe1a8a113b3005 -C extra-filename=-65fe1a8a113b3005 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow --cfg tuple_ty --cfg allow_clippy --cfg maybe_uninit --cfg doctests --cfg raw_ref_macros --cfg stable_const --cfg stable_offset_of`\n   Compiling aho-corasick v1.1.3\n     Running `rustc --crate-name aho_corasick --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aho-corasick-1.1.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"perf-literal\", \"std\"))' -C metadata=6668c9f838fb89b3 -C extra-filename=-6668c9f838fb89b3 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --cap-lints allow`\n   Compiling pyo3-build-config v0.23.3\n     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-build-config-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"resolve-config\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"python3-dll-a\", \"resolve-config\"))' -C metadata=d8ab86bb094cf645 -C extra-filename=-d8ab86bb094cf645 --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-build-config-d8ab86bb094cf645 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern target_lexicon=/home/trent/src/tiktoken/target/release/deps/libtarget_lexicon-a879275207ec599a.rlib --cap-lints allow`\n   Compiling quote v1.0.38\n     Running `rustc --crate-name quote --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/quote-1.0.38/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"proc-macro\"))' -C metadata=169332b6fe3d21d6 -C extra-filename=-169332b6fe3d21d6 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rmeta --cap-lints allow`\n   Compiling syn v2.0.95\n     Running `rustc --crate-name syn --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-2.0.95/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"extra-traits\"' --cfg 'feature=\"full\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=29d4a0ddbd98f61f -C extra-filename=-29d4a0ddbd98f61f --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rmeta --extern quote=/home/trent/src/tiktoken/target/release/deps/libquote-169332b6fe3d21d6.rmeta --extern unicode_ident=/home/trent/src/tiktoken/target/release/deps/libunicode_ident-17278dafa5f26de9.rmeta --cap-lints allow`\n     Running `/home/trent/src/tiktoken/target/release/build/pyo3-build-config-d8ab86bb094cf645/build-script-build`\n     Running `rustc --crate-name pyo3_build_config --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-build-config-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"resolve-config\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"python3-dll-a\", \"resolve-config\"))' -C metadata=b884267014f5753b -C extra-filename=-b884267014f5753b --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern once_cell=/home/trent/src/tiktoken/target/release/deps/libonce_cell-6870d82906744d65.rmeta --extern target_lexicon=/home/trent/src/tiktoken/target/release/deps/libtarget_lexicon-a879275207ec599a.rmeta --cap-lints allow`\n   Compiling pyo3-ffi v0.23.3\n   Compiling pyo3-macros-backend v0.23.3\n   Compiling pyo3 v0.23.3\n     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-ffi-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"generate-import-lib\"))' -C metadata=5c5f8f108a22b6ae -C extra-filename=-5c5f8f108a22b6ae --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-ffi-5c5f8f108a22b6ae -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rlib --cap-lints allow`\n     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-macros-backend-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"experimental-async\"))' -C metadata=bb4a4b3911c85e51 -C extra-filename=-bb4a4b3911c85e51 --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-macros-backend-bb4a4b3911c85e51 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rlib --cap-lints allow`\n     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"pyo3-macros\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"anyhow\", \"auto-initialize\", \"chrono\", \"chrono-tz\", \"default\", \"either\", \"experimental-async\", \"experimental-inspect\", \"extension-module\", \"eyre\", \"full\", \"generate-import-lib\", \"hashbrown\", \"indexmap\", \"indoc\", \"inventory\", \"macros\", \"multiple-pymethods\", \"nightly\", \"num-bigint\", \"num-complex\", \"num-rational\", \"py-clone\", \"pyo3-macros\", \"rust_decimal\", \"serde\", \"smallvec\", \"unindent\"))' -C metadata=65f0dbfaaf67ed5a -C extra-filename=-65f0dbfaaf67ed5a --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-65f0dbfaaf67ed5a -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rlib --cap-lints allow`\n   Compiling regex-automata v0.4.9\n     Running `rustc --crate-name regex_automata --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-automata-0.4.9/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"dfa\"' --cfg 'feature=\"dfa-build\"' --cfg 'feature=\"dfa-onepass\"' --cfg 'feature=\"dfa-search\"' --cfg 'feature=\"hybrid\"' --cfg 'feature=\"meta\"' --cfg 'feature=\"nfa\"' --cfg 'feature=\"nfa-backtrack\"' --cfg 'feature=\"nfa-pikevm\"' --cfg 'feature=\"nfa-thompson\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-literal-multisubstring\"' --cfg 'feature=\"perf-literal-substring\"' --cfg 'feature=\"std\"' --cfg 'feature=\"syntax\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --cfg 'feature=\"unicode-word-boundary\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"dfa\", \"dfa-build\", \"dfa-onepass\", \"dfa-search\", \"hybrid\", \"internal-instrument\", \"internal-instrument-pikevm\", \"logging\", \"meta\", \"nfa\", \"nfa-backtrack\", \"nfa-pikevm\", \"nfa-thompson\", \"perf\", \"perf-inline\", \"perf-literal\", \"perf-literal-multisubstring\", \"perf-literal-substring\", \"std\", \"syntax\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unicode-word-boundary\"))' -C metadata=01135b6ed6d4413e -C extra-filename=-01135b6ed6d4413e --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern aho_corasick=/home/trent/src/tiktoken/target/release/deps/libaho_corasick-6668c9f838fb89b3.rmeta --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --extern regex_syntax=/home/trent/src/tiktoken/target/release/deps/libregex_syntax-66f570e05dbe3825.rmeta --cap-lints allow`\n     Running `/home/trent/src/tiktoken/target/release/build/pyo3-macros-backend-bb4a4b3911c85e51/build-script-build`\n     Running `/home/trent/src/tiktoken/target/release/build/pyo3-ffi-5c5f8f108a22b6ae/build-script-build`\n     Running `/home/trent/src/tiktoken/target/release/build/pyo3-65f0dbfaaf67ed5a/build-script-build`\n     Running `rustc --crate-name pyo3_ffi --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-ffi-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"generate-import-lib\"))' -C metadata=9ad2a4f2678f35bd -C extra-filename=-9ad2a4f2678f35bd --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern libc=/home/trent/src/tiktoken/target/release/deps/liblibc-6b0fbe5bd5ba9d30.rmeta --cap-lints allow --cfg Py_3_7 --cfg Py_3_8 --cfg Py_3_9 --cfg Py_3_10 --cfg Py_3_11 --cfg Py_3_12 --cfg Py_3_13 --cfg Py_GIL_DISABLED --cfg rustc_has_once_lock --cfg invalid_from_utf8_lint --cfg c_str_lit --cfg diagnostic_namespace --check-cfg 'cfg(Py_LIMITED_API)' --check-cfg 'cfg(Py_GIL_DISABLED)' --check-cfg 'cfg(PyPy)' --check-cfg 'cfg(GraalPy)' --check-cfg 'cfg(py_sys_config, values(\"Py_DEBUG\", \"Py_REF_DEBUG\", \"Py_TRACE_REFS\", \"COUNT_ALLOCS\"))' --check-cfg 'cfg(invalid_from_utf8_lint)' --check-cfg 'cfg(pyo3_disable_reference_pool)' --check-cfg 'cfg(pyo3_leak_on_drop_without_reference_pool)' --check-cfg 'cfg(diagnostic_namespace)' --check-cfg 'cfg(c_str_lit)' --check-cfg 'cfg(rustc_has_once_lock)' --check-cfg 'cfg(Py_3_7)' --check-cfg 'cfg(Py_3_8)' --check-cfg 'cfg(Py_3_9)' --check-cfg 'cfg(Py_3_10)' --check-cfg 'cfg(Py_3_11)' --check-cfg 'cfg(Py_3_12)' --check-cfg 'cfg(Py_3_13)'`\n     Running `rustc --crate-name pyo3_macros_backend --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-macros-backend-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"experimental-async\"))' -C metadata=c7424188824c71f8 -C extra-filename=-c7424188824c71f8 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern heck=/home/trent/src/tiktoken/target/release/deps/libheck-5e22b1dffa7f4255.rmeta --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rmeta --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rmeta --extern quote=/home/trent/src/tiktoken/target/release/deps/libquote-169332b6fe3d21d6.rmeta --extern syn=/home/trent/src/tiktoken/target/release/deps/libsyn-29d4a0ddbd98f61f.rmeta --cap-lints allow --cfg rustc_has_once_lock --cfg invalid_from_utf8_lint --cfg c_str_lit --cfg diagnostic_namespace --check-cfg 'cfg(Py_LIMITED_API)' --check-cfg 'cfg(Py_GIL_DISABLED)' --check-cfg 'cfg(PyPy)' --check-cfg 'cfg(GraalPy)' --check-cfg 'cfg(py_sys_config, values(\"Py_DEBUG\", \"Py_REF_DEBUG\", \"Py_TRACE_REFS\", \"COUNT_ALLOCS\"))' --check-cfg 'cfg(invalid_from_utf8_lint)' --check-cfg 'cfg(pyo3_disable_reference_pool)' --check-cfg 'cfg(pyo3_leak_on_drop_without_reference_pool)' --check-cfg 'cfg(diagnostic_namespace)' --check-cfg 'cfg(c_str_lit)' --check-cfg 'cfg(rustc_has_once_lock)' --check-cfg 'cfg(Py_3_7)' --check-cfg 'cfg(Py_3_8)' --check-cfg 'cfg(Py_3_9)' --check-cfg 'cfg(Py_3_10)' --check-cfg 'cfg(Py_3_11)' --check-cfg 'cfg(Py_3_12)' --check-cfg 'cfg(Py_3_13)'`\n   Compiling fancy-regex v0.13.0\n   Compiling regex v1.11.1\n   Compiling bstr v1.11.3\n     Running `rustc --crate-name regex --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-1.11.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"perf-backtrack\"' --cfg 'feature=\"perf-cache\"' --cfg 'feature=\"perf-dfa\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-onepass\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"pattern\", \"perf\", \"perf-backtrack\", \"perf-cache\", \"perf-dfa\", \"perf-dfa-full\", \"perf-inline\", \"perf-literal\", \"perf-onepass\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unstable\", \"use_std\"))' -C metadata=9ab83d1dbb2872e1 -C extra-filename=-9ab83d1dbb2872e1 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern aho_corasick=/home/trent/src/tiktoken/target/release/deps/libaho_corasick-6668c9f838fb89b3.rmeta --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --extern regex_automata=/home/trent/src/tiktoken/target/release/deps/libregex_automata-01135b6ed6d4413e.rmeta --extern regex_syntax=/home/trent/src/tiktoken/target/release/deps/libregex_syntax-66f570e05dbe3825.rmeta --cap-lints allow`\n     Running `rustc --crate-name fancy_regex --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/fancy-regex-0.13.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"perf\", \"std\", \"track_caller\", \"unicode\"))' -C metadata=2bd42caf041ad10c -C extra-filename=-2bd42caf041ad10c --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern bit_set=/home/trent/src/tiktoken/target/release/deps/libbit_set-40d90f83eb5bab57.rmeta --extern regex_automata=/home/trent/src/tiktoken/target/release/deps/libregex_automata-01135b6ed6d4413e.rmeta --extern regex_syntax=/home/trent/src/tiktoken/target/release/deps/libregex_syntax-66f570e05dbe3825.rmeta --cap-lints allow`\n     Running `rustc --crate-name bstr --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bstr-1.11.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"serde\", \"std\", \"unicode\"))' -C metadata=cc96e78f4e9fd97f -C extra-filename=-cc96e78f4e9fd97f --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --extern regex_automata=/home/trent/src/tiktoken/target/release/deps/libregex_automata-01135b6ed6d4413e.rmeta --cap-lints allow`\n   Compiling pyo3-macros v0.23.3\n     Running `rustc --crate-name pyo3_macros --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-macros-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"experimental-async\", \"multiple-pymethods\"))' -C metadata=97a13ca0f34dda72 -C extra-filename=-97a13ca0f34dda72 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rlib --extern pyo3_macros_backend=/home/trent/src/tiktoken/target/release/deps/libpyo3_macros_backend-c7424188824c71f8.rlib --extern quote=/home/trent/src/tiktoken/target/release/deps/libquote-169332b6fe3d21d6.rlib --extern syn=/home/trent/src/tiktoken/target/release/deps/libsyn-29d4a0ddbd98f61f.rlib --extern proc_macro --cap-lints allow`\n     Running `rustc --crate-name pyo3 --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"pyo3-macros\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"anyhow\", \"auto-initialize\", \"chrono\", \"chrono-tz\", \"default\", \"either\", \"experimental-async\", \"experimental-inspect\", \"extension-module\", \"eyre\", \"full\", \"generate-import-lib\", \"hashbrown\", \"indexmap\", \"indoc\", \"inventory\", \"macros\", \"multiple-pymethods\", \"nightly\", \"num-bigint\", \"num-complex\", \"num-rational\", \"py-clone\", \"pyo3-macros\", \"rust_decimal\", \"serde\", \"smallvec\", \"unindent\"))' -C metadata=99d0b007138eadf4 -C extra-filename=-99d0b007138eadf4 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern cfg_if=/home/trent/src/tiktoken/target/release/deps/libcfg_if-e4ded2c19830fbdd.rmeta --extern indoc=/home/trent/src/tiktoken/target/release/deps/libindoc-b4e94d9ecbd21a39.so --extern libc=/home/trent/src/tiktoken/target/release/deps/liblibc-6b0fbe5bd5ba9d30.rmeta --extern memoffset=/home/trent/src/tiktoken/target/release/deps/libmemoffset-65fe1a8a113b3005.rmeta --extern once_cell=/home/trent/src/tiktoken/target/release/deps/libonce_cell-05003007543cfa87.rmeta --extern pyo3_ffi=/home/trent/src/tiktoken/target/release/deps/libpyo3_ffi-9ad2a4f2678f35bd.rmeta --extern pyo3_macros=/home/trent/src/tiktoken/target/release/deps/libpyo3_macros-97a13ca0f34dda72.so --extern unindent=/home/trent/src/tiktoken/target/release/deps/libunindent-d53e2a0385a47a80.rmeta --cap-lints allow --cfg Py_3_7 --cfg Py_3_8 --cfg Py_3_9 --cfg Py_3_10 --cfg Py_3_11 --cfg Py_3_12 --cfg Py_3_13 --cfg Py_GIL_DISABLED --cfg rustc_has_once_lock --cfg invalid_from_utf8_lint --cfg c_str_lit --cfg diagnostic_namespace --check-cfg 'cfg(Py_LIMITED_API)' --check-cfg 'cfg(Py_GIL_DISABLED)' --check-cfg 'cfg(PyPy)' --check-cfg 'cfg(GraalPy)' --check-cfg 'cfg(py_sys_config, values(\"Py_DEBUG\", \"Py_REF_DEBUG\", \"Py_TRACE_REFS\", \"COUNT_ALLOCS\"))' --check-cfg 'cfg(invalid_from_utf8_lint)' --check-cfg 'cfg(pyo3_disable_reference_pool)' --check-cfg 'cfg(pyo3_leak_on_drop_without_reference_pool)' --check-cfg 'cfg(diagnostic_namespace)' --check-cfg 'cfg(c_str_lit)' --check-cfg 'cfg(rustc_has_once_lock)' --check-cfg 'cfg(Py_3_7)' --check-cfg 'cfg(Py_3_8)' --check-cfg 'cfg(Py_3_9)' --check-cfg 'cfg(Py_3_10)' --check-cfg 'cfg(Py_3_11)' --check-cfg 'cfg(Py_3_12)' --check-cfg 'cfg(Py_3_13)'`\n       Dirty tiktoken v0.8.0 (/home/trent/src/tiktoken): the toolchain changed\n   Compiling tiktoken v0.8.0 (/home/trent/src/tiktoken)\n     Running `rustc --crate-name _tiktoken --edition=2021 src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type cdylib --emit=dep-info,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=2d15c1d1b98ec97b --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern bstr=/home/trent/src/tiktoken/target/release/deps/libbstr-cc96e78f4e9fd97f.rlib --extern fancy_regex=/home/trent/src/tiktoken/target/release/deps/libfancy_regex-2bd42caf041ad10c.rlib --extern pyo3=/home/trent/src/tiktoken/target/release/deps/libpyo3-99d0b007138eadf4.rlib --extern regex=/home/trent/src/tiktoken/target/release/deps/libregex-9ab83d1dbb2872e1.rlib --extern rustc_hash=/home/trent/src/tiktoken/target/release/deps/librustc_hash-b006f4d81e95dfaf.rlib`\nwarning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n   --> src/lib.rs:508:16\n    |\n508 |         buffer.into_py(py)\n    |                ^^^^^^^\n    |\n    = note: `#[warn(deprecated)]` on by default\n\nwarning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n   --> src/lib.rs:555:38\n    |\n555 |         let py_completions = PyList::new_bound(\n    |                                      ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n   --> src/lib.rs:559:36\n    |\n559 |                 .map(|seq| PyList::new_bound(py, &seq[..])),\n    |                                    ^^^^^^^^^\n\nwarning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n   --> src/lib.rs:561:34\n    |\n561 |         (tokens, py_completions).into_py(py)\n    |                                  ^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:589:38\n    |\n589 |             Ok(bytes) => Ok(PyBytes::new_bound(py, &bytes).into()),\n    |                                      ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:596:32\n    |\n596 |             return Ok(PyBytes::new_bound(py, bytes).into());\n    |                                ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:599:32\n    |\n599 |             return Ok(PyBytes::new_bound(py, bytes).into());\n    |                                ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:611:31\n    |\n611 |             .map(|x| PyBytes::new_bound(py, x).into())\n    |                               ^^^^^^^^^\n\nwarning: `tiktoken` (lib) generated 8 warnings\n    Finished `release` profile [optimized] target(s) in 13.17s\nCopying rust artifact from target/release/lib_tiktoken.so to build/lib.linux-x86_64-cpython-313t/tiktoken/_tiktoken.cpython-313t-x86_64-linux-gnu.so\n(py313t) {pytorch} [12.6] [trent@dgx/ttypts/3(~s/tiktoken)%] python setup.py install\nrunning install\n/home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/setuptools/_distutils/cmd.py:79: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nrunning build\nrunning build_py\ncopying tiktoken/_educational.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/__init__.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/registry.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/model.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/load.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken/core.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\ncopying tiktoken_ext/openai_public.py -> build/lib.linux-x86_64-cpython-313t/tiktoken_ext\nrunning egg_info\nwriting tiktoken.egg-info/PKG-INFO\nwriting dependency_links to tiktoken.egg-info/dependency_links.txt\nwriting requirements to tiktoken.egg-info/requires.txt\nwriting top-level names to tiktoken.egg-info/top_level.txt\nreading manifest file 'tiktoken.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwarning: no files found matching 'Makefile'\nadding license file 'LICENSE'\nwriting manifest file 'tiktoken.egg-info/SOURCES.txt'\ncopying tiktoken/py.typed -> build/lib.linux-x86_64-cpython-313t/tiktoken\nrunning build_ext\nrunning build_rust\ncargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --\n       Fresh target-lexicon v0.12.16\n       Fresh unicode-ident v1.0.14\n       Fresh memchr v2.7.4\n       Fresh regex-syntax v0.8.5\n       Fresh autocfg v1.4.0\n       Fresh aho-corasick v1.1.3\n       Fresh heck v0.5.0\n       Fresh bit-vec v0.6.3\n       Fresh indoc v2.0.5\n       Fresh cfg-if v1.0.0\n       Fresh once_cell v1.20.2\n       Fresh unindent v0.2.3\n       Fresh rustc-hash v1.1.0\n       Fresh proc-macro2 v1.0.92\n       Fresh regex-automata v0.4.9\n       Fresh libc v0.2.169\n       Fresh bit-set v0.5.3\n       Fresh pyo3-build-config v0.23.3\n       Fresh quote v1.0.38\n       Fresh memoffset v0.9.1\n       Fresh bstr v1.11.3\n       Fresh fancy-regex v0.13.0\n       Fresh regex v1.11.1\n       Fresh syn v2.0.95\n       Fresh pyo3-macros-backend v0.23.3\n       Fresh pyo3-ffi v0.23.3\n       Fresh pyo3-macros v0.23.3\n       Fresh pyo3 v0.23.3\n       Fresh tiktoken v0.8.0 (/home/trent/src/tiktoken)\nwarning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n   --> src/lib.rs:508:16\n    |\n508 |         buffer.into_py(py)\n    |                ^^^^^^^\n    |\n    = note: `#[warn(deprecated)]` on by default\n\nwarning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n   --> src/lib.rs:555:38\n    |\n555 |         let py_completions = PyList::new_bound(\n    |                                      ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n   --> src/lib.rs:559:36\n    |\n559 |                 .map(|seq| PyList::new_bound(py, &seq[..])),\n    |                                    ^^^^^^^^^\n\nwarning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n   --> src/lib.rs:561:34\n    |\n561 |         (tokens, py_completions).into_py(py)\n    |                                  ^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:589:38\n    |\n589 |             Ok(bytes) => Ok(PyBytes::new_bound(py, &bytes).into()),\n    |                                      ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:596:32\n    |\n596 |             return Ok(PyBytes::new_bound(py, bytes).into());\n    |                                ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:599:32\n    |\n599 |             return Ok(PyBytes::new_bound(py, bytes).into());\n    |                                ^^^^^^^^^\n\nwarning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n   --> src/lib.rs:611:31\n    |\n611 |             .map(|x| PyBytes::new_bound(py, x).into())\n    |                               ^^^^^^^^^\n\nwarning: `tiktoken` (lib) generated 8 warnings\n    Finished `release` profile [optimized] target(s) in 0.03s\nCopying rust artifact from target/release/lib_tiktoken.so to build/lib.linux-x86_64-cpython-313t/tiktoken/_tiktoken.cpython-313t-x86_64-linux-gnu.so\nrunning install_lib\ncreating /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken_ext\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken_ext/openai_public.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken_ext\ncreating /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/_educational.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/__init__.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/py.typed -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/registry.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/model.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/load.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/_tiktoken.cpython-313t-x86_64-linux-gnu.so -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\ncopying build/lib.linux-x86_64-cpython-313t/tiktoken/core.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\nbyte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken_ext/openai_public.py to openai_public.cpython-313.pyc\nbyte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/_educational.py to _educational.cpython-313.pyc\nbyte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/__init__.py to __init__.cpython-313.pyc\nbyte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/registry.py to registry.cpython-313.pyc\nbyte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/model.py to model.cpython-313.pyc\nbyte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/load.py to load.cpython-313.pyc\nbyte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/core.py to core.cpython-313.pyc\nrunning install_egg_info\nCopying tiktoken.egg-info to /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken-0.8.0-py3.13.egg-info\nrunning install_scripts\n```\n\n\n\n:::\n\nAfter this, you should be able to import the `tiktoken` module in Python:\n\n```bash\n% cd ..\n% python -Xgil=0\nPython 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tiktoken\n>>>\n```\n\n#### Torch\n\nInstall PyTorch 2.6 via `pip` with the conda `py313t` environment active:\n\n```bash\n% conda activate py313t\n% pip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126\n```\n\nIf you have trouble installing PyTorch, consult their [Getting Started](\nhttps://pytorch.org/get-started/locally/) guide.\n\nYou can verify torch installed correctly as follows:\n\n```bash\n% python -Xgil=0\nPython 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> torch.cuda.is_available()\nTrue\n```\n\n#### IPython Kernel\n\nInstalling IPython Kernel allows us to use our free-threaded Python\ninstallation via the Jupyter Lab instance we install in the `py313`\nenvironment.\n\n```bash\nconda activate py313t\npip install ipykernel\n```\n\nOnce `ipykernel` is installed, run the following:\n\n```bash\n% python3.13t -m ipykernel --install --name py313t --user\nInstalled kernelspec py313t in /home/trent/.local/share/jupyter/kernels/py313t\n```\n\nThis will install a kernel configuration file, `kernel.json`, which we need\nto tweak by adding the `-Xgil=0` startup flag to the Python interpreter:\n\n```bash\n% cd ~/.local/jupyter/share/kernels/py313t\n% cp kernel.json kernel.json.orig\n% vi kernel.json\n# Edit kernel.json to make it look like the diff below.\n% diff -u kernel.json.orig kernel.json\n```\n\n```diff\n--- kernel.json.orig    2025-02-04 15:02:21.814112004 -0800\n+++ kernel.json 2025-02-04 15:02:36.553806199 -0800\n@@ -1,6 +1,7 @@\n {\n  \"argv\": [\n   \"/home/trent/mambaforge/envs/py313t/bin/python3.13t\",\n+  \"-Xgil=0\",\n   \"-Xfrozen_modules=off\",\n   \"-m\",\n   \"ipykernel_launcher\",\n@@ -12,4 +13,4 @@\n  \"metadata\": {\n   \"debugger\": true\n  }\n```\n\n::: {.hidden}\n\n(Hidden for now as we removed the immediate need for `datrie` by fixing the\nHTTP server routing logic.)\n\n#### Datrie and Cython\n\n[datrie](https://github.com/pytries/datrie) is a Python library that provides\na *trie* (or *digital search tree*) data structure by way of the [libdatrie](\nhttps://linux.thai.net/~thep/datrie/datrie.html) C library.  The Python\n`datrie` library isn't strictly necessary to run `parallelopedia.gpt2`, but\nother components rely on it, so it's handy to get installed now, if possible.\n\nIt relies upon Cython, and thus, for now, you need to install a free-threaded\ncompatible version of Cython first, as follows:\n\n```bash\nconda activate py313t\npip install git+https://github.com/cython/cython\n```\nThen, clone the `datrie` repo and install as follows:\n\n```bash\nconda activate py313t\ngit clone https://github.com/pytries/datrie --recursive\ncd datrie\npython setup.py build\npython setup.py install\n```\n\nIf everything goes well, you should see something like this when you launch\nPython and import `datrie`:\n\n```bash\n% python\nPython 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import datrie\n>>>\n```\n\n:::\n\n### Normal 3.13 Env (py313)\n\nThe second `py313` environment is almost identical to `py313t`, except it is\nnot a `python-freethreading` installation, and, additionally, we install\nJupyter Lab.  We can install `tiktoken` directly via `pip`, so we don't need\nthe supporting Rust cruft.\n\n```bash\nconda create -n py313 python=3.13 \\\n    nodejs pip tqdm flake8 jupyterlab requests \\\n        -c conda-forge\nconda activate py313\npip install numpy tiktoken safetensors\npip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126\n```\n\n## Parallelopedia\n\nAll of the code in this article is available in the [Parallelopedia](\nhttps://github.com/tpn/parallelopedia) repository on Github.  The code we'll\nbe focusing on in this post lives in the [parallelopedia.gpt2](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)\nmodule.\n\nThere is also a web user interface component named [Parallelopedia-UI](\nhttps://github.com/tpn/parallelopedia-ui), which we will use later in the\npost.\n\nClone the repositories as follows:\n\n```bash\n% git clone https://github.com/tpn/parallelopedia\n% git clone https://github.com/tpn/parallelopedia-ui\n```\n\n::: {.callout-important .env-vars collapse=\"false\" title=\"Important Environment Variables\"}\n\nThe code and command examples in this post will assume you've added the\n`src` directory to your `PYTHONPATH`, the `bin` directory to your `PATH`,\nand set the `PARALLELOPEDIA_ROOT` environment variable to the root of the\nrepository.  You can do this as follows:\n\n```bash\ncd parallelopedia\nexport PYTHONPATH=$(pwd)/src:$PYTHONPATH\nexport PATH=$(pwd)/bin:$PATH\nexport PARALLELOPEDIA_ROOT=$(pwd)\ncd ..\ncd parallelopedia-ui\nexport PARALLELOPEDIA_UI=$(pwd)\n```\n\nIt is recommended that you add these to your shell.  For me, using zsh, I\nuse the following:\n\n```zsh\nexport PARALLELOPEDIA_ROOT=~s1/parallelopedia\nexport PARALLELOPEDIA_UI=~s1/parallelopedia-ui\nexport PYTHONPATH=$PARALLELOPEDIA_ROOT/src:$PYTHONPATH\nexport PATH=$PARALLELOPEDIA_ROOT/bin:$PATH\n```\n\n:::\n\nYou can perform a quick sanity check that things are working as follows:\n\n```bash\n% python -Xgil=0 -m parallelopedia.http.server --help\nusage: server.py [-h] [--ip IP] [--port PORT] [--debug] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--threads THREADS] [--protocol-class PROTOCOL_CLASS] [--app-classes APP_CLASSES [APP_CLASSES ...]] [--listen-backlog LISTEN_BACKLOG]\n\nRun the HTTP server.\n\noptions:\n  -h, --help            show this help message and exit\n  --ip IP               IP address to bind the server to.\n  --port PORT           Port number to bind the server to.\n  --debug               Enable debug mode for asyncio.\n  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}\n                        Set the logging level.\n  --threads THREADS     Number of threads to use.\n  --protocol-class PROTOCOL_CLASS\n                        The protocol class to use for the server.\n  --app-classes APP_CLASSES [APP_CLASSES ...]\n                        Space-separated list of HTTP application classes.\n  --listen-backlog LISTEN_BACKLOG\n                        The listen backlog for the server.\n\n```\n:::\n\n# PyTorch and LLM Crash Course\n\nMy involvement with PyTorch and Large Language Models (LLMs) started around\nlate November last year, 2024.  Going in, I knew nothing about PyTorch, nor\ndeep neural networks, nor LLMs---other than having enjoyed using LLMs\nthoroughly the past couple of years.  I had never trained an AI model of\nany kind.  I did have a bit of NumPy and data science exposure up my sleeve,\nplus general familiarity with Python.\n\nThanks to [Andrej Karpathy](https://karpathy.ai/)'s phenomenal YouTube series\non deep neural networks and LLMs titled [Neural Networks: From Zero to Hero](\nhttps://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ),\nover the course of about 3 weeks or so I went from zero to... well I\nwouldn't necessarily say *hero*---perhaps zero to *not-completley-clueless*\nis more apropos.\n\nAndrej's content is a fantastic resource to learn everything you need to\nknow to understand how modern LLMs work from the ground-up.  It's not a\nshort series---there are 19 hours, 21 minutes and two seconds of content\nacross ten videos---and you'll probably spend double that if you *really*\nwant to properly absorb the content.\n\nNone of the work presented in this post would have been possible had I not\ninvested the time in Andrej's series.  If you're reading this Andrej, thanks,\nand keep up the brilliant work!\n\n## Training GPT-2 (124M) Locally\n\nEquipped with my new knowledge about LLMs, PyTorch, and, thanks to Andrej's\nfinal video in the series titled [Let's reproduce GPT-2 (124M)](\nhttps://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)\nand the accompanying [build-nanogpt](\nhttps://github.com/karpathy/build-nanogpt) Github repo, I was able to train\na local GPT-2 model via PyTorch, from scratch, using the [edu_fineweb10B](\nhttps://huggingface.co/rhysjones/gpt2-124M-edu-fineweb-10B) dataset.\n\nI only had to make one change in order to run locally:\n[Use 8 for micro batch size instead of 64](\nhttps://github.com/tpn/build-nanogpt/commit/0069c4a35d1a362c1fd50f9f5bce00a170e15904).\nWith that change in place, I was able to train GPT-2 from scratch as follows:\n\n```bash\n% conda activate py313\n% git clone gh:tpn/build-nanogpt\n% cd build-nanogpt\n# Download the fineweb dataset.\n% python fineweb.py\n# Train!\n% torchrun --standalone --nproc_per_node=4 train_gpt2.py\n```\n\nThis was run on an NVIDIA DGX workstation from 2017, which has an Intel\nXeon(R) CPU E5-2698 v4 @ 2.20GHz (20 cores, 40 threads), and four Tesla\nV100-DGXS-32GB GPUs.\n\nTraining in parallel across all four GPUs yielded around 36,000 tokens/sec,\nwith an average time of about 14.5 seconds per loop iteration.  Training\ntook about 3 days and 5 hours for 19,072 steps.  All four GPUs were pegged\nclose to their 300W power limit for those three days.\n\n::: {.callout-note}\n\nAmusingly, well after the fact, I decided to see what kind of training\nperformance I'd get on my Windows 11 gaming box (via WSL2 and Ubuntu 22.04),\nwhich has an AMD Ryzen 9 7950X3D (16 cores, 32 threads) and NVIDIA RTX 4090.\nTraining via `python train_gpt2.py` (`torchrun` wasn't needed as I wasn't\nusing multiple GPUs) yielded about 125,000 tokens/sec, and the loop iteration\ntime averaged about 4.2 seconds.\n\nOn my work Windows 11 box, which has an Intel Core i9-14900K (24 cores, 32\nthreads) and NVIDIA RTX 5090, I got about 195,000 tokens/sec, and the loop\niteration averaged about 2.7 seconds.\n\nSo training 19,072 steps on my 5090 box would have taken around ~14 hours or\nso, and, at the very least, would have reduced my electricity bill for that\nmonth by a decent bit :-)\n\n:::\n\nOnce training completes, a `log/model_19072.pt` file is produced, which is\nthe checkpoint of the model at that final step, obtained via a call to\n`torch.save()`.  The model has 124M parameters---which is tiny by modern\nstandards---and is just under 500MB on disk.\n\nYou can download that very model I trained via the HuggingFace dataset I set\nup here: [model_19072.pt](\nhttps://huggingface.co/datasets/trentnelson/parallelopedia-data-gpt2/blob/main/model_19072.pt).\nOnce downloaded, place the file in `$PARALLELOPEDIA_ROOT/data`;\nalternatively, if you run the Jupyter Notebook below, it'll automatically\ndownload the model from HuggingFace on first run.\n\n# PyTorch GPT-2 Implementation\n\nLet's introduce the first version of the Python code we're going to use.\nAgain, all of this has been made possible thanks to Andrej Karpathy's work\nwith his [YouTube series](\nhttps://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)\nand [build-nanogpt](\nhttps://github.com/karpathy/build-nanogpt) repo, so any and all code you see\nin this post can typically be traced back to something equivalent that appears\nin [train_gpt2.py](\nhttps://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py).  None\nof this code would have made any sense to me a month or two ago---but I can\npromise you that if you devote sufficient time to watching and understanding\nthe entire series, you'll have a comprehensive understanding of all of the\npieces present in this post.\n\nYou can follow along in a Jupyter Lab notebook if you activate the `py313`\nenvironment and launch `jupyter lab`.  If you correctly registered your\n`py313t` kernel per the instructions earlier, you should see an option when\ncreating a new notebook to use the `py313t` Python kernel, which will be the\nfree-threaded version.  On the right of this page you should see all of the\nnotebooks referenced.\n\n## Initial Implementation\n\nThe code below roughly corresponds to my first version of the code in the\ncommit [3ed4fe6: Add gpt2.py](\nhttps://github.com/tpn/parallelopedia/blob/3ed4fe60a767a12b31fca183fed00fef43c65827/src/parallelopedia/gpt2.py),\nwith some formatting and style tweaks to ensure the code is viewable on mobile\ndevices without requiring horizontal scrolling.\n\nWe'll revise this code later in the post, but for now, it's a good starting\npoint to get a feel for how we can use PyTorch to load a GPT-2 model\ncheckpoint, tokenize some input text, and generate some output text.\n\n::: {#initial-model-setup-code}\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-setup >}}\n\n\n\n\n\n:::\n\n## Loading the Model\n\nWith the code above executed in a preceding Jupyter Notebook cell, we can\nload the model as follows:\n\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-load-model >}}\n\n\n\n\n\n\n## Generating Text\n\nOnce we've got a model instance, text can be generated by simply calling\nthe model's `generate()` function with a prompt, and, optionally, some\nadditional parameters like seed and max length.  This is also referred to\nas inference---the two terms are interchangeable, and mean the same thing:\nthe act of providing some input tokens---your encoded prompt---to your\ntrained model, and having it generate tokens in response.\n\nNote that this isn't a *chat* or *instruction* model, nor has it been\nfine-tuned to remotely resemble something actually usable.  So you can't\nask it questions or have it write code for you.\n\nWhat you can do, though, is provide it with a half written sentence, and\nthen laugh at the ridiculous content it generates in response.  Although\nnote that its syntax is pretty good---the model has clearly learned enough\nduring training about how the English language is structured, which words\nmake sense when placed together, that sort of thing.  It just has no clue\nabout underlying semantics.\n\n::: {.desktop-only}\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-generate-1-desktop >}}\n\n\n\n\n\n:::\n\n::: {.tablet-only}\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-generate-1-tablet >}}\n\n\n\n\n\n:::\n\n::: {.phone-only}\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-generate-1-phone >}}\n\n\n\n\n\n:::\n\nNow, it's worth noting at this point that a 124 million parameter GPT2 model,\ntrained from scratch for 19,072 iterations on the `edu_fineweb10b` data set,\nwith a final loss score of 3.052, is, quite frankly, hot garbage :-)\n\nDo not be expecting output from this model to rival anything close to what\nyou'd expect from a contemporary LLM.  In fact, we can't even rely on it to\neven remotely generate content that is factual in nature.  It spews hot\nprobabilisitic garbage that mostly conforms to the structure of the English\nlanguage.\n\nBut at least it's *our* hot garbage that we trained from nothing, and it's\nall we need to start playing around with generating text in parallel.\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Hot Garbage Example\"}\n\nProvide a slightly different seed to the prompt above and you'll precisely\nsee some hot garbage generation in action.  In the following example, with\nan identical prompt as the one prior, it just streams nonsense until hitting\nits max token limit, nary a stop token in sight.\n\n::: {.desktop-only}\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-generate-2-desktop >}}\n\n\n\n\n\n:::\n\n::: {.tablet-only}\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-generate-2-tablet >}}\n\n\n\n\n\n:::\n\n::: {.phone-only}\n\n\n\n{{< embed gpt2-v1.ipynb#gpt2-v1-generate-2-phone >}}\n\n\n\n\n\n:::\n\nlolwat.\n\n:::\n\n# Parallel PyTorch Inference\n\nNow that we've got generation working, let's tackle the fun part:\ninvestigating whether or not PyTorch model inference can be done\nsimultaneously, in parallel, by multiple threads, running on multiple cores\nat the same time, in a single free-threaded Python process.  And ideally, we\nshould only need one GPU, with all of these threads *sharing* it as fairly\nas possible.  Although if we have multiple GPUs, we should be able to\ndistribute the incoming work evenly across those too, if we want.\n\nThis is novel, uncharted territory we're able to now explore thanks to\nthe free-threaded version of Python.\n\n::: {.callout-note title=\"Simultaneous vs Parallel vs Concurrent\"}\n\nThe phrasing I've used above---*\"simultaneously, in parallel\"*---is a bit\nredundant.  They both imply the same thing.  When I use either word in this\npost, I'm explicitly referring to the new ability unlocked by free-threaded\nPython, where multiple threads can be running Python code on different CPU\ncores at the same time---i.e. *simultaneously*.  And thus, you're performing\nwork in *parallel*.\n\nWhen I use the term *concurrent* or *concurrency*, I'm using it in the\ntraditional sense within the context of Python: making progress on multiple\nthings at a time.  This term is well suited to describe things like\nweb servers, where a single Python process, with a single thread, running on\na single CPU core, can service multiple clients at any given time (by way\nof [non-blocking socket I/O and event multiplexing](\nhttps://speakerdeck.com/trent/pyparallel-how-we-removed-the-gil-and-exploited-all-cores?slide=27)),\nbut it's not servicing any of those clients *simultaneously* on different\ncores, because that would require multiple threads running in *parallel*.\n\nSome more details regarding *parallelism* vs *concurrency* can be found on\n[slide\n8](\nhttps://speakerdeck.com/trent/parallelism-and-concurrency-with-python?slide=8)\nof a deck I put together many years ago titled\n[Parallelism and Concurrency in Python](\nhttps://speakerdeck.com/trent/parallelism-and-concurrency-with-python).\n\n:::\n\nSo how do we test this out?  I guess we could spin up some threads and have\nthem all call `model.generate()`, but that's a little boring.\n\nWhy not try implement a pure Python, multi-threaded `asyncio`-based HTTP\nserver, expose a `/generate`-esque style `GET` endpoint, and wire that up\nto the model generation code we saw above, allowing us to serve web requests\nfor generation in parallel, ideally in an `asyncio`-friendly manner, where\nwe can stream individual tokens back one-by-one via HTTP's chunked-encoding\ntransfer protocol, giving each thread's event loop the ability to service\nmultiple clients *concurrently* in a reasonably fair manner, whilst also\nservicing many clients in *parallel* across all threads, and for kicks, get\nAI to whip up a janky little React Bootstrap UI that we can slap in front of\nit all to test it?\n\n## Pure Python Multi-threaded HTTP Server\n\nFirst thing we need is a nice and simple `asyncio`-based HTTP server, that\nalso happens to work with multi-threading now that we have a free-threaded\nPython at our disposal.\n\nLuckily, I have one of those laying around already!  In support of another\narticle I'm actively working on (which was meant to get published before\nthis post), I ported the [HTTP Server](\nhttps://github.com/pyparallel/pyparallel/blob/branches/3.3-px/Lib/async/http/server.py)\nI wrote many years ago for [PyParallel](https://pyparallel.org)^[And I'm\nsure I used the existing Python stdlib `http.server` code at the time as the\nbasis; ain't nobody got time to be writing new web servers from scratch.] to\nuse the new `asyncio` facilities available with Python, and then slapped\nmultiple threads on it, where each thread gets its own `asyncio` event loop.\n\nTurns out, thankfully, that this Just Works, at least on\nLinux^[Unfortunately, it doesn't appear to work on Windows as-is; using the\nexact same code, only one thread can be seen running when the server is\nloaded.  It's not doing a round-robin between *all* threads, like you'd\nexpect to see with the GIL enabled, there's just a single sole thread\nattempting to service all incoming requests, with all other threads sitting\nidle.  I don't know if it's because of something quirky with regards to\nadditional, non-main-thread threads not getting their own event loop\n(hopefully easy to fix), or something more insidious related to how we're\nmisuing I/O completion ports behind the scenes in `IocpProactor()` now that\nwe have free-threading (much harder to fix).  I haven't had time to\ninvestigate in more detail.]---we can now have a pure Python HTTP server,\nrunning in a single Python process, that'll happily saturate every CPU core\nunder load.\n\nThe server code lives in [`parallelopedia.http.server`](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/http/server.py),\nand it includes a super janky little notion of *\"HTTP Apps\"*, the purpose of\nwhich can be best demonstrated with a simple example:\n\n```python\nclass PlaintextApp(HttpApp):\n    @route\n    def plaintext(self, request):\n        response = text_response(request, 'Hello, World!')\n        self.server.send_response(response)\n\nclass SleeperApp(HttpApp):\n    @route\n    def sleep(self, request, seconds):\n        time.sleep(int(seconds))\n        msg = f'Slept for {seconds} seconds.')\n        response = text_response(request, msg)\n        self.server.send_response(response)\n\n# Create a server with the two apps.\napp_classes=[PlaintextApp, SleeperApp]\nserver = HttpServer(app_classes=app_classes)\n```\n\nIn the above example, the HTTP server will route requests for the\n`/plaintext` endpoint to an instance of the `PlaintextApp`'s\n`plaintext()` routine, and `/sleep` requests get routed to the\n`SleeperApp`'s `sleep()` routine.\n\nThe *\"slapped multiple threads on it\"* activity I refered to earlier is\nhandled by some new [async and threading scaffolding](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/http/server.py#L1344)\nadded to the bottom of that module, with the pertinent pieces reproduced\nbelow:\n\n```python\n\nasync def main_async(\n    args: argparse.Namespace,\n    protocol_class: type,\n    *protocol_args: Tuple\n) -> None:\n    \"\"\"\n    This is the main function for the server when it is running in\n    asynchronous mode.  It will create a server instance and then\n    call serve_forever() on it.\n\n    Arguments:\n\n        args (argparse.Namespace): Supplies the command-line arguments.\n\n        protocol_class (type): Supplies the protocol class to use.\n\n        protocol_args (tuple): Supplies the arguments to pass to the\n            protocol class constructor.\n\n    \"\"\"\n    loop = asyncio.get_running_loop()\n\n    if os.name in ('nt', 'cygwin'):\n        reuse_port = False\n    else:\n        reuse_port = True\n\n    reuse_address = True\n\n    server = await loop.create_server(\n        lambda: protocol_class(*protocol_args),\n        args.ip,\n        args.port,\n        backlog=args.listen_backlog,\n        reuse_address=reuse_address,\n        reuse_port=reuse_port,\n    )\n\n    async with server:\n        await server.serve_forever()\n\n\ndef start_event_loop(\n    args: argparse.Namespace,\n    protocol_class: type,\n    *protocol_args: Tuple\n) -> None:\n    \"\"\"\n    This function will start the asyncio event loop and run\n    the main_async() function.  It is intended to be the\n    target of a threading.Thread.\n\n    Arguments:\n\n        args (argparse.Namespace): Supplies the command-line arguments.\n\n        protocol_class (type): Supplies the protocol class to use.\n\n        protocol_args (tuple): Supplies the arguments to pass to the\n            protocol class constructor.\n    \"\"\"\n    asyncio.run(\n        main_async(\n            args,\n            protocol_class,\n            *protocol_args,\n        ),\n        debug=args.debug,\n    )\n\n\ndef main_threaded_multi_accept(\n    args: argparse.Namespace,\n    protocol_class: type,\n    *protocol_args: Tuple\n) -> None:\n    \"\"\"\n    This is the main function for the server when it is running in\n    multi-threaded mode with multiple accept sockets.  Each thread\n    will have its own asyncio loop issue a create_server() call for\n    the given host/port and protocol.\n\n    Arguments:\n\n        args (argparse.Namespace): Supplies the command-line arguments.\n\n        protocol_class (type): Supplies the protocol class to use.\n\n        protocol_args (tuple): Supplies the arguments to pass to the\n            protocol class constructor.\n    \"\"\"\n    import threading\n\n    threads = []\n    for _ in range(args.threads):\n        thread = threading.Thread(\n            target=start_event_loop,\n            args=(args, protocol_class, *protocol_args),\n        )\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n\ndef main(args: Optional[argparse.Namespace] = None):\n    \"\"\"\n    Main entry point for parallelopedia.http.server module.\n    \"\"\"\n    args = parse_arguments()\n\n    logging.basicConfig(\n        level=getattr(logging, args.log_level),\n        format='%(asctime)s - %(levelname)s - %(message)s',\n    )\n\n    # Use multiple threads to load the application classes.\n    app_classes = get_classes_from_strings_parallel(\n        args.app_classes,\n    )\n\n    protocol_class = get_class_from_string(args.protocol_class)\n    protocol_args = (app_classes,)\n\n    if args.threads == 1:\n        asyncio.run(\n            main_async(\n                args,\n                protocol_class,\n                *protocol_args,\n            ),\n            debug=args.debug,\n        )\n    else:\n        main_threaded_multi_accept(\n            args,\n            protocol_class,\n            *protocol_args,\n        )\n\nif __name__ == '__main__':\n    main()\n```\n\n## GPT2 HTTP App\n\nWith the HTTP server scaffolding in place, we can now whip up a little\n`Gpt2App` class that has a `generate()` method.  Incoming requests to\nthe `/generate` endpoint will be routed to that routine by the server.\n\n### Synchronous Up-Front Generation\n\nNow, we could take the simple approach, where the `Gpt2App.generate()`\ncall goes off and calls `model.generate()` and then patiently waits for the\nentire response to be generated before sending anything back to the user.\n\nThat code would look something like this:\n\n```python\nclass Gpt2App(HttpApp):\n    @route\n    def generate(self, request: Request,\n                 *args: List,\n                 **kwds: Dict) -> None:\n        prompt = args[0]\n        model = get_model()\n        result = model.generate(prompt=prompt)\n        respose = text_response(request, result)\n        self.server.send_response(response)\n```\n\nBut when have you ever interacted with an LLM via a web interface where it\nwaits until it generates *all* of the response up-front before sending it\nback to you?  Never; you can see it generate the response in real time, and\nthat's what we want to mimic here in this experiment.\n\n### Our Goals\n\nThe high-level goals for our solution are thus:\n\n1. We want to send a generated token back to the user as soon as it becomes\n   available.\n\n2. We want to ensure the client receiving the token can display it as soon\n   as they receive it---so we need to be cognizant of what HTTP transfer\n   protocol we use to send bytes back.  If we just used normal HTTP transfer\n   encoding, the client would wait until we've sent *everything* before the\n   user sees it, despite the fact that we've been trickling individual\n   tokens to them the entire time.\n\n3. We want to play nicely with the `asyncio` ecosystem upon which our\n   hosting HTTP server is based---so we need to be cognizant of the current\n   thread's event loop, and make sure we don't impede that thread's ability\n   to *concurrently* serve other clients that are being handled by the event\n   loop.\n\nThankfully, as we saw earlier with the implementation of the\n`GPT.generate()` routine, generating tokens in response to a prompt is\ninherently a *token-by-token* process.  So the algorithm at least provides\nus with the means to obtain a single token at a time, which takes care of\nthe first point.\n\nSecond, HTTP's chunked-encoding transfer protocol will allow a HTTP client\nto immediately *\"see\"* the tokens we send back to it as soon as we send\nthem, provided we enable `TCP_NODELAY` on the underlying socket to ensure\nthe operating system sends the bytes out to the client as soon as we send\nthem.\n\n::: {.callout-note}\n\nIf we didn't do this, the default behavior of [Nagle's algorithm](\nhttps://en.wikipedia.org/wiki/Nagle%27s_algorithm) would apply, and the\noperating system would delay sending individual bytes back when we request,\nin the hope that it can accumulate more bytes to send all at once at a\nslightly later point in time.  This is advantageous for maximizing\nthroughput, but it impedes latency, and in our case, we want the lower\nlatency afforded by immediately sending the bytes back to the client as soon\nas we generate them.\n\n:::\n\n[Chunked-encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding)\nworks by setting an HTTP response header `Transfer-Encoding: chunked`, and\nthen in the body, each chunk is transmitted by its length and then the chunk\nitself.  The server communicates to the client that the transfer has\ncompleted once a zero-length chunk is received.\n\nSo, as long as we send our tokens back via chunked-encoding, any HTTP/1.1\nclient will be able to reassemble them back into the generated text, giving\nthe visual appearance of real time model generation.  That will take care\nof the second point.\n\nLastly, in order to play nice within the greater `asyncio` ecosystem, we\nneed to give control back to the underlying thread's `asyncio` event loop\nafter we generate a token and yield a decoded text fragment, which can\nthankfully be done via a simple call to `await asyncio.sleep(0)`, provided\nwe're generating text from the model from within an `async` callback.\n\nThis ensures multiple *concurrent* clients being handled by our thread's\nevent loop will be handled fairly; they'll all receive generated tokens\nat the same rate.\n\n### Asynchronous Token-by-Token Generation\n\nThe first thing we need to do is to change our `Gpt2App.generate()` call\ninto something that is `async` compatible, in anticipation of some later\ncode that we write needing to issue an `await asyncio.sleep(0)`, which\ncan only be done within a call frame of an asynchronous method.\n\nWhen our `Gpt2App.generate()` routine is called, we're still within the\ncontext of the `asyncio` protocol's `data_received()` routine, which is a\nnormal, synchronous method, *not* an enlightened `async` method that can\nparticipate in an `asyncio` event loop.\n\nSo, in order to transition from a synchronous callback to an asynchronous\none, we can use the current event loop's `create_task()` routine to enqueue\nan `async` method for execution.\n\n#### Step 1: Have generate() enqueue an async generate_response().\n\nThus, our `Gpt2App.generate()` call will look something like this:\n\n```python\n\nclass Gpt2App(HttpApp):\n\n    ...\n\n    @route\n    def generate(self, request: Request,\n                 *args: List,\n                 **kwds: Dict) -> None:\n\n        # Extract the \"prompt\" provided in the incoming request.\n        text = args[0]\n\n        # Obtain the event loop and schedule the response\n        # generation via our async generation coroutine.\n        # We have to do it like this as at this point we're\n        # still within the call frame of the data_received()\n        # protocol callback, which isn't an async function.\n        loop = asyncio.get_running_loop()\n        async_task = self.generate_response(request, text, **kwds)\n        loop.create_task(async_task)\n```\n\n#### Step 2: Implement an async generate_response()\n\nOur asynchronous `generate_response()` routine will be the bridge between\ngenerating tokens from the model, and sending those tokens back to the\nclient via chunked-encoding.\n\nIt is responsible for preparing the response to use chunked-encoding,\nand then enabling `TCP_NODELAY` on the socket.\n\nThen, assuming that our model has an `async_generate_for()` routine,\nwhich we'll implement in the next step, we perform an `async for` loop\nover that routine in order to obtain individual *decoded* tokens.  As\nsoon as we receive a token, we can send it back to the client via the\n`response` object's `send_chunk()` routine.\n\nOnce we've exhausted the async generator (i.e. it either generated the\nmaximum number of requested tokens, or it encountered a stop token), we\ncan re-enable `TCP_NODELAY`, and then return.\n\nA simplified version of the Python code is presented below.  I have omitted\nmost of the error handling and query parameter parsing code for simplicity;\nsee the expandable code block at the end for the full version.\n\n```python\n\nclass Gpt2App(HttpApp):\n\n    ...\n\n    async def generate_response(\n        self, request: Request,\n        prompt: str,\n        **kwds: Dict\n    ) -> None:\n\n        # Prepare a chunked-encoding response.\n        response = request.response\n        response.code = 200\n        response.message = 'OK'\n        response.chunked_response = True\n        response.content_type = 'text/plain'\n\n        # Obtain the model.\n        model = get_model()\n\n        # We want to enable TCP_NODELAY for the duration of\n        # the response.  This ensures packets are sent\n        # immediately without any internal buffering.\n        try:\n            response.enable_tcp_nodelay()\n            enabled_nodelay = True\n        except Exception as e:\n            logging.error(f'Error enabling TCP_NODELAY: {e}')\n            enabled_nodelay = False\n\n        # Write the chunked header immediately.\n        response_bytes = bytes(response)\n        if not self.write(response_bytes):\n            # Encountered a disconnect, return.\n            return\n\n        # N.B. From herein, all data must be transferred to\n        #      the client via chunked encoding with the\n        #      `response.send_chunk()` routine.\n\n        # Send the initial prompt text.\n        response.send_chunk(prompt)\n\n        # Obtain decoded tokens from the model one at a time\n        # via an `async for` loop, sending the token back to\n        # the client as soon as it's available.\n        async for decoded_token in model.generate_async_for(prompt):\n            response.send_chunk(decoded_token)\n\n        # Terminate the chunked-encoding response.\n        response.end_chunks()\n\n        # Disable TCP_NODELAY now that the response is complete.\n        # The reasoning behind this is that the client may have\n        # issued the HTTP request with a keep-alive header, and\n        # plans on reusing this connection for a different request\n        # next, which won't necessarily want `TCP_NODELAY` active.\n        if enabled_nodelay:\n            response.disable_tcp_nodelay()\n```\n\n::: {.callout-note #gpt2app-generate-response-full collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Full Code for async Gpt2App.generate_response()\"}\n\nThe actual code has more robust error-handling facilities and support for\nextracting the query string parameters from the incoming request URI and\nconverting them into keyword arguments suitable for passing to the model.\n\nAdditionally, we haven't touched on how we initialize or obtain instances of\nour models yet, so the model-related code won't make much sense until later\nin the article.\n\n```python\n\nclass Gpt2App(HttpApp):\n    routes = make_routes()\n    route = router(routes)\n\n    def __init__(self, server: HttpServer) -> None:\n        super().__init__(server)\n        self.printable = PRINTABLE\n\n    def is_connected(self):\n        # server.transport will be severed when the client\n        # disconnects, so we can use this to determine if\n        # the client is still connected.\n        server = self.server\n        transport = None\n        try:\n            transport = server.transport\n        except AttributeError:\n            pass\n        return transport is not None\n\n    def write(self, response_bytes):\n        server = self.server\n        transport = None\n        try:\n            transport = server.transport\n        except AttributeError:\n            pass\n        if transport is not None:\n            transport.write(response_bytes)\n            return True\n        else:\n            return False\n\n    async def generate_response(\n        self, request: Request, prompt: str, **kwds: Dict\n    ) -> None:\n\n        response = request.response\n\n        response.code = 200\n        response.message = 'OK'\n        response.chunked_response = True\n        response.content_type = 'text/plain'\n\n        if kwds is None:\n            kwds = {}\n        max_length = min(int(kwds.get('max_length', 100) or 100), 1024)\n        top_k = min(int(kwds.get('top_k', 50) or 50), 50)\n        seed = kwds.get('seed', None)\n        if seed:\n            try:\n                seed = int(seed)\n            except ValueError:\n                seed = None\n        if not seed:\n            seed = random.randint(0, 2**32 - 1)\n\n        device = kwds.get('device', None)\n\n        model_name = kwds.get('model', None)\n        if model_name == 'gpt2-xl':\n            models = PRETRAINED_MODELS\n            get_next = get_next_pretrained_model\n        else:\n            model_name = 'gpt2'\n            models = MODELS\n            get_next = get_next_model\n\n        model = None\n        if device is not None:\n            if device == 'cpu':\n                model = models[-1]\n            elif device.startswith('cuda:'):\n                try:\n                    index = int(device[5:])\n                except ValueError:\n                    index = -1\n                if index < 0 or index >= NUM_GPUS:\n                    index = -1\n                if index != -1:\n                    model = models[index]\n            elif device == 'cuda':\n                model = models[random.randint(0, NUM_GPUS - 1)]\n\n        if not model:\n            # Get a model.  If there are multiple models available, e.g. if we\n            # have multiple GPUs, this will balance the load a bit.\n            model = get_next()\n\n        expose_headers = (\n            'Access-Control-Expose-Headers: '\n            'X-Max-Length, '\n            'X-Top-K, '\n            'X-Seed, '\n            'X-Model-Name, '\n            'X-Model-Device'\n        )\n        response.other_headers.extend([\n            expose_headers,\n            f'X-Max-Length: {max_length}',\n            f'X-Top-K: {top_k}',\n            f'X-Seed: {seed}',\n            f'X-Model-Name: {model_name}',\n            f'X-Model-Device: {model.device}',\n        ])\n\n        # We want to enable TCP_NODELAY for the duration of\n        # the response.  This ensures packets are sent\n        # immediately without any internal buffering.\n        try:\n            response.enable_tcp_nodelay()\n            enabled_nodelay = True\n        except Exception as e:\n            logging.error(f'Error enabling TCP_NODELAY: {e}')\n            enabled_nodelay = False\n\n        # Write the chunked header immediately.\n        response_bytes = bytes(response)\n        if not self.write(response_bytes):\n            # Encountered a disconnect, return.\n            return\n\n        # N.B. From herein, all data must be transferred to\n        #      the client via chunked encoding with the\n        #      `response.send_chunk()` routine.\n\n        # Send the initial prompt text.\n        response.send_chunk(prompt)\n\n        # Obtain an async generator instance to the model's\n        # new async token generation routine.\n        generate_tokens = model.generate_async_for(\n            prompt,\n            max_length=max_length,\n            top_k=top_k,\n            seed=seed,\n        )\n        async for decoded_token in generate_tokens:\n            if decoded_token == -1:\n                # A non-printable token was generated,\n                # terminating generation.\n                response.send_chunk(\n                    OOPS_NON_PRINTABLE_ENCOUNTERED\n                )\n                break\n\n            # If the client has forcibly disconnected,\n            # terminate generation.\n            if not self.is_connected():\n                break\n\n            # Otherwise, send the decoded token to the client\n            # via chunked encoding.\n            response.send_chunk(decoded_token)\n\n        # Send the termination chunk.  This may fail at the\n        # socket.send() level if the client has already\n        # disconnected, which is harmless.\n        response.end_chunks()\n\n        # Disable TCP_NODELAY now that the response is complete.\n        # Again, this may fail at the socket level if the client\n        # has already disconnected, which is harmless.\n        if enabled_nodelay:\n            try:\n                response.disable_tcp_nodelay()\n            except Exception as e:\n                msg = f'Error disabling TCP_NODELAY: {e}'\n                logging.error(msg)\n\n```\n\n:::\n\n\n#### Step 3: Implement an async GPT.async_generate_for()\n\nIn the code example above, we assumed the `GPT` model we've been using had\ngrown a new `async` routine named `async_generate_for()`, which we'll cover\nnow.\n\nThis routine is essentially an `asyncio`-friendly version of the original\n`generate()` routine we wrote.  It shares a lot of the same code, with a few\nnotable tweaks in order to support the fact that it is being called from a\ncallback that was enqueued on a thread's `asyncio` event loop, and it is\nexpected to yield a token as soon as it is available, and then pass control\nback to the event loop in order for it to service other clients before it\ncontinues with generating the next token.\n\nIt also has the notion of checking for *\"printable\"* characters.  This came\nabout when I was initially testing this code via `curl` which would\nsometimes balk and exit in the middle of streaming the response, citing that\nit encountered corrupted data or something like that.\n\nAfter investigation, it turned out that sometimes, for whatever reason, the\nmodel just generates a junk, nonsensical token (like 65534, which is well\noutside the highest token number of 50384).  I have no idea why it happens,\nalthough I'll note it happens on the OpenAI GPT2 XL model available on\nHuggingFace (which we'll discuss later) too, so, eh.\n\nI deal with this by checking if we've generated a non-printable token after\ndecoding it, and, if so, return -1 and terminate the loop.  The\n[full version](#gpt2app-generate-response-full) of the\n`Gpt2App.generate_response()` routine that we introduced above checks if\nwe return -1, and if so, terminates generation with an oopsie message, e.g.:\n\n```python\nOOPS_NON_PRINTABLE_ENCOUNTERED = (\n    'Oops! Non-printable token encountered.  Generation terminated.'\n)\n...\n\nasync for decoded_token in generate_tokens:\n    if decoded_token == -1:\n        # A non-printable token was generated,\n        # terminating generation.\n        response.send_chunk(\n            OOPS_NON_PRINTABLE_ENCOUNTERED\n        )\n        break\n```\n\nAfter yielding a valid decoded token, we issue an `await asyncio.sleep(0)`\ncall, which returns control back to the event loop for it to potentially\nhandle other concurrent clients.  If there are no other clients, or after\nit has handled all other enqueued work, generation resumes.\n\nThe full code follows, it is simple enough as-is that I didn't feel the need\nto omit any details like in the prior example.\n\n```python\n\nclass GPT:\n\n    ...\n\n    async def generate_async_for(\n        self, text: str, max_length: int = 1024, top_k: int = 50,\n        seed: int = None,\n    ):\n        \"\"\"\n        Asynchronously generate text from the model, yielding tokens\n        one at a time as soon as they are available.\n\n        Args:\n\n            text (str): Supplies the prompt.\n\n            max_length (int): Supplies the maximum total length,\n                including prompt.\n\n            top_k (int): Supplies the number of tokens to consider\n                at each generation step.\n\n            seed (int): Optionally supplies the manual seed to use\n                for the generator.  If None, the model's manual\n                seed will be used.\n\n        Yields:\n\n            byte: The newly generated decoded token.  If -1, a\n            non-printable token was generated, and generation\n            was terminated.\n        \"\"\"\n\n        enc = self.enc\n        stop_token = self.stop_token\n\n        # Encode the prompt -> tensor of shape (1, T)\n        tokens = enc.encode(text)\n        x = torch.tensor(\n            tokens, dtype=torch.long, device=self.device\n        ).unsqueeze(0)\n\n        sample_rng = torch.Generator(device=self.device)\n        if seed is None:\n            seed = self.manual_seed\n        sample_rng.manual_seed(seed)\n\n        logging.debug(\n            f'[generate_async_for] Starting generation loop for {text} '\n            f'with seed {seed}.'\n        )\n\n        start_time = time.perf_counter()\n        count = 0\n        while x.size(1) < max_length:\n            count += 1\n            with torch.no_grad():\n                # Forward pass, ignoring the returned loss.\n                (logits, _) = self(x)\n\n            # Take the logits at the last time-step (shape:\n            # (1, vocab_size)).\n            logits = logits[:, -1, :]\n\n            # Convert to probabilities.\n            probs = F.softmax(logits, dim=-1)\n\n            # Top-k sampling.\n            topk_probs, topk_indices = torch.topk(\n                probs, k=top_k, dim=-1,\n            )\n\n            # Sample the next token.\n            next_idx = torch.multinomial(\n                topk_probs,\n                num_samples=1,\n                generator=sample_rng,\n            )\n            next_token = torch.gather(topk_indices, -1, next_idx)\n\n            # If the next token is the stop token, we're done.\n            next_token_item = next_token.item()\n            if next_token_item == stop_token:\n                break\n\n            # Append token to current sequence.  Although we only\n            # yield a singular decoded token below, we still need\n            # to keep track of the entire sequence for subsequent\n            # generation steps.\n            x = torch.cat((x, next_token), dim=1)\n\n            # Decode the newly-generated token.  Note that a single\n            # token will often be decoded to multiple characters.\n            new_text_fragment = enc.decode([next_token.item()])\n\n            # If any of the characters in the decoded text\n            # representation aren't printable, terminate\n            # generation.\n            if not all(c in self.printable for c in new_text_fragment):\n                yield -1\n                break\n\n            yield new_text_fragment\n\n            # Yield control back to the event loop before continuing\n            # generation.  If we didn't do this, this client would\n            # hog the thread's event loop, preventing other clients\n            # associated with the loop from getting serviced.  (As\n            # we're now running multiple threads in parallel, other\n            # clients associated with event loops on other threads\n            # would not be impacted.)\n            await asyncio.sleep(0)\n\n        elapsed = time.perf_counter() - start_time\n        logging.debug(\n            f\"[generate_async_for] Generated {count} tokens in \"\n            f\"{elapsed:.2f} seconds (~{count / elapsed:.2f} tok/s)\"\n        )\n```\n\nThis routine was the last piece we needed to implement to satisfy\n[our three goals](#our-goals) captured earlier, so, we're now ready to test\nit out!\n\n## Test Drive!\n\n### Launching the HTTP Server\n\nWe can launch an instance of our multi-threaded HTTP web server with our new\n`Gpt2App` HTTP application via the command line as follows:\n\n```bash\n% python -Xgil=0 -m parallelopedia.http.server  \\\n    --threads 40 --ip 0.0.0.0 --port 4444       \\\n    --app-classes parallelopedia.gpt2.Gpt2App   \\\n                  parallelopedia.http.server.PlaintextApp\n```\n\nThis will start up a multi-threaded HTTP server listening on all interfaces\non port 4444, with 40 threads, and two HTTP applications: our `Gpt2App`,\nwhich will service requests to the `/generate` endpoint, and a\n`PlaintextApp` that just returns \"Hello, World!\" to any incoming request\nreceived for the `/plaintext` endpoint.\n\n### Visualizing Chunked-Encoding\n\nLet's visualize the generation response in a way that shows us the raw\nchunked-encoding, without doing any client-side reassembly.  We can achieve\nthat with `echo` and `netcat` (`nc`):\n\n```bash\n% echo -en \\\n 'GET /generate/The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42 ' \\\n 'HTTP/1.1\\r\\nConnection: close\\r\\nHost: dgx\\r\\n\\r\\n' | nc dgx 4444\n\n```\n\nThat should yield the following raw output:\n\n```\nHTTP/1.1 200 OK\nServer: Parallelopedia Web Server v1.0\nDate: Fri, 07 Feb 2025 23:32:02 GMT\nAccept-Ranges: bytes\nContent-Type: text/plain\nAccess-Control-Allow-Origin: *\nConnection: close\nTransfer-Encoding: chunked\nAccess-Control-Expose-Headers: X-Max-Length, X-Top-K, X-Seed, X-Model-Name, X-Model-Device\nX-Max-Length: 20\nX-Top-K: 50\nX-Seed: 42\nX-Model-Name: gpt2\nX-Model-Device: cuda:0\n\n13\nThe quick brown fox\n3\n is\n2\n a\n4\n sub\n7\nspecies\n5\n that\nB\n originated\n3\n in\n9\n southern\n9\n Scotland\n3\n as\n2\n a\n8\n variety\n3\n of\n4\n fox\n1\n.\n5\n This\n0\n\n```\n\nAs you can see, we've enabled chunked-encoding by way of the\n`Transfer-Encoding: chunked` header.  And the body of the response is\ncomprised of these *\"chunks\"*; specifically, each bit of decoded text is\npreceded by its length, in bytes, then followed by `\\r\\n`, then followed by\nthe text itself.\n\nThe zero-length chunk at the end indicates completion of the transfer, and\nas we requested `Connection: close` in our headers, our HTTP server closes\nthe connection once the generation has completed.\n\n::: {.callout-note}\n\nIn HTTP/1.1, *\"keep-alive\"* is the default behavior---i.e., a server won't\nclose a connection unless the client specifically requests it.  This is\nthe opposite of HTTP/1.0 behavior, where the server will close a connection\nby default unless a client furnishes a `Connection: keep-alive` header.\n\n:::\n\n### Verifying via Curl\n\nIf we switch over to `curl` and run the same generation request, we'll see\nthe *reassembled* text, and, provided we supply the `--no-buffer` argument,\n`curl` will also display decoded text as soon as it receives it.\n\n```bash\n% curl --no-buffer --verbose \\\n    'http://dgx:4444/generate/The%20quick%20brown%20fox?' \\\n        'max_length=20&seed=42&device=cuda'\n*   Trying 10.0.132.48:4444...\n* Connected to dgx (10.0.132.48) port 4444 (#0)\n> GET /generate/The%20quick%20brown%20fox?max_length=20&seed=42&device=cuda HTTP/1.1\n> Host: dgx:4444\n> User-Agent: curl/7.81.0\n> Accept: */*\n> \n* Mark bundle as not supporting multiuse\n< HTTP/1.1 200 OK\n< Server: Parallelopedia Web Server v1.0\n< Date: Fri, 07 Feb 2025 23:05:34 GMT\n< Accept-Ranges: bytes\n< Content-Type: text/plain\n< Access-Control-Allow-Origin: *\n< Transfer-Encoding: chunked\n< Access-Control-Expose-Headers: X-Max-Length, X-Top-K, X-Seed, X-Model-Name, X-Model-Device\n< X-Max-Length: 20\n< X-Top-K: 50\n< X-Seed: 42\n< X-Model-Name: gpt2\n< X-Model-Device: cuda:2\n< \nThe quick brown fox is a subspecies that originated in southern Scotland as a variety of fox. This* Connection #0 to host dgx left intact\n```\n\n### Launching the React Bootstrap UI\n\nThe React Bootstrap UI can be launched as follows:\n\n```bash\n% cd $PARALLELOPEDIA_UI # i.e. root of gh:tpn/parallelopedia-ui\n% conda activate py313t # or whatever has recent nodejs/npm\n% npm start run\n```\n\n::: {.callout-note}\n\nFull disclaimer: I'm not a web developer.  I don't know JavaScript, React,\nor Bootstrap, or anything else in the modern web stack.  So like any good\ndeveloper in 2025 when confronted with a task they have absolutely no\nbusiness doing... I palmed it off to AI---either ChatGPT, local LLMs via\n[LM Studio](https://lmstudio.ai/), or, more recently, [Aider](\nhttps://aider.chat).\n\nTL;DR: the web interface code probably sucks.\n\n:::\n\nRunning `npm start run` should open up a browser window pointing at\n`http://localhost:3000`.  Ignore the *Wiki* tab---that's for another\narticle---and switch to the *GPT2* tab.  You should see something similar\nto the GIF below, which is a demo of me using the interface to generate\nsome text:\n\n::: {.callout-note #parallelopedia-ui-gpt2-example collapse=\"false\" appearance=\"simple\" icon=\"false\" title=\"Parallelopedia UI Demo\"}\n\n![Parallelopedia UI Demo](images/parallelopedia-ui-example.gif)\n\n[GPT2.jsx Code](\nhttps://github.com/tpn/parallelopedia-ui/blob/main/src/GPT2.jsx)\n\n:::\n\nNot too shabby!  Granted, it's a tad jarring seeing characters per second\ninstead of tokens per second.  If we wanted to display a live tokens/sec\nrate, some possible options might be:\n\n1. Alter `GPT.async_generate_for()` to calculate how long it took to\n   generate each token (whilst we're in the main generation loop),\n   convert that into a tokens/sec rate, and then change our API on both the\n   server side and the JavaScript UI side such that each chunk that gets\n   sent back is encoded with both the rate *and* the actual chunk.  A\n   drawback of this approach is that hitting the `/generate` endpoint with\n   `curl` or `netcat` would look wacky, as you'd see token/sec floats in\n   between each generated set of decoded characters.\n\n2. Send the raw integer tokens back as they're generated, instead of first\n   decoding them.  That would allow client-side JavaScript to calculate\n   tokens/sec, but it would make it impossible to easily inspect the output\n   with our command line tools like `curl` or `echo ... | nc`.  It would\n   also require adding a JavaScript decoding library on the client side\n   (although that's not such a big deal, I think you can just do `npm\n   install tiktoken` these days).\n\n3. Have the JavaScript client re-encode the decoded characters received back\n   into their GPT2-tokenizer token representation in order to determine\n   actual underlying token count, and recalculate the rate based off that.\n   This rate would differ from the tokens/sec rate observed on the server\n   because it would also include network latency.\n\n4. Do something more advanced with a separate WebSocket or something,\n   where the live tokens/sec generation rate can be displayed independently\n   to the generated decoded tokens.\n\n4. Just YOLO it and have the JavaScript code divide the characters per\n   second by, say, three, and pretend that's roughly the tokens/sec rate\n   (assuming that on average, one token represents three characters in any\n   given response).\n\nI don't want to do any of that!  So characters per second it is, for now,\nhowever jarring it may be.\n\n## Parallel Generation\n\nAlright, we've exercised the `/generate` endpoint a few different ways via a\nsingle client connection, and it looks like it's behaving the way we hoped\nit would, albeit with a whopping simultaneous client count of, precisely,\none.\n\nLet's look at introducing some simultaneous load by way of multiple machines\nall issuing `/generate` requests at the same time, first via the\n`echo ... | nc` approach, and then via `curl`.\n\nI'll leverage `tmux` and the `:setw synchronize-panes` command, which sends\nmy console keystrokes to all active panes within a given `tmux` session\nwindow.\n\n### Netcat Example\n\nI captured a 15 frames-per-second GIF of this in action, which you can view\nbelow.  It shows terminal sessions to six machines, arranged vertically, all\nexecuting the same `echo ... | nc` command depicted above.\n\n::: {.callout-note}\n\nThis might seem like a bit of a silly test, especially when it's working\ncorrectly, because the output is pretty benign, and we don't *really* know\nthat these requests were actually being served simultaneously by different\nthreads on the server side.\n\nWhen I first tried it, though, it absolutely did *not* work correctly---one\nserver would get correct output, another would get the HTTP headers in\ntriplicate, whilst two other sessions just went straight into the chunked\nresponses, with no preceding HTTP headers in sight.  I had a bug in my HTTP\nrouting code (that exists in the PyParallel code I wrote ten years ago upon\nwhich I based the new HTTP server on!) which was entirely to blame.  With\nthat code ripped out and `@route` reimplemented in a much simpler fashion,\neverything worked well.\n\n:::\n\n::: {.callout-note #parallel-netcat-generation collapse=\"false\" appearance=\"simple\" icon=\"false\" title=\"Parallel Netcat Generation\"}\n\n![Parallel Netcat Generation](\nimages/parallelopedia-multi-threaded-gpu-inference-gpt2-nc.gif)\n\n:::\n\nI extracted the last frame of the GIF, below, where you can see that at\nleast there was some variance between which GPUs were selected:\n\n![Parallel Netcat Generation - Last Frame](\nimages/parallelopedia-multi-threaded-gpu-inference-gpt2-nc-last-frame-2.png)\n\n### Curl Example\n\nI did the same thing with the panes arranged horizontally, using `curl`\ninstead and a `max_length=1000` and no seed, which helps in visualizing\nthe parallel generation, as you can clearly see different clients are\nreceiving completely different outputs.\n\n::: {.callout-note #parallel-curl-generation collapse=\"false\" appearance=\"simple\" icon=\"false\" title=\"Parallel Curl Generation\"}\n\n![Parallel Curl Generation](\nimages/parallelopedia-multi-threaded-gpu-inference-gpt2-curl-1000.gif)\n\n:::\n\n## Let's Load Test This Sucker!\n\nSo far so good!  Looks like we can absolutely do PyTorch model generation in\nparallel via multiple threads thanks to free-threaded Python.\n\nLet's ramp it up a notch and see what happens if we attempt to load test\nthe `/generate` endpoint from a different server.  The `leopard` server\nyou see in the next example is an Intel(R) Xeon(R) W-2275 CPU @ 3.30GHz\n(14 cores, 28 threads) and is connected to the `dgx` box via a 10G Ethernet\nlink.\n\nUsing the fork of [`wrk`](https://github.com/tpn/wrk) I hacked together some\nten years ago or so whilst load testing PyParallel, let's kick off a run\nwith 14 threads for 30 seconds.  There will be one connection per thread,\nand HTTP/1.1 will be used, so the implicit *keep-alive* behavior means that\nas soon as one `/generate` request completes, another one is immediately\ndispatched.\n\nFor posterity, the console command being used is as follows:\n\n```bash\n% time ./wrk -v --latency -c14 -t14 -d30s \\\n    'http://dgx:4444/generate/'\n    'The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42'\n```\n\nIn the GIF below you'll see two terminal windows.  The smaller foreground\nwindow on the left is `leopard`, the session doing the `wrk` load test.  The\nbackground window that takes up the rest of the screen is logged in to\n`dgx`, our server, and it's running a GPU-enabled build of [`btop`](\nhttps://github.com/aristocratos/btop), which is a beautiful console app for\ndisplaying resource usage.  I particularly like `btop` in this example as it\ndoes a good job of conveying the CPU and GPU load that kicks in when the\nload test starts.\n\nThe 40 CPU cores can be seen in the top pane, and, as expected, about 14 of\nthem whirr into life as soon as the load test begins, which tracks, as there\nare now 14 clients attempting to hammer our `/generate` end point.\n\nBelow that, you can see the four Tesla V100-DGXS-32GB GPUs also whirr into\naction, handling the generation between them with relative ease.\n\nOn the bottom right, I've filtered the process tree to just display our\n`python` HTTP server invocation.  (I would love it if `btop` showed the\nindividual threads and their load, as it would beautifully depict Python\nsaturating cores in parallel now that there's no GIL impeding execution,\nhowever, I don't believe that's currently possible with this version of\n`btop`.)\n\n### Parallel Load Testing (No GIL)\n\n::: {.callout-note #parallel-generation-load-test-no-gil collapse=\"false\" appearance=\"simple\" icon=\"false\" title=\"Parallel Generation Load Test: No GIL\"}\n\n![Parallel Generation Load Test - No GIL](\nimages/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14.gif)\n\n:::\n\nI've extracted the last frame, below, to allow closer inspection at the end\nof the run, without the annoyance of the GIF looping.\n\n![Parallel Generation Load Test No GIL - Last Frame](\nimages/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-last-frame-optimized.png)\n\nThe console output from `wrk` is captured in the callout below.\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Console Output of wrk: No GIL\"}\n\n```\nwrk 4.0.0 [epoll] Copyright (C) 2012 Will Glozer\nRunning 30s test @ http://dgx:4444/generate/The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42\n  14 threads and 14 connections\n  Thread Stats   Min      Avg       Stdev     Max     +/- Stdev\n    Latency   130.40ms  197.38ms   54.41ms 528.61ms   89.19%\n    Req/Sec     1.00      5.63      2.32    10.00     67.76%\n  Latency Distribution\n      0%  130.40ms\n      1%  139.79ms\n      2%  143.50ms\n      3%  145.78ms\n      4%  147.53ms\n      5%  149.78ms\n      6%  151.18ms\n      7%  152.43ms\n      8%  153.42ms\n      9%  155.29ms\n     10%  156.15ms\n     11%  157.29ms\n     12%  158.27ms\n     13%  158.90ms\n     14%  159.70ms\n     15%  160.73ms\n     16%  161.64ms\n     17%  162.32ms\n     18%  163.09ms\n     19%  163.60ms\n     20%  164.41ms\n     21%  165.17ms\n     22%  165.61ms\n     23%  166.08ms\n     24%  166.75ms\n     25%  167.36ms\n     26%  167.93ms\n     27%  168.51ms\n     28%  169.05ms\n     29%  169.49ms\n     30%  170.23ms\n     31%  170.92ms\n     32%  171.54ms\n     33%  172.10ms\n     34%  172.66ms\n     35%  173.29ms\n     36%  173.98ms\n     37%  174.39ms\n     38%  174.96ms\n     39%  175.47ms\n     40%  176.17ms\n     41%  176.80ms\n     42%  177.24ms\n     43%  177.75ms\n     44%  178.49ms\n     45%  178.92ms\n     46%  179.57ms\n     47%  180.15ms\n     48%  180.62ms\n     49%  181.31ms\n     50%  182.00ms\n     51%  182.60ms\n     52%  183.22ms\n     53%  184.14ms\n     54%  185.01ms\n     55%  185.53ms\n     56%  186.21ms\n     57%  187.30ms\n     58%  188.03ms\n     59%  188.66ms\n     60%  189.14ms\n     61%  190.47ms\n     62%  191.13ms\n     63%  191.69ms\n     64%  192.34ms\n     65%  193.06ms\n     66%  193.87ms\n     67%  194.51ms\n     68%  195.63ms\n     69%  196.68ms\n     70%  197.67ms\n     71%  198.34ms\n     72%  199.90ms\n     73%  201.43ms\n     74%  202.66ms\n     75%  203.56ms\n     76%  205.00ms\n     77%  206.48ms\n     78%  207.76ms\n     79%  209.30ms\n     80%  210.53ms\n     81%  212.11ms\n     82%  213.30ms\n     83%  215.90ms\n     84%  218.09ms\n     85%  220.16ms\n     86%  224.57ms\n     87%  227.99ms\n     88%  230.93ms\n     89%  234.77ms\n     90%  241.08ms\n     91%  252.68ms\n     92%  302.34ms\n     93%  321.04ms\n     94%  335.47ms\n     95%  339.80ms\n     96%  358.81ms\n     97%  369.67ms\n     98%  379.52ms\n     99%  397.66ms\n    100%  411.36ms\n  2128 requests in 30.06s, 1.17MB read\nRequests/sec:     70.80\nTransfer/sec:     39.75KB\n\n```\n\n:::\n\nA visualization of the latencies is presented below.  All code for\nvisualization is from the [*Data Visualization*](wrk-analysis-preview.html)\nJupyter Notebook, which you can [preview here](wrk-analysis.ipynb), or\naccess directly on [Github](\nhttps://github.com/tpn/website/blob/main/articles/pytorch-and-python-free-threading/wrk-analysis.ipynb).\n\n::: {.theme-light}\n![Parallel Load Test Latency Distribution (No GIL)](wrk-01-no-gil.svg)\n:::\n\n::: {.theme-dark}\n![Parallel Load Test Latency Distribution (No GIL)](wrk-01-no-gil-dark.svg)\n:::\n\n### Ablation Test: Re-enable the GIL\n\nLet's see what happens if we re-enable the GIL.  We should see poor resource\nutilization on the server side, as only one Python thread can run at a time,\nand the statistics reported on the client side should be equally dismal.\n\nExpand the callout below to view the GIF.  I have used another terminal\nwindow to launch the server with the `-Xgil=1` argument, which re-enables\nthe GIL.  I then switch back over to `leopard` and perform the `wrk` load\ntest like before.\n\n::: {.callout-note #parallel-load-test-gil collapse=\"false\" appearance=\"simple\" icon=\"false\" title=\"Parallel Generation Load Test: GIL Enabled\"}\n\n![Parallel Generation Load Test - GIL Enabled](\nimages/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-GIL-enabled.gif)\n\n:::\n\nAs with before, I've extracted the last frame, below, to allow closer\ninspection at the end of the run, without the annoyance of the GIF looping.\n\n![Parallel Generation Load Test - GIL Enabled - Last Frame](\nimages/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-GIL-enabled-last-frame-optimized.png)\n\nAs we expected: re-enabling the GIL absolutely murders resource utilization.\n\nThe console output from `wrk` follows in the next callout.\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Console Output of wrk: GIL Enabled\"}\n\n```\nwrk 4.0.0 [epoll] Copyright (C) 2012 Will Glozer\nRunning 30s test @ http://dgx:4444/generate/The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42\n  14 threads and 14 connections\n  Thread Stats   Min      Avg       Stdev     Max     +/- Stdev\n    Latency     1.11s     1.46s    92.76ms   1.82s    84.47%\n    Req/Sec     0.00      0.00      0.00     0.00    100.00%\n  Latency Distribution\n      0%    1.11s\n      1%    1.16s\n      2%    1.22s\n      3%    1.24s\n      4%    1.29s\n      5%    1.36s\n      6%    1.37s\n      7%    1.37s\n      8%    1.37s\n      9%    1.38s\n     10%    1.38s\n     11%    1.39s\n     12%    1.39s\n     13%    1.39s\n     14%    1.40s\n     15%    1.40s\n     16%    1.40s\n     17%    1.41s\n     18%    1.41s\n     19%    1.41s\n     20%    1.41s\n     21%    1.41s\n     22%    1.41s\n     23%    1.42s\n     24%    1.42s\n     25%    1.42s\n     26%    1.42s\n     27%    1.42s\n     28%    1.42s\n     29%    1.42s\n     30%    1.42s\n     31%    1.43s\n     32%    1.43s\n     33%    1.43s\n     34%    1.43s\n     35%    1.43s\n     36%    1.43s\n     37%    1.43s\n     38%    1.44s\n     39%    1.44s\n     40%    1.44s\n     41%    1.44s\n     42%    1.44s\n     43%    1.45s\n     44%    1.45s\n     45%    1.45s\n     46%    1.45s\n     47%    1.45s\n     48%    1.45s\n     49%    1.45s\n     50%    1.46s\n     51%    1.46s\n     52%    1.46s\n     53%    1.46s\n     54%    1.46s\n     55%    1.47s\n     56%    1.47s\n     57%    1.47s\n     58%    1.47s\n     59%    1.47s\n     60%    1.47s\n     61%    1.48s\n     62%    1.48s\n     63%    1.48s\n     64%    1.48s\n     65%    1.48s\n     66%    1.49s\n     67%    1.49s\n     68%    1.49s\n     69%    1.49s\n     70%    1.49s\n     71%    1.49s\n     72%    1.49s\n     73%    1.50s\n     74%    1.50s\n     75%    1.50s\n     76%    1.50s\n     77%    1.50s\n     78%    1.51s\n     79%    1.51s\n     80%    1.51s\n     81%    1.51s\n     82%    1.51s\n     83%    1.51s\n     84%    1.52s\n     85%    1.52s\n     86%    1.52s\n     87%    1.53s\n     88%    1.53s\n     89%    1.53s\n     90%    1.54s\n     91%    1.55s\n     92%    1.56s\n     93%    1.56s\n     94%    1.59s\n     95%    1.59s\n     96%    1.60s\n     97%    1.70s\n     98%    1.75s\n     99%    1.77s\n    100%    1.82s\n  221 requests in 30.07s, 129.42KB read\n  Socket errors: connect 0, read 0, write 0, timeout 60\nRequests/sec:      7.35\nTransfer/sec:      4.30KB\n\n```\n:::\n\nAnd a visualization of the latencies follows.  Note that there were 60\nsocket timeouts in this case, whereas no timeouts were encountered in the\nprior run with the GIL disabled.\n\n::: {.theme-light}\n![Parallel Load Test Latency Distribution (GIL Enabled)](wrk-01-gil.svg)\n:::\n\n::: {.theme-dark}\n![Parallel Load Test Latency Distribution (GIL Enabled)](wrk-01-gil-dark.svg)\n:::\n\n### Parallel Load Testing: No GIL vs GIL\n\nViewing them side by side:\n\n::: {.theme-light}\n![Parallel Load Test Latency Distribution (No GIL & GIL Enabled Combined)](\nwrk-01-combined.svg)\n:::\n\n::: {.theme-dark}\n![Parallel Load Test Latency Distribution (No GIL & GIL Enabled Combined)](\nwrk-01-combined-dark.svg)\n:::\n\nRemember to keep in mind that the No GIL vs GIL requests/sec was 70.80 vs\n7.35, nearly a ten-fold increase:\n\n::: {.theme-light}\n![Parallel Load Test Requests/sec (No GIL vs GIL)](\nwrk-01-rps-combined.svg)\n:::\n\n::: {.theme-dark}\n![Parallel Load Test Requests/sec (No GIL vs GIL)](\nwrk-01-rps-combined-dark.svg)\n:::\n\n### Parallel Load Testing: How does Plaintext Fare?\n\nAs we launched our HTTP server invocation earlier with the `PlaintextApp`\nclass, which simply responds to the `/plaintext` endpoint with `b'Hello\nWorld\\n'`, let's throw some load at that too.\n\nThis doesn't have anything to do with PyTorch; it's an orthogonal load test\nthat's fun because it depicts the stark difference between GIL vs no GIL.\n\nThe console command was issued on `leopard`, like last time, as follows:\n\n```bash\n% time ./wrk -v --latency -c14 -t14 -d30s 'http://dgx:4444/plaintext'\n```\n\nConsole outputs for both tests follow.\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Console Output of wrk Plaintext: No GIL\"}\n```\nwrk 4.0.0 [epoll] Copyright (C) 2012 Will Glozer\nRunning 30s test @ http://dgx:4444/plaintext\n  14 threads and 14 connections\n  Thread Stats   Min      Avg       Stdev     Max     +/- Stdev\n    Latency   127.00us   11.68ms   55.98ms 818.74ms   94.91%\n    Req/Sec    20.00      4.89k     1.46k    6.43k    87.86%\n  Latency Distribution\n      0%  127.00us\n      1%  148.00us\n      2%  150.00us\n      3%  152.00us\n      4%  153.00us\n      5%  154.00us\n      6%  155.00us\n      7%  156.00us\n      8%  157.00us\n      9%  157.00us\n     10%  158.00us\n     11%  159.00us\n     12%  159.00us\n     13%  160.00us\n     14%  161.00us\n     15%  161.00us\n     16%  162.00us\n     17%  162.00us\n     18%  163.00us\n     19%  163.00us\n     20%  164.00us\n     21%  165.00us\n     22%  165.00us\n     23%  166.00us\n     24%  166.00us\n     25%  167.00us\n     26%  167.00us\n     27%  168.00us\n     28%  168.00us\n     29%  169.00us\n     30%  169.00us\n     31%  170.00us\n     32%  170.00us\n     33%  171.00us\n     34%  171.00us\n     35%  172.00us\n     36%  172.00us\n     37%  173.00us\n     38%  173.00us\n     39%  174.00us\n     40%  174.00us\n     41%  175.00us\n     42%  175.00us\n     43%  176.00us\n     44%  176.00us\n     45%  177.00us\n     46%  178.00us\n     47%  178.00us\n     48%  179.00us\n     49%  179.00us\n     50%  180.00us\n     51%  181.00us\n     52%  181.00us\n     53%  182.00us\n     54%  183.00us\n     55%  183.00us\n     56%  184.00us\n     57%  185.00us\n     58%  186.00us\n     59%  186.00us\n     60%  187.00us\n     61%  188.00us\n     62%  189.00us\n     63%  190.00us\n     64%  191.00us\n     65%  192.00us\n     66%  193.00us\n     67%  194.00us\n     68%  195.00us\n     69%  196.00us\n     70%  197.00us\n     71%  198.00us\n     72%  199.00us\n     73%  200.00us\n     74%  202.00us\n     75%  203.00us\n     76%  205.00us\n     77%  207.00us\n     78%  208.00us\n     79%  211.00us\n     80%  213.00us\n     81%  216.00us\n     82%  219.00us\n     83%  223.00us\n     84%  229.00us\n     85%  236.00us\n     86%  246.00us\n     87%  263.00us\n     88%  308.00us\n     89%  599.00us\n     90%    8.77ms\n     91%   19.48ms\n     92%   30.74ms\n     93%   42.55ms\n     94%   54.97ms\n     95%   68.92ms\n     96%   85.23ms\n     97%  106.42ms\n     98%  139.96ms\n     99%  282.83ms\n    100%  462.36ms\n  1970103 requests in 30.01s, 420.86MB read\nRequests/sec:  65652.68\nTransfer/sec:     14.02MB\n\n```\n:::\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Console Output of wrk Plaintext: GIL Enabled\"}\n```\nwrk 4.0.0 [epoll] Copyright (C) 2012 Will Glozer\nRunning 30s test @ http://dgx:4444/plaintext\n  14 threads and 14 connections\n  Thread Stats   Min      Avg       Stdev     Max     +/- Stdev\n    Latency   187.00us    4.81ms   19.24ms 295.28ms   97.89%\n    Req/Sec     4.00    442.80    133.07   666.00     66.01%\n  Latency Distribution\n      0%  187.00us\n      1%  577.00us\n      2%  645.00us\n      3%  670.00us\n      4%  684.00us\n      5%  698.00us\n      6%  712.00us\n      7%  724.00us\n      8%  735.00us\n      9%  745.00us\n     10%  755.00us\n     11%  763.00us\n     12%  770.00us\n     13%  778.00us\n     14%  789.00us\n     15%  801.00us\n     16%  814.00us\n     17%  831.00us\n     18%    0.85ms\n     19%    0.88ms\n     20%    0.95ms\n     21%    1.19ms\n     22%    1.25ms\n     23%    1.27ms\n     24%    1.29ms\n     25%    1.31ms\n     26%    1.33ms\n     27%    1.34ms\n     28%    1.36ms\n     29%    1.37ms\n     30%    1.38ms\n     31%    1.40ms\n     32%    1.42ms\n     33%    1.43ms\n     34%    1.46ms\n     35%    1.48ms\n     36%    1.52ms\n     37%    1.63ms\n     38%    1.80ms\n     39%    1.84ms\n     40%    1.87ms\n     41%    1.89ms\n     42%    1.90ms\n     43%    1.92ms\n     44%    1.93ms\n     45%    1.95ms\n     46%    1.96ms\n     47%    1.97ms\n     48%    1.99ms\n     49%    2.01ms\n     50%    2.02ms\n     51%    2.05ms\n     52%    2.07ms\n     53%    2.11ms\n     54%    2.17ms\n     55%    2.31ms\n     56%    2.39ms\n     57%    2.44ms\n     58%    2.48ms\n     59%    2.50ms\n     60%    2.53ms\n     61%    2.55ms\n     62%    2.57ms\n     63%    2.59ms\n     64%    2.61ms\n     65%    2.64ms\n     66%    2.66ms\n     67%    2.69ms\n     68%    2.73ms\n     69%    2.77ms\n     70%    2.84ms\n     71%    2.96ms\n     72%    3.04ms\n     73%    3.09ms\n     74%    3.12ms\n     75%    3.15ms\n     76%    3.18ms\n     77%    3.22ms\n     78%    3.26ms\n     79%    3.31ms\n     80%    3.38ms\n     81%    3.51ms\n     82%    3.65ms\n     83%    3.74ms\n     84%    3.82ms\n     85%    3.90ms\n     86%    4.01ms\n     87%    4.21ms\n     88%    4.33ms\n     89%    4.41ms\n     90%    4.48ms\n     91%    4.58ms\n     92%    4.82ms\n     93%    5.03ms\n     94%    5.30ms\n     95%    5.72ms\n     96%    6.34ms\n     97%    7.54ms\n     98%   31.73ms\n     99%  116.44ms\n    100%  169.73ms\n  183217 requests in 30.02s, 39.14MB read\nRequests/sec:   6102.29\nTransfer/sec:      1.30MB\n\n```\n:::\n\nCombined requests/sec visualization depicts a similar 10x improvement:\n\n::: {.theme-light}\n![Plaintext Parallel Load Test Requests/sec (No GIL vs GIL)](\nwrk-02-rps-combined.svg)\n:::\n\n::: {.theme-dark}\n![Plaintext Parallel Load Test Requests/sec (No GIL vs GIL)](\nwrk-02-rps-combined-dark.svg)\n:::\n\nThe latencies are a bit misleading when viewed in isolation, as you can see\nthe tail end of the *No GIL* run incur higher latencies compared to the\n*GIL* run---however, as can be seen above, the *No GIL* run was doing 10x\nmore requests/sec.\n\n::: {.theme-light}\n![Plaintext Parallel Load Test Latency Distribution (No GIL vs GIL)](\nwrk-02-combined.svg)\n:::\n\n::: {.theme-dark}\n![Plaintext Parallel Load Test Latency Distribution (No GIL vs GIL)](\nwrk-02-combined-dark.svg)\n:::\n\n### Parallel Load Test Summary\n\nI think it's safe to say we've achieved our original goals in a satisfactory\nmanner.  We can absolutely now do parallel model inference in a single\nPython process, thanks to free-threading, and the whole thing works great\nwhen wrapped around an `asyncio`-based HTTP server that yields tokens\none-by-one as soon as they're generated, which is a closer representation of\nwhat you'd want in the real world if you were to deploy such a thing.\n\nMy takeaway from all of this: free-threading kicks ass.  Having devoted so\nmuch time toward a parallel Python solution with [PyParallel](\nhttps://pyparallel.org) over a decade ago, it makes me incredibly happy to\nsee a working, performant solution finally getting mainlined into the core\nPython language.\n\nIt's important to consider how many other things are simultaneously unlocked\nby Python free-threading.  The HTTP server we've demonstrated above also has\ndirectory/file-serving behavior built-in, so it also functions like a normal\nweb server would.  In a separate article, I'll introduce the *Wiki* server\ncomponent, which features a `WikiApp` HTTP app that loads a 56GB XML file,\nplus about 12GB of supporting index structures (by way of `datrie` digital\nsearch tries, and NumPy arrays).  This app happily loads in the same single\nPython process as our existing `Gpt2App`---demonstrating the power of\naccessing huge data structures in parallel by multiple threads, something\nthat couldn't be done with `multiprocessing` without paying for the cost to\nreplicate that memory overhead in every process.\n\n# Finishing Up\n\nI'll tackle two more topics before we conclude this post.  First, I want to\nbriefly touch on a handful of the changes I made to the GPT-2 implementation\nafter the version we introduced in the [Initial Implementation](\n#initial-implementation) section.\n\nSecond, let's see what happens if we throw the new graph compilation\noptimizations in PyTorch 2.0 at our solution; specifically, can we still use\n`model = torch.compile(model)` in our free-threading solution?\n\n## Reviewing Initial Implementation\n\n### Skipping Weight Initialization\n\nThe first version of the `GPT` class we introduced [here](\n#initial-model-setup-code) looked like this:\n\n```python\nclass GPT(nn.Module):\n\n    def __init__(self, config, device):\n        super().__init__()\n        self.config = config\n        self.device = device\n        self.manual_seed = 42\n\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=nn.Embedding(config.vocab_size, config.n_embd),\n                wpe=nn.Embedding(config.block_size, config.n_embd),\n                h=nn.ModuleList(\n                    [Block(config) for _ in range(config.n_layer)]\n                ),\n                ln_f=nn.LayerNorm(config.n_embd),\n            )\n        )\n        self.lm_head = nn.Linear(\n            config.n_embd, config.vocab_size, bias=False\n        )\n\n        self.transformer.wte.weight = self.lm_head.weight\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n```\n\nAs I mentioned earlier, I ripped that almost verbatim from Andrej's\n`train_gpt2.py` code.  For about two weeks during development of this work,\nI was using a local free-threaded build of PyTorch, but had forgotten that I\nhad built it in *debug* configuration.\n\nEverything was dog slow, but I had no frame of reference for how long things\n*should* have been taking, having had no real prior experience with PyTorch,\nbut I was getting annoyed enough at how long it seemed to take to load the\nmodel---which I was doing frequently during development---so I added a bunch\nof timing code around things to try and bisect where the overhead was being\nintroduced.\n\nLooking at the `__init__()` code above, though, I was thoroughly perplexed\nby the `_init_weights(self, module)` function.  Why would we be initializing\nweights at the end of the `GPT` constructor if we were just about to\noverride them with the weights we were loading from the checkpoint?\n\nEliminating the call to `_init_weights()` entirely shaved off 15 seconds\nfrom the time it took to simply create an *\"empty\"* `GPT()` instance---i.e.\nbefore we'd even loaded the weights.  However, it was still taking 15\nseconds just to execute this block of code:\n\n```python\n\n    self.transformer = nn.ModuleDict(\n        dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            wpe=nn.Embedding(config.block_size, config.n_embd),\n            h=nn.ModuleList(\n                [Block(config) for _ in range(config.n_layer)]\n            ),\n            ln_f=nn.LayerNorm(config.n_embd),\n        )\n    )\n    self.lm_head = nn.Linear(\n        config.n_embd, config.vocab_size, bias=False\n    )\n```\n\nI hoisted that code out into a separate `_init_transformer()` routine and\nsurrounded it with some optional `torch.profiler.profile()` glue:\n\n```python\n\n    timer = ElapsedTimer()\n\n    with timer:\n        if not self.torch_profile_activities:\n            self._init_transformer()\n        else:\n            with torch.profiler.profile(\n                activities=self.torch_profile_activities,\n                with_stack=True,\n            ) as prof:\n                self._init_transformer()\n            self.torch_profile_init_transformer = prof\n\n    msg = f'Initialized GPT model in {timer.elapsed:.3f} seconds.'\n    logging.info(msg)\n```\n\nThe profiling data yielded some interesting insight:\n\n```python\n> print(model.torch_profile_init_transformer.key_averages().table(sort_by='cpu_time_total'))\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\n           aten::uniform_        74.31%       10.861s        74.31%       10.861s     111.966ms            97\n            aten::normal_        25.64%        3.747s        25.64%        3.747s        1.873s             2\n             aten::detach         0.01%       1.408ms         0.03%       3.712ms      24.914us           149\n              aten::empty         0.02%       2.570ms         0.02%       2.570ms      17.247us           149\n                   detach         0.02%       2.304ms         0.02%       2.304ms      15.464us           149\n              aten::fill_         0.01%       1.013ms         0.01%       1.013ms      40.500us            25\n              aten::zero_         0.00%     305.888us         0.00%     305.888us      12.236us            25\n    cudaDeviceSynchronize         0.00%      16.073us         0.00%      16.073us      16.073us             1\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 14.615s\n```\n\nSo `aten::uniform_` and `aten::normal_` were taking up, *literally*, 99.95%\nof the time during the transformer *initialization*.  That also seemed\nbananas for the exact same reason calling `_init_weights()` seemed bananas:\nwhy was so much time and effort being spent initializing distributions when\nwe were going to immediately overwrite all the weights when we load the\nmodel from the checkpoint?\n\nNow, granted, had I not been using a debug build of PyTorch, those 14.597\nseconds spent on weight initialization would be more like unnoticeable\nmilliseconds at best.  But I didn't know that at the time, so after a bit of\ndigging, I found out I could subclass my `Embedding` and `Linear` layers\n(which were the ones contributing to the uniform and normal distribution\nsetup time overhead) in such a way that weight initialization would be\nskipped entirely.\n\nSo, if you look at the [`gpt2.py`](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py#L535)\nimplementation on Github, you'll see that I'm using a bunch of `NoInit`\nclasses, as follows:\n\n```python\n# ==============================================================\n# Classes\n# ==============================================================\n\n# N.B. We have simple \"no-init\" overrides for nn.Embedding and\n#      nn.Linear which skip the default initialization routines,\n#      significantly reducing the time to load the model by\n#      avoiding uniform and random distribution initialization.\n#      As we immediately load all the weights from the model\n#      checkpoint straight after creating the model, we don't\n#      need the default initialization routines.\n\nclass NoInitEmbedding(nn.Embedding):\n    def reset_parameters(self):\n        # Skip default uniform initialization.\n        pass\n\n\nclass NoInitLinear(nn.Linear):\n    def reset_parameters(self):\n        # Skip default Kaiming initialization.\n        pass\n\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # Key, query, value projections for all heads, but in a batch.\n        self.c_attn = NoInitLinear(config.n_embd, 3 * config.n_embd)\n\n        # Output projection.\n        self.c_proj = NoInitLinear(config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n\n        # Regularization.\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n\n    ...\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = NoInitLinear(config.n_embd, 4 * config.n_embd)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = NoInitLinear(4 * config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n\n    ...\n\nclass GPT:\n\n    ...\n\n    def _init_transformer(self):\n        \"\"\"\n        Initialize the transformer.\n        \"\"\"\n        config = self.config\n        self.transformer = nn.ModuleDict(\n            dict(\n                wte=NoInitEmbedding(config.vocab_size, config.n_embd),\n                wpe=NoInitEmbedding(config.block_size, config.n_embd),\n                h=nn.ModuleList(\n                    [Block(config) for _ in range(config.n_layer)]\n                ),\n                ln_f=nn.LayerNorm(config.n_embd),\n            )\n        )\n        self.lm_head = NoInitLinear(\n            config.n_embd,\n            config.vocab_size,\n            bias=False,\n        )\n        self.transformer.wte.weight = self.lm_head.weight\n\n```\n\n\nA few weeks after I'd put this code in, I came across this HuggingFace\narticle on [Big Model Inference](\nhttps://huggingface.co/docs/accelerate/en/usage_guides/big_modeling) where\nthey discuss the very problem I was hitting, which is problematic on much\nlarger models even when you're using a release build of PyTorch---not just\npip-squeak GPT2 models with a debug build---and they have a\ndecorator-oriented solution:\n\n```python\nfrom accelerate import init_empty_weights\nwith init_empty_weights():\n    my_model = ModelClass(...)\n```\n\nGranted, I couldn't use `accelerate` because it depends on packages that are\nnot available for free-threaded builds yet, but I wanted to include the\ninformation here for future reference.\n\n### Can We Load Other Checkpoints?\n\nGPT2 was the last model where OpenAI made the weights publicly available.\nSo, in theory, I should be able to download their largest GPT2\nmodel---[GPT2 XL](https://huggingface.co/openai-community/gpt2-xl)---figure\nout how to extract the weights, and then load them into our janky little\n`GPT` class instead of the ones from the locally-trained checkpoint.\n\nTurns out downloading it is easy via `huggingface-cli` (again, not something\nthat works with free-threading, so you'll need to activate the `py313`\nenvironment and `pip install -U \"huggingface_hub[cli]\"` per [these](\nhttps://huggingface.co/docs/huggingface_hub/en/guides/cli) instructions):\n\n```bash\n% huggingface-cli download openai-community/gpt2-xl\n# That should download the model into the following directory.\n% cd ~/.cache/huggingface/hub/models--openai-community--gpt2-xl\n% tree -h .\ntree -h\n[4.0K]  .\n├── [4.0K]  blobs\n│   ├── [ 124]  057e43033439e068b325f32af95dde9efc9552ae\n│   ├── [6.0G]  0f8b28eb05a8075f48b61b6f35332978c74fc7763fa9fb4051a1c30511736a6a\n│   ├── [1018K]  1f1d9aaca301414e7f6c9396df506798ff4eb9a6\n│   ├── [446K]  226b0752cac7789c48f0cb3ec53eda48b7be36cc\n│   ├── [ 12K]  34f1cc31333e001c3047539dd604123dafd37346\n│   ├── [5.8G]  47d4a280f9274f015e8dd1e7e2e1066415fc1f37030ec30d00615973e750e578\n│   ├── [1.3M]  4b988bccc9dc5adacd403c00b4704976196548f8\n│   ├── [ 445]  602b71f15d40ed68c5f96330e3f3175a76a32126\n│   ├── [6.3G]  7b92a55d852494754e2856681833ee0ff05580ee32c1d8d60a1177bbf1f4703a\n│   ├── [ 689]  932bdee9f4b3878f7f285187564b46f256243aff\n│   ├── [  26]  be4d21d94f3b4687e5a54d84bf6ab46ed0f8defd\n│   ├── [ 165]  c791c4d01fc484a57262ee57c878dd5b35863fe7\n│   ├── [6.0G]  cd2a29e31040ef64d9362cb96801969c9f67b9e0bdbd6e00b9dda57cdbe17435\n│   └── [5.8G]  fbc167e52fa30c56fbed46e8b45c25893c4afb165ef666864abc0e70d5c117a4\n├── [4.0K]  refs\n│   └── [  40]  main\n└── [4.0K]  snapshots\n    └── [4.0K]  15ea56dee5df4983c59b2538573817e1667135e2\n        ├── [  52]  config.json -> ../../blobs/932bdee9f4b3878f7f285187564b46f256243aff\n        ├── [  76]  flax_model.msgpack -> ../../blobs/fbc167e52fa30c56fbed46e8b45c25893c4afb165ef666864abc0e70d5c117a4\n        ├── [  52]  generation_config_for_text_generation.json -> ../../blobs/c791c4d01fc484a57262ee57c878dd5b35863fe7\n        ├── [  52]  generation_config.json -> ../../blobs/057e43033439e068b325f32af95dde9efc9552ae\n        ├── [  52]  merges.txt -> ../../blobs/226b0752cac7789c48f0cb3ec53eda48b7be36cc\n        ├── [  76]  model.safetensors -> ../../blobs/0f8b28eb05a8075f48b61b6f35332978c74fc7763fa9fb4051a1c30511736a6a\n        ├── [  76]  pytorch_model.bin -> ../../blobs/cd2a29e31040ef64d9362cb96801969c9f67b9e0bdbd6e00b9dda57cdbe17435\n        ├── [  52]  README.md -> ../../blobs/34f1cc31333e001c3047539dd604123dafd37346\n        ├── [  76]  rust_model.ot -> ../../blobs/7b92a55d852494754e2856681833ee0ff05580ee32c1d8d60a1177bbf1f4703a\n        ├── [  76]  tf_model.h5 -> ../../blobs/47d4a280f9274f015e8dd1e7e2e1066415fc1f37030ec30d00615973e750e578\n        ├── [  52]  tokenizer_config.json -> ../../blobs/be4d21d94f3b4687e5a54d84bf6ab46ed0f8defd\n        ├── [  52]  tokenizer.json -> ../../blobs/4b988bccc9dc5adacd403c00b4704976196548f8\n        └── [  52]  vocab.json -> ../../blobs/1f1d9aaca301414e7f6c9396df506798ff4eb9a6\n```\n\nWell that's pretty neat, if not a tad duplicative, given the 1.5B parameter\nmodel is shipped in five different ~6GB encodings:\n\n```\n│   ├── [6.0G]  model.safetensors   (0f8b28...)\n│   ├── [5.8G]  tf_model.h5         (47d4a2...)\n│   ├── [6.3G]  rust_model.ot       (7b92a5...)\n│   ├── [6.0G]  pytorch_model.bin   (cd2a29...)\n│   └── [5.8G]  flax_model.msgpack  (fbc167...)\n```\n\nThe `pytorch_model.bin` one is a pickled `dict` of tensors obtained from a\n`torch.save()` call---i.e. basically equivalent to the checkpoint we'd been\nusing for the locally trained GPT2 model we used.  For whatever reason, that\none didn't interest me much, but `model.safetensors` did.\n\nThankfully, I could `pip install safetensors` in the free-threaded `py313t`\nenvironment, so I wrote some [helper glue](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/util.py#L178)\nto give me back a `HuggingFaceModel` with the tensors accessible via a\n`safetensors` attribute if I passed it the appropriate model name, e.g.:\n\n```python\n@dataclass\nclass HuggingFaceModel:\n    name: str\n    config: dict\n    safetensors: \"safetensors.safe_open\"\n    tokenizer: dict\n    tokenizer_config: dict\n    vocab: dict\n\ndef get_huggingface_model(model_name: str) -> HuggingFaceModel:\n    \"\"\"\n    Returns a Hugging Face model object for the given model name.\n\n    Args:\n\n        model_name (str): Supplies the name of the Hugging Face model.  This\n            should be in the format of `namespace/model`, e.g. for GPT2 XL:\n            `openai-community/gpt2-xl`.  This will be expanded out to the\n            following directory:\n                `~/.cache/huggingface/hub/models--openai-community--gpt2-xl`\n\n    Returns:\n\n        HuggingFaceModel: Returns a HuggingFaceModel object containing the\n            model name, configuration, and SafeTensors object.\n    \"\"\"\n    base = os.path.expanduser('~/.cache/huggingface/hub/models--')\n    (namespace, model) = model_name.split('/')\n    base_path = f'{base}{namespace}--{model}'\n    ref_path = f'{base_path}/refs/main'\n    with open(ref_path, 'r') as f:\n        ref = f.read().strip()\n    snapshots_dir = f'{base_path}/snapshots/{ref}'\n    safetensors_path = f'{snapshots_dir}/model.safetensors'\n    import safetensors\n    timer = ElapsedTimer()\n    logging.debug(f'About to load safetensors from {safetensors_path}...')\n    with timer:\n        st = safetensors.safe_open(\n            safetensors_path,\n            framework=\"pt\",\n            device=\"cpu\",\n        )\n    msg = (\n        f'Loaded safetensors from {safetensors_path} '\n        f'in {timer.elapsed:.4f} seconds.'\n    )\n    logging.info(msg)\n\n    config_path = f'{snapshots_dir}/config.json'\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n\n    tokenizer_path = f'{snapshots_dir}/tokenizer.json'\n    with open(tokenizer_path, 'r') as f:\n        tokenizer = json.load(f)\n\n    tokenizer_config_path = f'{snapshots_dir}/tokenizer_config.json'\n    with open(tokenizer_config_path, 'r') as f:\n        tokenizer_config = json.load(f)\n\n    vocab_path = f'{snapshots_dir}/vocab.json'\n    with open(vocab_path, 'r') as f:\n        vocab = json.load(f)\n\n    return HuggingFaceModel(\n        model_name,\n        config,\n        st,\n        tokenizer,\n        tokenizer_config,\n        vocab,\n    )\n```\n\nNow, I remember Andrej had a [`GPT.from_pretrained()`](\nhttps://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py#L130)\nroutine that was geared toward loading the GPT2 models via the\n`transformers` Python package along the following lines:\n\n```python\nfrom transformers import GPT2LMHeadModel\nhf_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n```\n\nThe technique he used to prime is local `GPT` class from the larger model\nloaded from HuggingFace piqued my interest.  I've reproduced the applicable\ncode below, with some formatting tweaks only.\n\n```python\n# create a from-scratch initialized minGPT model\nconfig = GPTConfig(**config_args)\nmodel = GPT(config)\nsd = model.state_dict()\nsd_keys = sd.keys()\n# discard this mask / buffer, not a param\nsd_keys = [\n    key for key in sd_keys if not key.endswith('.attn.bias')\n]\n\n# init a huggingface/transformers model\nmodel_hf = GPT2LMHeadModel.from_pretrained(model_type)\nsd_hf = model_hf.state_dict()\n\n# copy while ensuring all of the parameters are aligned\n# and match in names and shapes\nsd_keys_hf = sd_hf.keys()\n\n# ignore `.attn.masked_bias`; just a buffer\nsd_keys_hf = [\n    key for key in sd_keys_hf\n        if not k.endswith('.attn.masked_bias')\n]\n\n# ditto; ignore `.attn.bias`, just the mask (buffer)\nsd_keys_hf = [\n    key for key in sd_keys_hf\n        if not key.endswith('.attn.bias')\n]\n\ntransposed = [\n    'attn.c_attn.weight',\n    'attn.c_proj.weight',\n    'mlp.c_fc.weight',\n    'mlp.c_proj.weight',\n]\n\n# basically the openai checkpoints use a \"Conv1D\" module,\n# but we only want to use a vanilla Linear, this means\n# that we have to transpose these weights when we import them\nassert len(sd_keys_hf) == len(sd_keys), (\n    f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n)\n\nfor k in sd_keys_hf:\n    if any(k.endswith(w) for w in transposed):\n        # special treatment for the Conv1D weights\n        # we need to transpose\n        assert sd_hf[k].shape[::-1] == sd[k].shape\n        with torch.no_grad():\n            sd[k].copy_(sd_hf[k].t())\n    else:\n        # vanilla copy over the other parameters\n        assert sd_hf[k].shape == sd[k].shape\n        with torch.no_grad():\n            sd[k].copy_(sd_hf[k])\n```\n\nDo all of that fiddling and *et voilà*, you've just primed your `GPT` model\nwith the OpenAI weights!\n\nThe reason Andrej's approach piqued my interest---in combination with poking\naround at a `safetensors` instance and realizing I could easily extract all\n147-or-so tensors by name---was that it may allow us to copy the tensors\nfrom the model read from disk into *our* model in parallel, using multiple\nthreads.\n\nSo I hacked together a variant of Andrej's `GPT.from_pretrained()` that\nlooked as follows:\n\n```python\nclass GPT:\n\n    ...\n\n    @classmethod\n    def from_pretrained(cls,\n                        model_name: str,\n                        map_location: Optional[str] = None,\n                        manual_seed: Optional[int] = None,\n                        torch_profile_activities: Optional[List[type]] = None,\n                        ) -> \"GPT\":\n        \"\"\"\n        Load a GPT model from a pretrained model.\n\n        Arguments:\n\n            model_name (str): Supplies the model name to use.  See the\n                docstring for `.util.get_huggingface_safetensors()` for\n                more information about the format.\n\n            map_location (str): Optionally supplies the device to map the\n                loaded tensor parameters to.  If None, \"cuda\" will be used\n                if available, otherwise \"cpu\".\n\n            manual_seed (int): Optionally supplies the manual seed to use for\n                the model.  If None, a random seed will be used.\n\n            torch_profile_activities (list): Optionally supplies a list of\n                torch.profiler.ProfilerActivity to profile.\n\n        \"\"\"\n        if manual_seed is None:\n            # Use a random seed.\n            manual_seed = random.randint(0, 2**32 - 1)\n\n        if map_location is None:\n            if torch.cuda.is_available():\n                map_location = \"cuda\"\n            else:\n                map_location = \"cpu\"\n\n        timer = ElapsedTimer()\n        with timer:\n            hf_model = get_huggingface_model(model_name)\n        msg = (\n            f'Loaded HuggingFace model {model_name} in '\n            f'{timer.elapsed:.3f} seconds.'\n        )\n        logging.info(msg)\n\n        config = GPTConfig(**{\n            'block_size': hf_model.config['n_ctx'],\n            'vocab_size': hf_model.config['vocab_size'],\n            'n_layer': hf_model.config['n_layer'],\n            'n_head': hf_model.config['n_head'],\n            'n_embd': hf_model.config['n_embd'],\n        })\n        checkpoint = GPTCheckpoint(**{\n            'model': None,\n            'step': 0,\n            'val_loss': 0.0,\n            'config': config,\n        })\n\n        with timer:\n            model = cls(\n                checkpoint=checkpoint,\n                device=map_location,\n                manual_seed=manual_seed,\n                torch_profile_activities=torch_profile_activities,\n            )\n        logging.info(f'Created GPT model in {timer.elapsed:.3f} seconds.')\n\n        # This logic is based heavily off build-nanogpt's `train_gpt2.py`;\n        # specifically: GPT.from_pretrained().\n\n        exclude = ('.attn.bias', '.attn.masked_bias', 'lm_head.weight')\n        transpose = (\n            'attn.c_attn.weight',\n            'attn.c_proj.weight',\n            'mlp.c_fc.weight',\n            'mlp.c_proj.weight',\n        )\n\n        # Identify the HuggingFace keys we're interested in.\n        st = hf_model.safetensors\n\n        # Identify our model keys we're interested in.\n        sd = model.state_dict()\n        sd_keys = [k for k in sd.keys() if not k.endswith(exclude)]\n        hf_keys = [k.replace('transformer.', '') for k in sd_keys]\n\n        # Copying tensors in parallel yields decent speedups,\n        # at least on my V100s which have five concurrent copy\n        # engines.\n        def copy_tensor(hf_key, sd_key):                            # <1>\n            hf_tensor = st.get_tensor(hf_key)                       # <1>\n            if hf_key.endswith(transpose):                          # <1>\n                assert hf_tensor.shape[::-1] == sd[sd_key].shape    # <1>\n                with torch.no_grad():                               # <1>\n                    sd[sd_key].copy_(hf_tensor.t())                 # <1>\n            else:                                                   # <1>\n                assert hf_tensor.shape == sd[sd_key].shape          # <1>\n                with torch.no_grad():                               # <1>\n                    sd[sd_key].copy_(hf_tensor)                     # <1>\n\n        keys = zip(hf_keys, sd_keys)\n        max_workers = min(os.cpu_count(), len(sd_keys))\n        with timer:\n            with ThreadPoolExecutor(                                # <2>\n                max_workers=max_workers                             # <2>\n            ) as executor:                                          # <2>\n                futures = {                                         # <2>\n                    executor.submit(copy_tensor, hf_key, sd_key):   # <2>\n                        (hf_key, sd_key)                            # <2>\n                            for (hf_key, sd_key) in keys            # <2>\n                }                                                   # <2>\n                for future in as_completed(futures):                # <2>\n                    future.result()                                 # <2>\n        logging.info(\n            f'Copied weights with {max_workers} thread(s) '\n            f'in {timer.elapsed:.3f} seconds.'\n        )\n\n        device = map_location\n        with timer:\n            model.to(device)\n        msg = f'Moved model to {device} in {timer.elapsed:.3f} seconds.'\n        logging.info(msg)\n\n        return model\n```\n1. Define a `copy_tensor` function that is provided with the name of a\n   tensor key as it appears in the HuggingFace model (`hf_key`) and as\n   it appears in our `state_dict` model (`sd_key`).  The `sd` state dict\n   is accessible to all threads, so they can simply copy their tensor\n   via `sd[sd_key].copy_(hf_tensor)`.\n\n2. Use a `ThreadPoolExecutor()` to dispatch the `copy_tensor` operations in\n   parallel.  In this case I'm using workers equivalent to the number of CPU\n   cores, *or* number of tensors, whichever is fewer.\n\nSo that all worked pretty well, free-threading can absolutely be used to\nspeed up things like loading tensors.\n\nAdditionally, if you play around with this locally, the `gpt2-xl` model will\nbe available to select from the drop-down in the UI and also can be used via\nthe command line (`--model gpt2-xl`) or in the REST `/generate` endpoint as\na query parameter (`/generate/foo...?model=gpt2-xl`).\n\n### Multi-GPU Support\n\nThe final big change I introduced was multi-GPU support plus some very\nrudimentary round-robin-esque behavior that could be used from the\n`generate()` routines to obtain a reference to a model depending on the\nincoming user's request.\n\nThe `generate()` routine now obtains a model by the following:\n\n```python\nmodel = get_next_model()\n```\n\nThe implementation for which is [here](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py#L103),\nreproduced in part below:\n\n```python\nNUM_GPUS = torch.cuda.device_count()\n# Add a CPU version at the end.\nTOTAL_MODELS = NUM_GPUS + 1\nMODELS = [None] * TOTAL_MODELS\nMODELS_ROUND_ROBIN = itertools.cycle(range(TOTAL_MODELS))\n\ndef get_next_model_random():\n    # Randomly select a GPU to use.\n    return MODELS[random.randint(0, TOTAL_MODELS - 1)]\n\ndef get_next_model_round_robin():\n    with MODELS_LOCK:\n        index = next(MODELS_ROUND_ROBIN)\n    return MODELS[index]\n\nget_next_model = get_next_model_round_robin\n\ndef load_models():\n    global MODELS\n    max_workers = min(TOTAL_MODELS, os.cpu_count())\n    timer = ElapsedTimer()\n    with timer:\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {\n                executor.submit(\n                    GPT.from_local_pretrained,\n                    model_path=MODEL_CHECKPOINT,\n                    map_location=f'cuda:{i}',\n                    torch_profile_activities=TORCH_PROFILE_ACTIVITIES,\n                ): i for i in range(NUM_GPUS)\n            }\n            # Add the CPU model.\n            futures[executor.submit(\n                GPT.from_local_pretrained,\n                model_path=MODEL_CHECKPOINT,\n                map_location='cpu',\n                torch_profile_activities=TORCH_PROFILE_ACTIVITIES,\n            )] = NUM_GPUS\n            for future in as_completed(futures):\n                i = futures[future]\n                model = future.result()\n                MODELS[i] = model\n    msg = (\n        f'Loaded model on {NUM_GPUS} GPU(s) and 1 CPU in '\n        f'{timer.elapsed:.3f} seconds.'\n    )\n    logging.info(msg)\n\nPRETRAINED_MODELS = [None] * TOTAL_MODELS\nPRETRAINED_MODELS_ROUND_ROBIN = itertools.cycle(range(TOTAL_MODELS))\n\ndef get_next_pretrained_model_random():\n    # Randomly select a GPU to use.\n    return PRETRAINED_MODELS[random.randint(0, TOTAL_MODELS - 1)]\n\ndef get_next_pretrained_model_round_robin():\n    with PRETRAINED_MODELS_LOCK:\n        index = next(PRETRAINED_MODELS_ROUND_ROBIN)\n    return PRETRAINED_MODELS[index]\n\nget_next_pretrained_model = get_next_pretrained_model_round_robin\n\ndef load_pretrained_models():\n    global PRETRAINED_MODELS\n    max_workers = min(TOTAL_MODELS, os.cpu_count())\n    timer = ElapsedTimer()\n    with timer:\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {\n                executor.submit(\n                    GPT.from_pretrained,\n                    model_name='openai-community/gpt2-xl',\n                    map_location=f'cuda:{i}',\n                    torch_profile_activities=TORCH_PROFILE_ACTIVITIES,\n                ): i for i in range(NUM_GPUS)\n            }\n            # Add the CPU model.\n            futures[executor.submit(\n                GPT.from_pretrained,\n                model_name='openai-community/gpt2-xl',\n                map_location='cpu',\n                torch_profile_activities=TORCH_PROFILE_ACTIVITIES,\n            )] = NUM_GPUS\n            for future in as_completed(futures):\n                i = futures[future]\n                model = future.result()\n                PRETRAINED_MODELS[i] = model\n    msg = (\n        f'Loaded gpt2-xl model on {NUM_GPUS} GPU(s) and 1 CPU in '\n        f'{timer.elapsed:.3f} seconds.'\n    )\n    logging.info(msg)\n```\n\n## Model Optimization\n\nTwo facilities are available to speed up your model in PyTorch:\n[TorchScript](https://pytorch.org/docs/stable/jit.html), and [Torch Dynamo](\nhttps://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html), with\nthe latter being a newer approach than the former.\n\nIn our [parallelopedia.gpt2](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)\nmodule, our `Gpt2App` class has a `classmethod` named `init_once()`, which,\nas you can probably guess, gets called once by the HTTP server when starting\nup.  This is where you stash expensive setup code like loading and compiling\nmodels.\n\nThe code looks similar to the following.  We have two globals,\n`TORCH_JIT_COMPILE` and `TORCH_DYNAMO_COMPILE` that, when set, will attempt\nto optimize the models using the selected method.\n\n```python\n\nclass Gpt2App(HttpApp):\n\n    ...\n\n    @classmethod\n    def init_once(cls):\n        load_models()\n        load_pretrained_models()\n\n        global MODELS, PRETRAINED_MODELS\n\n        # This doesn't work because torch.jit doesn't handle our\n        # async generator.\n        global TRY_JIT_COMPILE\n        if TRY_JIT_COMPILE:\n            for (i, model) in enumerate(MODELS):\n                model.config = dataclasses.asdict(model.config)\n                timer = ElapsedTimer()\n                with timer:\n                    model = torch.jit.script(model)\n                    MODELS[i] = model\n                logging.info(\n                    f'JIT compiled model {i} in {timer.elapsed:.3f} seconds.'\n                )\n\n        global TRY_TORCH_COMPILE\n        if TRY_TORCH_COMPILE:\n            for (i, model) in enumerate(MODELS):\n                model.config = dataclasses.asdict(model.config)\n                timer = ElapsedTimer()\n                with timer:\n                    model = torch.compile(model)\n                    MODELS[i] = model\n                logging.info(\n                    f'torch.compiled model {i} in '\n                    f'{timer.elapsed:.3f} seconds.'\n                )\n\n            for (i, model) in enumerate(PRETRAINED_MODELS):\n                model.config = dataclasses.asdict(model.config)\n                timer = ElapsedTimer()\n                with timer:\n                    model = torch.compile(model)\n                    PRETRAINED_MODELS[i] = model\n                logging.info(\n                    f'torch.compiled pretrained model {i} in '\n                    f'{timer.elapsed:.3f} seconds.'\n                )\n```\n### TorchScript\n\nTorchScript doesn't work at all for our model---it balks on the `async def\ngenerate_async_for()` routine that is the workhorse of our asynchronous\ntoken-by-token generation.\n\nAnd that ended my TorchScript experiment :-)\n\n### Torch Dynamo (torch.compile)\n\nTorch Dynamo is a new feature that was introduced by PyTorch 2.0 that hooks\ninto the Python interpreter and traces model execution and then builds\noptimized kernels based on the information observed during runtime tracing.\n\nWhen it works, it works really well, and you can get significant speedups\nboth in training and inference with literally a single line:\n\n```python\nmodel = torch.compile(model)\n```\n\nThe first problem we hit with Dynamo is that it's explicitly not supported\nby PyTorch on free-threaded builds:\n\n```python\n>>> model = torch.compile(model)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[3], line 1\n----> 1 model = torch.compile(model)\n\nFile ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/__init__.py:2526, in compile(model, fullgraph, dynamic, backend, mode, options, disable)\n   2524     raise RuntimeError(\"torch.compile is not supported on Python 3.14+\")\n   2525 elif sysconfig.get_config_var(\"Py_GIL_DISABLED\") == 1:\n-> 2526     raise RuntimeError(\n   2527         \"torch.compile is not supported on Python built with GIL disabled\"\n   2528     )\n   2530 # Decorator mode\n   2531 if model is None:\n\nRuntimeError: torch.compile is not supported on Python built with GIL disabled\n```\n\nLet's hack that `torch/__init__.py` file as follows and try again.\n\n```diff\n--- __init__.py.orig    2025-02-09 13:28:27.892979258 -0800\n+++ __init__.py 2025-02-09 13:30:13.879909529 -0800\n@@ -2523,9 +2523,7 @@\n     if sys.version_info >= (3, 14):\n         raise RuntimeError(\"torch.compile is not supported on Python 3.14+\")\n     elif sysconfig.get_config_var(\"Py_GIL_DISABLED\") == 1:\n-        raise RuntimeError(\n-            \"torch.compile is not supported on Python built with GIL disabled\"\n-        )\n+        print(\"torch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\")\n\n     # Decorator mode\n     if model is None:\n```\n\n```python\n>>> model = torch.compile(model)\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[3], line 1\n----> 1 model = torch.compile(model)\n\nFile ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/__init__.py:2563, in compile(model, fullgraph, dynamic, backend, mode, options, disable)\n   2560 else:\n   2561     backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n-> 2563 return torch._dynamo.optimize(\n   2564     backend=backend,\n   2565     nopython=fullgraph,\n   2566     dynamic=dynamic,\n   2567     disable=disable,\n   2568 )(model)\n\nFile ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/_dynamo/eval_frame.py:842, in optimize(*args, **kwargs)\n    839         kwargs[\"nopython\"] = ca_kwargs_override[\"fullgraph\"]\n    840     return optimize(*args, **kwargs)\n--> 842 return _optimize(rebuild_ctx, *args, **kwargs)\n\nFile ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/_dynamo/eval_frame.py:881, in _optimize(rebuild_ctx, backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\n    845 def _optimize(\n    846     rebuild_ctx: Callable[[], Union[OptimizeContext, _NullDecorator]],\n    847     backend=\"inductor\",\n   (...)\n    853     dynamic=None,\n    854 ) -> Union[OptimizeContext, _NullDecorator]:\n    855     \"\"\"\n    856     The main entrypoint of TorchDynamo.  Do graph capture and call\n    857     backend() to optimize extracted graphs.\n   (...)\n    879             ...\n    880     \"\"\"\n--> 881     check_if_dynamo_supported()\n    882     # Note: The hooks object could be global instead of passed around, *however* that would make\n    883     # for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.\n    884     # There is some prior art around this, w/r/t nesting backend calls are enforced to be the same\n    885     # compiler, however, this feels onerous for callback and hooks, and it feels better to give our users an\n    886     # easier to understand UX at the cost of a little more plumbing on our end.\n    887     hooks = Hooks(guard_export_fn=guard_export_fn, guard_fail_fn=guard_fail_fn)\n\nFile ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/_dynamo/eval_frame.py:805, in check_if_dynamo_supported()\n    803     raise RuntimeError(\"Python 3.14+ not yet supported for torch.compile\")\n    804 elif sysconfig.get_config_var(\"Py_GIL_DISABLED\") == 1:\n--> 805     raise RuntimeError(\n    806         \"torch.compile is not supported on Python built with GIL disabled\"\n    807     )\n\nRuntimeError: torch.compile is not supported on Python built with GIL disabled\n```\n\nLet's hack `torch/_dynamo/eval_frame.py` too:\n\n```diff\n--- eval_frame.py.orig  2025-02-09 13:32:18.266470283 -0800\n+++ eval_frame.py       2025-02-09 13:32:32.746291774 -0800\n@@ -802,9 +802,7 @@\n     if sys.version_info >= (3, 14):\n         raise RuntimeError(\"Python 3.14+ not yet supported for torch.compile\")\n     elif sysconfig.get_config_var(\"Py_GIL_DISABLED\") == 1:\n-        raise RuntimeError(\n-            \"torch.compile is not supported on Python built with GIL disabled\"\n-        )\n+        print(\"torch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\")\n\n\n def is_dynamo_supported():\n```\n\nNow let's try again:\n\n```python\n>>> model = torch.compile(model)\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n>>>\n```\n\nSo, we can forcibly coerce PyTorch Dynamo to compile our model even if we're\nin free-threaded mode.\n\nBut does it work?  And is it faster?  Let's investigate.\n\nWe can invoke our [`parallelopedia.gpt2`](\nhttps://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)\nmodule directly with various command line arguments to test out generation\nperformance.  The accompanying `--help` is furnished below for reference:\n\n```bash\n% python -m parallelopedia.gpt2 --help\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\nusage: gpt2.py [-h] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--model {gpt2,gpt2-xl}] [--device DEVICE]\n               [--max-length MAX_LENGTH] [--top-k TOP_K] [--seed SEED] [--prompt PROMPT] [--torch-compile] [--torch-jit]\n               [--torch-compile-fullgraph] [--torch-compile-reduce-overhead] [--torch-compile-max-autotune]\n               [--generate-slim] [--rounds ROUNDS] [--wrap WRAP] [--note NOTE]\n\nRun the GPT2 module.\n\noptions:\n  -h, --help            show this help message and exit\n  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}\n                        Set the logging level.\n  --model {gpt2,gpt2-xl}\n                        Select the model to use.\n  --device DEVICE       Select the device to use.\n  --max-length MAX_LENGTH\n                        Set the maximum length of the generated text.\n  --top-k TOP_K         Set the top-k value for sampling.\n  --seed SEED           Set the random seed for generation.\n  --prompt PROMPT       Set the prompt for generation.\n  --torch-compile       Compile the models using torch.compile().\n  --torch-jit           Compile the models using torch.jit.script().\n  --torch-compile-fullgraph\n                        Compile the models using torch.compile() with fullgraph=True.\n  --torch-compile-reduce-overhead\n                        Compile the models using torch.compile() with mode=\"reduce-overhead\"\n  --torch-compile-max-autotune\n                        Compile the models using torch.compile() with mode=\"max_autotune\".\n  --generate-slim       Use the generate_slim() method for generation.\n  --rounds ROUNDS       Set the number of rounds for generation.\n  --wrap WRAP           Set the wrap width for text output.\n  --note NOTE           Set a note to include in the JSON output.\n```\n\nThe full source code for the module's `main()` function follows.  Based on\nthe command line parameters furnished, we can test various generation\nvariants, such as no `torch.compile()`, `torch.compile()` and nothing else,\nand more advanced permutations such as the following:\n\n```python\nkwds = {\n    'fullgraph': True,\n    'model': 'max-autotune',\n}\nmodel = torch.compile(model, **kwds)\n```\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Full Source Code for parallelopedia.gpt2.main()\"}\n\n```python\ndef main():\n    \"\"\"\n    Main entry point for the parallelopedia.gpt2 module.\n    \"\"\"\n    args = parse_arguments()\n\n    logging.basicConfig(\n        level=getattr(logging, args.log_level),\n        format='%(asctime)s - %(levelname)s - %(message)s',\n    )\n\n    start_time = time.time()\n    start_timestamp = datetime.datetime.now().isoformat()\n\n    timer = ElapsedTimer()\n    with timer:\n        if args.model == 'gpt2-xl':\n            model = GPT.from_pretrained(\n                model_name='openai-community/gpt2-xl',\n                map_location=args.device,\n            )\n        else:\n            model = GPT.from_local_pretrained(\n                model_path=MODEL_CHECKPOINT,\n                map_location=args.device,\n                manual_seed=args.seed,\n            )\n    logging.info(\n        f'Loaded {args.model} on {args.device} '\n        f'in {timer.elapsed:.3f} seconds.'\n    )\n\n    if args.torch_compile:\n        if args.torch_jit:\n            msg = 'Cannot specify both --torch-compile and --torch-jit.'\n            raise ValueError(msg)\n        model.config = dataclasses.asdict(model.config)\n        kwds = {}\n        if args.torch_compile_fullgraph:\n            kwds['fullgraph'] = True\n        if args.torch_compile_reduce_overhead:\n            if args.torch_compile_max_autotune:\n                msg = (\n                    'Cannot specify both --torch-compile-reduce-overhead '\n                    'and --torch-compile-max-autotune.'\n                )\n                raise ValueError(msg)\n            kwds['mode'] = 'reduce-overhead'\n        elif args.torch_compile_max_autotune:\n            kwds['mode'] = 'max-autotune'\n        with timer:\n            model = torch.compile(model, **kwds)\n        msg = f'torch.compiled model in {timer.elapsed:.3f} seconds.'\n        logging.info(msg)\n    elif args.torch_jit:\n        model.config = dataclasses.asdict(model.config)\n        with timer:\n            model = torch.jit.script(model)\n        msg = f'JIT compiled model in {timer.elapsed:.3f} seconds.')\n        logging.info(msg)\n\n    seed = args.seed\n    if seed is None or seed == '':\n        seed = random.randint(0, 2**32 - 1)\n\n    if args.generate_slim:\n        text_tokens = model.enc.encode(args.prompt)\n        prompt_token_length = len(text_tokens)\n\n    rates = []\n    for i in range(args.rounds):\n        logging.info(f'Round {i + 1} of {args.rounds}.')\n        if args.generate_slim:\n            with timer:\n                x = model.generate_slim(\n                    text_tokens,\n                    max_length=args.max_length,\n                    top_k=args.top_k,\n                    seed=seed,\n                )\n            elapsed = timer.elapsed\n            count = x.size(1) - prompt_token_length\n            tokens_per_sec = count / elapsed\n            rates.append(tokens_per_sec)\n            logging.info(\n                f'Generated {count} tokens in {elapsed:.2f} seconds '\n                f'({tokens_per_sec:.2f} tokens/sec)'\n            )\n            output = model.enc.decode(x[0].tolist())\n        else:\n            save_rate = lambda x: rates.append(x)\n            output = model.generate(\n                args.prompt,\n                max_length=args.max_length,\n                top_k=args.top_k,\n                seed=seed,\n                save_rate=save_rate,\n            )\n\n        if args.wrap:\n            output = textwrap.fill(output, width=args.wrap)\n        logging.info(f'Output:\\n{output}')\n\n    # The filename is of the form:\n    #   `gpt2-rates-<yyyy-mm-dd-hh-ss-mm.sss>-[optional].json`\n    now = datetime.datetime.now()\n    timestamp = now.strftime('%Y-%m-%d-%H-%M-%S-%f')\n    filename = f\"gpt2-rates-{timestamp}\"\n    if args.torch_compile:\n        filename += '-torch-compile'\n        if args.torch_compile_reduce_overhead:\n            filename += '-reduce-overhead'\n        elif args.torch_compile_max_autotune:\n            filename += '-max-autotune'\n        if args.torch_compile_fullgraph:\n            filename += '-fullgraph'\n    if args.generate_slim:\n        filename += '-generate-slim'\n\n    conda_env_name = os.environ.get('CONDA_DEFAULT_ENV', 'Unknown')\n    filename += f'-{conda_env_name}'\n\n    filename += '.json'\n\n    if not isinstance(model.config, dict):\n        model_config = dataclasses.asdict(model.config)\n    else:\n        model_config = model.config\n\n    end_time = time.time()\n    end_timestamp = datetime.datetime.now().isoformat()\n\n    if args.device.startswith('cuda'):\n        ix = args.device.find(':')\n        if ix == -1:\n            device_index = 0\n        else:\n            device_index = int(args.device[ix+1:])\n\n        device_name = torch.cuda.get_device_name(device_index)\n    else:\n        device_name = 'CPU'\n\n    try:\n        is_gil_enabled = sys._is_gil_enabled()\n    except AttributeError:\n        is_gil_enabled = False\n\n    # Prepare a dictionary with the details to save.\n    run_details = {\n        \"rates\": rates,\n        \"model_config\": model_config,\n        \"args\": vars(args),\n        \"start_timestamp\": start_timestamp,\n        \"end_timestamp\": end_timestamp,\n        \"elapsed\": f'{end_time - start_time:.3f}',\n        \"device_name\": device_name,\n        \"conda_env_name\": conda_env_name,\n        \"is_gil_enabled\": is_gil_enabled,\n        \"note\": args.note,\n    }\n\n    # Write the JSON file.\n    with open(filename, \"w\") as json_file:\n        json.dump(run_details, json_file, indent=4)\n\n    logging.info(f\"Run details saved to {filename}.\")\n\n```\n:::\n\nLet's take a look at whether or not `torch.compile()` improves performance\nin our `py313t` free-threaded environment, first.\n\n#### Performance Comparison\n\nI wrote a `bash` script [`run-py313t-gpt2-compile-tests.sh`](\nhttps://github.com/tpn/website/blob/main/articles/pytorch-and-python-free-threading/run-py313t-gpt2-compile-tests.sh),\nreproduced below, that ran various permutations of generation with different\n`torch.compile()` options.\n\n```bash\n#!/bin/bash\n\n# Ensure our environment name is `py313t`.\nif [ \"$CONDA_DEFAULT_ENV\" != \"py313t\" ]; then\n  echo \"Error: Conda environment is not 'py313t'.\"\n  exit 1\nfi\n\n# Ensure PARALLELOPEDIA_ROOT is set.\nif [ -z \"$PARALLELOPEDIA_ROOT\" ]; then\n  echo \"Error: PARALLELOPEDIA_ROOT is not set.\"\n  exit 1\nfi\n\n\nSEED=42\nDEVICE=\"cuda:3\"\nROUNDS=20\n\nOPTIONS=(\n  \"--torch-compile\"\n  \"--torch-compile --torch-compile-fullgraph\"\n  \"--torch-compile --torch-compile-reduce-overhead\"\n  \"--torch-compile --torch-compile-reduce-overhead --torch-compile-fullgraph\"\n  \"--torch-compile --torch-compile-reduce-overhead --torch-compile-fullgraph\"\n  \"--torch-compile --torch-compile-max-autotune\"\n  \"--torch-compile --torch-compile-max-autotune --torch-compile-fullgraph\"\n)\n\necho \"GPT.generate() variants\"\ntime python -Xgil=0 -m parallelopedia.gpt2    \\\n    --seed=$SEED                              \\\n    --rounds=$ROUNDS                          \\\n    --device=$DEVICE\n\nfor opt in \"${OPTIONS[@]}\"; do\n    # Split opt into separate arguments.\n    eval set -- $opt\n    time python -Xgil=0 -m parallelopedia.gpt2    \\\n        --seed=$SEED                              \\\n        --rounds=$ROUNDS                          \\\n        --device=$DEVICE                          \\\n        \"$@\"\ndone\n```\n\nThe full log file for the run is captured in the callout below.\n\n::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"run-py313t-gpt2-compile-tests.log\"}\n\n```bash\nGPT.generate() variants\n2025-02-10 20:47:17,958 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.463 seconds.\n2025-02-10 20:47:17,965 - INFO - Initialized GPT model in 0.007 seconds.\n2025-02-10 20:47:18,191 - INFO - Loaded model weights in 0.226 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:47:18,558 - INFO - Created GPT model in 0.600 seconds.\n2025-02-10 20:47:18,642 - INFO - Moved model to cuda:3 in 0.083 seconds.\n2025-02-10 20:47:18,642 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:47:18,642 - INFO - Loaded gpt2 on cuda:3 in 1.148 seconds.\n2025-02-10 20:47:18,642 - INFO - Round 1 of 20.\n2025-02-10 20:47:19,454 - INFO - Generated 91 tokens in 0.81 seconds (112.32 tokens/sec)\n2025-02-10 20:47:19,454 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:19,454 - INFO - Round 2 of 20.\n2025-02-10 20:47:20,091 - INFO - Generated 91 tokens in 0.64 seconds (142.89 tokens/sec)\n2025-02-10 20:47:20,092 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:20,092 - INFO - Round 3 of 20.\n2025-02-10 20:47:20,731 - INFO - Generated 91 tokens in 0.64 seconds (142.49 tokens/sec)\n2025-02-10 20:47:20,731 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:20,731 - INFO - Round 4 of 20.\n2025-02-10 20:47:21,366 - INFO - Generated 91 tokens in 0.63 seconds (143.47 tokens/sec)\n2025-02-10 20:47:21,366 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:21,366 - INFO - Round 5 of 20.\n2025-02-10 20:47:22,000 - INFO - Generated 91 tokens in 0.63 seconds (143.62 tokens/sec)\n2025-02-10 20:47:22,000 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:22,000 - INFO - Round 6 of 20.\n2025-02-10 20:47:22,634 - INFO - Generated 91 tokens in 0.63 seconds (143.62 tokens/sec)\n2025-02-10 20:47:22,635 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:22,635 - INFO - Round 7 of 20.\n2025-02-10 20:47:23,270 - INFO - Generated 91 tokens in 0.64 seconds (143.25 tokens/sec)\n2025-02-10 20:47:23,271 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:23,271 - INFO - Round 8 of 20.\n2025-02-10 20:47:23,904 - INFO - Generated 91 tokens in 0.63 seconds (143.67 tokens/sec)\n2025-02-10 20:47:23,905 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:23,905 - INFO - Round 9 of 20.\n2025-02-10 20:47:24,539 - INFO - Generated 91 tokens in 0.63 seconds (143.56 tokens/sec)\n2025-02-10 20:47:24,539 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:24,539 - INFO - Round 10 of 20.\n2025-02-10 20:47:25,174 - INFO - Generated 91 tokens in 0.63 seconds (143.56 tokens/sec)\n2025-02-10 20:47:25,174 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:25,174 - INFO - Round 11 of 20.\n2025-02-10 20:47:25,812 - INFO - Generated 91 tokens in 0.64 seconds (142.78 tokens/sec)\n2025-02-10 20:47:25,812 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:25,812 - INFO - Round 12 of 20.\n2025-02-10 20:47:26,444 - INFO - Generated 91 tokens in 0.63 seconds (143.92 tokens/sec)\n2025-02-10 20:47:26,445 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:26,445 - INFO - Round 13 of 20.\n2025-02-10 20:47:27,080 - INFO - Generated 91 tokens in 0.63 seconds (143.34 tokens/sec)\n2025-02-10 20:47:27,080 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:27,081 - INFO - Round 14 of 20.\n2025-02-10 20:47:27,716 - INFO - Generated 91 tokens in 0.63 seconds (143.34 tokens/sec)\n2025-02-10 20:47:27,716 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:27,716 - INFO - Round 15 of 20.\n2025-02-10 20:47:28,350 - INFO - Generated 91 tokens in 0.63 seconds (143.65 tokens/sec)\n2025-02-10 20:47:28,350 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:28,350 - INFO - Round 16 of 20.\n2025-02-10 20:47:28,984 - INFO - Generated 91 tokens in 0.63 seconds (143.73 tokens/sec)\n2025-02-10 20:47:28,984 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:28,984 - INFO - Round 17 of 20.\n2025-02-10 20:47:29,617 - INFO - Generated 91 tokens in 0.63 seconds (143.79 tokens/sec)\n2025-02-10 20:47:29,618 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:29,618 - INFO - Round 18 of 20.\n2025-02-10 20:47:30,260 - INFO - Generated 91 tokens in 0.64 seconds (141.71 tokens/sec)\n2025-02-10 20:47:30,261 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:30,261 - INFO - Round 19 of 20.\n2025-02-10 20:47:30,896 - INFO - Generated 91 tokens in 0.63 seconds (143.36 tokens/sec)\n2025-02-10 20:47:30,896 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:30,896 - INFO - Round 20 of 20.\n2025-02-10 20:47:31,530 - INFO - Generated 91 tokens in 0.63 seconds (143.55 tokens/sec)\n2025-02-10 20:47:31,531 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:31,531 - INFO - Run details saved to gpt2-rates-2025-02-10-20-47-31-531204-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m19.736s\nuser\t0m22.944s\nsys\t0m1.382s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n2025-02-10 20:47:37,663 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.484 seconds.\n2025-02-10 20:47:37,670 - INFO - Initialized GPT model in 0.006 seconds.\n2025-02-10 20:47:37,896 - INFO - Loaded model weights in 0.226 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:47:38,263 - INFO - Created GPT model in 0.600 seconds.\n2025-02-10 20:47:38,347 - INFO - Moved model to cuda:3 in 0.083 seconds.\n2025-02-10 20:47:38,347 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:47:38,347 - INFO - Loaded gpt2 on cuda:3 in 1.168 seconds.\n2025-02-10 20:47:38,349 - INFO - torch.compiled model in 0.002 seconds.\n2025-02-10 20:47:38,349 - INFO - Round 1 of 20.\n2025-02-10 20:47:39,174 - INFO - Generated 91 tokens in 0.82 seconds (110.47 tokens/sec)\n2025-02-10 20:47:39,174 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:39,174 - INFO - Round 2 of 20.\n2025-02-10 20:47:39,828 - INFO - Generated 91 tokens in 0.65 seconds (139.24 tokens/sec)\n2025-02-10 20:47:39,829 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:39,829 - INFO - Round 3 of 20.\n2025-02-10 20:47:40,483 - INFO - Generated 91 tokens in 0.65 seconds (139.04 tokens/sec)\n2025-02-10 20:47:40,484 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:40,484 - INFO - Round 4 of 20.\n2025-02-10 20:47:41,138 - INFO - Generated 91 tokens in 0.65 seconds (139.23 tokens/sec)\n2025-02-10 20:47:41,138 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:41,138 - INFO - Round 5 of 20.\n2025-02-10 20:47:41,791 - INFO - Generated 91 tokens in 0.65 seconds (139.47 tokens/sec)\n2025-02-10 20:47:41,791 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:41,791 - INFO - Round 6 of 20.\n2025-02-10 20:47:42,445 - INFO - Generated 91 tokens in 0.65 seconds (139.29 tokens/sec)\n2025-02-10 20:47:42,445 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:42,445 - INFO - Round 7 of 20.\n2025-02-10 20:47:43,099 - INFO - Generated 91 tokens in 0.65 seconds (139.09 tokens/sec)\n2025-02-10 20:47:43,100 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:43,100 - INFO - Round 8 of 20.\n2025-02-10 20:47:43,755 - INFO - Generated 91 tokens in 0.66 seconds (138.85 tokens/sec)\n2025-02-10 20:47:43,756 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:43,756 - INFO - Round 9 of 20.\n2025-02-10 20:47:44,409 - INFO - Generated 91 tokens in 0.65 seconds (139.29 tokens/sec)\n2025-02-10 20:47:44,410 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:44,410 - INFO - Round 10 of 20.\n2025-02-10 20:47:45,063 - INFO - Generated 91 tokens in 0.65 seconds (139.29 tokens/sec)\n2025-02-10 20:47:45,064 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:45,064 - INFO - Round 11 of 20.\n2025-02-10 20:47:45,718 - INFO - Generated 91 tokens in 0.65 seconds (139.11 tokens/sec)\n2025-02-10 20:47:45,718 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:45,719 - INFO - Round 12 of 20.\n2025-02-10 20:47:46,371 - INFO - Generated 91 tokens in 0.65 seconds (139.47 tokens/sec)\n2025-02-10 20:47:46,372 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:46,372 - INFO - Round 13 of 20.\n2025-02-10 20:47:47,046 - INFO - Generated 91 tokens in 0.67 seconds (135.03 tokens/sec)\n2025-02-10 20:47:47,046 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:47,046 - INFO - Round 14 of 20.\n2025-02-10 20:47:47,692 - INFO - Generated 91 tokens in 0.65 seconds (140.90 tokens/sec)\n2025-02-10 20:47:47,693 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:47,693 - INFO - Round 15 of 20.\n2025-02-10 20:47:48,339 - INFO - Generated 91 tokens in 0.65 seconds (140.84 tokens/sec)\n2025-02-10 20:47:48,339 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:48,340 - INFO - Round 16 of 20.\n2025-02-10 20:47:48,986 - INFO - Generated 91 tokens in 0.65 seconds (140.90 tokens/sec)\n2025-02-10 20:47:48,986 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:48,986 - INFO - Round 17 of 20.\n2025-02-10 20:47:49,634 - INFO - Generated 91 tokens in 0.65 seconds (140.40 tokens/sec)\n2025-02-10 20:47:49,635 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:49,635 - INFO - Round 18 of 20.\n2025-02-10 20:47:50,282 - INFO - Generated 91 tokens in 0.65 seconds (140.63 tokens/sec)\n2025-02-10 20:47:50,283 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:50,283 - INFO - Round 19 of 20.\n2025-02-10 20:47:50,929 - INFO - Generated 91 tokens in 0.65 seconds (140.73 tokens/sec)\n2025-02-10 20:47:50,930 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:50,930 - INFO - Round 20 of 20.\n2025-02-10 20:47:51,578 - INFO - Generated 91 tokens in 0.65 seconds (140.41 tokens/sec)\n2025-02-10 20:47:51,579 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:51,579 - INFO - Run details saved to gpt2-rates-2025-02-10-20-47-51-579116-torch-compile-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m20.073s\nuser\t0m23.400s\nsys\t0m1.313s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n2025-02-10 20:47:57,975 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.486 seconds.\n2025-02-10 20:47:57,982 - INFO - Initialized GPT model in 0.006 seconds.\n2025-02-10 20:47:58,209 - INFO - Loaded model weights in 0.227 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:47:58,575 - INFO - Created GPT model in 0.599 seconds.\n2025-02-10 20:47:58,659 - INFO - Moved model to cuda:3 in 0.084 seconds.\n2025-02-10 20:47:58,659 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:47:58,659 - INFO - Loaded gpt2 on cuda:3 in 1.170 seconds.\n2025-02-10 20:47:58,660 - INFO - torch.compiled model in 0.001 seconds.\n2025-02-10 20:47:58,661 - INFO - Round 1 of 20.\n2025-02-10 20:47:59,483 - INFO - Generated 91 tokens in 0.82 seconds (110.79 tokens/sec)\n2025-02-10 20:47:59,483 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:47:59,483 - INFO - Round 2 of 20.\n2025-02-10 20:48:00,153 - INFO - Generated 91 tokens in 0.67 seconds (135.92 tokens/sec)\n2025-02-10 20:48:00,153 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:00,153 - INFO - Round 3 of 20.\n2025-02-10 20:48:00,796 - INFO - Generated 91 tokens in 0.64 seconds (141.66 tokens/sec)\n2025-02-10 20:48:00,796 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:00,796 - INFO - Round 4 of 20.\n2025-02-10 20:48:01,439 - INFO - Generated 91 tokens in 0.64 seconds (141.69 tokens/sec)\n2025-02-10 20:48:01,439 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:01,439 - INFO - Round 5 of 20.\n2025-02-10 20:48:02,086 - INFO - Generated 91 tokens in 0.65 seconds (140.83 tokens/sec)\n2025-02-10 20:48:02,086 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:02,086 - INFO - Round 6 of 20.\n2025-02-10 20:48:02,730 - INFO - Generated 91 tokens in 0.64 seconds (141.33 tokens/sec)\n2025-02-10 20:48:02,731 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:02,731 - INFO - Round 7 of 20.\n2025-02-10 20:48:03,374 - INFO - Generated 91 tokens in 0.64 seconds (141.45 tokens/sec)\n2025-02-10 20:48:03,375 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:03,375 - INFO - Round 8 of 20.\n2025-02-10 20:48:04,017 - INFO - Generated 91 tokens in 0.64 seconds (141.76 tokens/sec)\n2025-02-10 20:48:04,017 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:04,017 - INFO - Round 9 of 20.\n2025-02-10 20:48:04,661 - INFO - Generated 91 tokens in 0.64 seconds (141.46 tokens/sec)\n2025-02-10 20:48:04,661 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:04,661 - INFO - Round 10 of 20.\n2025-02-10 20:48:05,303 - INFO - Generated 91 tokens in 0.64 seconds (141.82 tokens/sec)\n2025-02-10 20:48:05,303 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:05,303 - INFO - Round 11 of 20.\n2025-02-10 20:48:05,945 - INFO - Generated 91 tokens in 0.64 seconds (141.91 tokens/sec)\n2025-02-10 20:48:05,945 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:05,945 - INFO - Round 12 of 20.\n2025-02-10 20:48:06,587 - INFO - Generated 91 tokens in 0.64 seconds (141.83 tokens/sec)\n2025-02-10 20:48:06,587 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:06,587 - INFO - Round 13 of 20.\n2025-02-10 20:48:07,229 - INFO - Generated 91 tokens in 0.64 seconds (141.93 tokens/sec)\n2025-02-10 20:48:07,229 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:07,229 - INFO - Round 14 of 20.\n2025-02-10 20:48:07,871 - INFO - Generated 91 tokens in 0.64 seconds (141.88 tokens/sec)\n2025-02-10 20:48:07,871 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:07,871 - INFO - Round 15 of 20.\n2025-02-10 20:48:08,513 - INFO - Generated 91 tokens in 0.64 seconds (141.74 tokens/sec)\n2025-02-10 20:48:08,514 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:08,514 - INFO - Round 16 of 20.\n2025-02-10 20:48:09,157 - INFO - Generated 91 tokens in 0.64 seconds (141.58 tokens/sec)\n2025-02-10 20:48:09,157 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:09,157 - INFO - Round 17 of 20.\n2025-02-10 20:48:09,799 - INFO - Generated 91 tokens in 0.64 seconds (141.79 tokens/sec)\n2025-02-10 20:48:09,800 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:09,800 - INFO - Round 18 of 20.\n2025-02-10 20:48:10,442 - INFO - Generated 91 tokens in 0.64 seconds (141.75 tokens/sec)\n2025-02-10 20:48:10,442 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:10,442 - INFO - Round 19 of 20.\n2025-02-10 20:48:11,084 - INFO - Generated 91 tokens in 0.64 seconds (141.77 tokens/sec)\n2025-02-10 20:48:11,085 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:11,085 - INFO - Round 20 of 20.\n2025-02-10 20:48:11,727 - INFO - Generated 91 tokens in 0.64 seconds (141.75 tokens/sec)\n2025-02-10 20:48:11,727 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:11,728 - INFO - Run details saved to gpt2-rates-2025-02-10-20-48-11-727901-torch-compile-fullgraph-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m20.125s\nuser\t0m23.218s\nsys\t0m1.382s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n2025-02-10 20:48:17,900 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.486 seconds.\n2025-02-10 20:48:17,907 - INFO - Initialized GPT model in 0.006 seconds.\n2025-02-10 20:48:18,132 - INFO - Loaded model weights in 0.225 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:48:18,494 - INFO - Created GPT model in 0.594 seconds.\n2025-02-10 20:48:18,577 - INFO - Moved model to cuda:3 in 0.082 seconds.\n2025-02-10 20:48:18,577 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:48:18,577 - INFO - Loaded gpt2 on cuda:3 in 1.162 seconds.\n2025-02-10 20:48:18,581 - INFO - torch.compiled model in 0.004 seconds.\n2025-02-10 20:48:18,581 - INFO - Round 1 of 20.\n2025-02-10 20:48:19,397 - INFO - Generated 91 tokens in 0.82 seconds (111.57 tokens/sec)\n2025-02-10 20:48:19,398 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:19,398 - INFO - Round 2 of 20.\n2025-02-10 20:48:20,044 - INFO - Generated 91 tokens in 0.65 seconds (140.99 tokens/sec)\n2025-02-10 20:48:20,044 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:20,044 - INFO - Round 3 of 20.\n2025-02-10 20:48:20,690 - INFO - Generated 91 tokens in 0.65 seconds (140.95 tokens/sec)\n2025-02-10 20:48:20,690 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:20,690 - INFO - Round 4 of 20.\n2025-02-10 20:48:21,335 - INFO - Generated 91 tokens in 0.64 seconds (141.16 tokens/sec)\n2025-02-10 20:48:21,336 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:21,336 - INFO - Round 5 of 20.\n2025-02-10 20:48:21,981 - INFO - Generated 91 tokens in 0.64 seconds (141.10 tokens/sec)\n2025-02-10 20:48:21,981 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:21,981 - INFO - Round 6 of 20.\n2025-02-10 20:48:22,627 - INFO - Generated 91 tokens in 0.65 seconds (141.03 tokens/sec)\n2025-02-10 20:48:22,627 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:22,627 - INFO - Round 7 of 20.\n2025-02-10 20:48:23,274 - INFO - Generated 91 tokens in 0.65 seconds (140.67 tokens/sec)\n2025-02-10 20:48:23,275 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:23,275 - INFO - Round 8 of 20.\n2025-02-10 20:48:23,950 - INFO - Generated 91 tokens in 0.68 seconds (134.77 tokens/sec)\n2025-02-10 20:48:23,950 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:23,951 - INFO - Round 9 of 20.\n2025-02-10 20:48:24,637 - INFO - Generated 91 tokens in 0.69 seconds (132.66 tokens/sec)\n2025-02-10 20:48:24,637 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:24,637 - INFO - Round 10 of 20.\n2025-02-10 20:48:25,293 - INFO - Generated 91 tokens in 0.66 seconds (138.78 tokens/sec)\n2025-02-10 20:48:25,294 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:25,294 - INFO - Round 11 of 20.\n2025-02-10 20:48:25,952 - INFO - Generated 91 tokens in 0.66 seconds (138.35 tokens/sec)\n2025-02-10 20:48:25,952 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:25,952 - INFO - Round 12 of 20.\n2025-02-10 20:48:26,609 - INFO - Generated 91 tokens in 0.66 seconds (138.65 tokens/sec)\n2025-02-10 20:48:26,609 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:26,609 - INFO - Round 13 of 20.\n2025-02-10 20:48:27,265 - INFO - Generated 91 tokens in 0.66 seconds (138.73 tokens/sec)\n2025-02-10 20:48:27,265 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:27,265 - INFO - Round 14 of 20.\n2025-02-10 20:48:27,922 - INFO - Generated 91 tokens in 0.66 seconds (138.71 tokens/sec)\n2025-02-10 20:48:27,922 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:27,922 - INFO - Round 15 of 20.\n2025-02-10 20:48:28,579 - INFO - Generated 91 tokens in 0.66 seconds (138.54 tokens/sec)\n2025-02-10 20:48:28,580 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:28,580 - INFO - Round 16 of 20.\n2025-02-10 20:48:29,236 - INFO - Generated 91 tokens in 0.66 seconds (138.70 tokens/sec)\n2025-02-10 20:48:29,236 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:29,236 - INFO - Round 17 of 20.\n2025-02-10 20:48:29,894 - INFO - Generated 91 tokens in 0.66 seconds (138.52 tokens/sec)\n2025-02-10 20:48:29,894 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:29,894 - INFO - Round 18 of 20.\n2025-02-10 20:48:30,550 - INFO - Generated 91 tokens in 0.66 seconds (138.78 tokens/sec)\n2025-02-10 20:48:30,550 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:30,550 - INFO - Round 19 of 20.\n2025-02-10 20:48:31,208 - INFO - Generated 91 tokens in 0.66 seconds (138.36 tokens/sec)\n2025-02-10 20:48:31,209 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:31,209 - INFO - Round 20 of 20.\n2025-02-10 20:48:31,869 - INFO - Generated 91 tokens in 0.66 seconds (137.96 tokens/sec)\n2025-02-10 20:48:31,869 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:31,869 - INFO - Run details saved to gpt2-rates-2025-02-10-20-48-31-869451-torch-compile-reduce-overhead-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m20.138s\nuser\t0m23.350s\nsys\t0m1.425s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n2025-02-10 20:48:38,069 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.481 seconds.\n2025-02-10 20:48:38,076 - INFO - Initialized GPT model in 0.006 seconds.\n2025-02-10 20:48:38,305 - INFO - Loaded model weights in 0.228 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:48:38,672 - INFO - Created GPT model in 0.602 seconds.\n2025-02-10 20:48:38,756 - INFO - Moved model to cuda:3 in 0.085 seconds.\n2025-02-10 20:48:38,756 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:48:38,757 - INFO - Loaded gpt2 on cuda:3 in 1.169 seconds.\n2025-02-10 20:48:38,759 - INFO - torch.compiled model in 0.002 seconds.\n2025-02-10 20:48:38,759 - INFO - Round 1 of 20.\n2025-02-10 20:48:39,584 - INFO - Generated 91 tokens in 0.82 seconds (110.43 tokens/sec)\n2025-02-10 20:48:39,584 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:39,584 - INFO - Round 2 of 20.\n2025-02-10 20:48:40,223 - INFO - Generated 91 tokens in 0.64 seconds (142.66 tokens/sec)\n2025-02-10 20:48:40,223 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:40,223 - INFO - Round 3 of 20.\n2025-02-10 20:48:40,863 - INFO - Generated 91 tokens in 0.64 seconds (142.18 tokens/sec)\n2025-02-10 20:48:40,864 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:40,864 - INFO - Round 4 of 20.\n2025-02-10 20:48:41,501 - INFO - Generated 91 tokens in 0.64 seconds (142.76 tokens/sec)\n2025-02-10 20:48:41,502 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:41,502 - INFO - Round 5 of 20.\n2025-02-10 20:48:42,140 - INFO - Generated 91 tokens in 0.64 seconds (142.55 tokens/sec)\n2025-02-10 20:48:42,141 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:42,141 - INFO - Round 6 of 20.\n2025-02-10 20:48:42,779 - INFO - Generated 91 tokens in 0.64 seconds (142.60 tokens/sec)\n2025-02-10 20:48:42,779 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:42,779 - INFO - Round 7 of 20.\n2025-02-10 20:48:43,416 - INFO - Generated 91 tokens in 0.64 seconds (142.91 tokens/sec)\n2025-02-10 20:48:43,417 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:43,417 - INFO - Round 8 of 20.\n2025-02-10 20:48:44,058 - INFO - Generated 91 tokens in 0.64 seconds (141.92 tokens/sec)\n2025-02-10 20:48:44,059 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:44,059 - INFO - Round 9 of 20.\n2025-02-10 20:48:44,700 - INFO - Generated 91 tokens in 0.64 seconds (141.97 tokens/sec)\n2025-02-10 20:48:44,700 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:44,700 - INFO - Round 10 of 20.\n2025-02-10 20:48:45,339 - INFO - Generated 91 tokens in 0.64 seconds (142.54 tokens/sec)\n2025-02-10 20:48:45,339 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:45,339 - INFO - Round 11 of 20.\n2025-02-10 20:48:45,977 - INFO - Generated 91 tokens in 0.64 seconds (142.78 tokens/sec)\n2025-02-10 20:48:45,977 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:45,977 - INFO - Round 12 of 20.\n2025-02-10 20:48:46,617 - INFO - Generated 91 tokens in 0.64 seconds (142.24 tokens/sec)\n2025-02-10 20:48:46,618 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:46,618 - INFO - Round 13 of 20.\n2025-02-10 20:48:47,264 - INFO - Generated 91 tokens in 0.65 seconds (140.87 tokens/sec)\n2025-02-10 20:48:47,264 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:47,265 - INFO - Round 14 of 20.\n2025-02-10 20:48:47,912 - INFO - Generated 91 tokens in 0.65 seconds (140.53 tokens/sec)\n2025-02-10 20:48:47,913 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:47,913 - INFO - Round 15 of 20.\n2025-02-10 20:48:48,561 - INFO - Generated 91 tokens in 0.65 seconds (140.36 tokens/sec)\n2025-02-10 20:48:48,562 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:48,562 - INFO - Round 16 of 20.\n2025-02-10 20:48:49,185 - INFO - Generated 91 tokens in 0.62 seconds (146.20 tokens/sec)\n2025-02-10 20:48:49,185 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:49,185 - INFO - Round 17 of 20.\n2025-02-10 20:48:49,805 - INFO - Generated 91 tokens in 0.62 seconds (146.85 tokens/sec)\n2025-02-10 20:48:49,805 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:49,805 - INFO - Round 18 of 20.\n2025-02-10 20:48:50,426 - INFO - Generated 91 tokens in 0.62 seconds (146.54 tokens/sec)\n2025-02-10 20:48:50,427 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:50,427 - INFO - Round 19 of 20.\n2025-02-10 20:48:51,048 - INFO - Generated 91 tokens in 0.62 seconds (146.67 tokens/sec)\n2025-02-10 20:48:51,048 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:51,048 - INFO - Round 20 of 20.\n2025-02-10 20:48:51,667 - INFO - Generated 91 tokens in 0.62 seconds (146.98 tokens/sec)\n2025-02-10 20:48:51,668 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:51,668 - INFO - Run details saved to gpt2-rates-2025-02-10-20-48-51-668280-torch-compile-reduce-overhead-fullgraph-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m19.806s\nuser\t0m23.067s\nsys\t0m1.336s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n2025-02-10 20:48:57,852 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.472 seconds.\n2025-02-10 20:48:57,859 - INFO - Initialized GPT model in 0.007 seconds.\n2025-02-10 20:48:58,087 - INFO - Loaded model weights in 0.228 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:48:58,455 - INFO - Created GPT model in 0.603 seconds.\n2025-02-10 20:48:58,540 - INFO - Moved model to cuda:3 in 0.085 seconds.\n2025-02-10 20:48:58,540 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:48:58,541 - INFO - Loaded gpt2 on cuda:3 in 1.161 seconds.\n2025-02-10 20:48:58,543 - INFO - torch.compiled model in 0.003 seconds.\n2025-02-10 20:48:58,543 - INFO - Round 1 of 20.\n2025-02-10 20:48:59,366 - INFO - Generated 91 tokens in 0.82 seconds (110.79 tokens/sec)\n2025-02-10 20:48:59,366 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:48:59,366 - INFO - Round 2 of 20.\n2025-02-10 20:49:00,019 - INFO - Generated 91 tokens in 0.65 seconds (139.51 tokens/sec)\n2025-02-10 20:49:00,019 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:00,019 - INFO - Round 3 of 20.\n2025-02-10 20:49:00,706 - INFO - Generated 91 tokens in 0.69 seconds (132.47 tokens/sec)\n2025-02-10 20:49:00,707 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:00,707 - INFO - Round 4 of 20.\n2025-02-10 20:49:01,327 - INFO - Generated 91 tokens in 0.62 seconds (146.66 tokens/sec)\n2025-02-10 20:49:01,328 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:01,328 - INFO - Round 5 of 20.\n2025-02-10 20:49:01,950 - INFO - Generated 91 tokens in 0.62 seconds (146.40 tokens/sec)\n2025-02-10 20:49:01,950 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:01,950 - INFO - Round 6 of 20.\n2025-02-10 20:49:02,571 - INFO - Generated 91 tokens in 0.62 seconds (146.62 tokens/sec)\n2025-02-10 20:49:02,571 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:02,571 - INFO - Round 7 of 20.\n2025-02-10 20:49:03,193 - INFO - Generated 91 tokens in 0.62 seconds (146.54 tokens/sec)\n2025-02-10 20:49:03,193 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:03,193 - INFO - Round 8 of 20.\n2025-02-10 20:49:03,816 - INFO - Generated 91 tokens in 0.62 seconds (146.13 tokens/sec)\n2025-02-10 20:49:03,816 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:03,816 - INFO - Round 9 of 20.\n2025-02-10 20:49:04,444 - INFO - Generated 91 tokens in 0.63 seconds (145.02 tokens/sec)\n2025-02-10 20:49:04,444 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:04,444 - INFO - Round 10 of 20.\n2025-02-10 20:49:05,066 - INFO - Generated 91 tokens in 0.62 seconds (146.47 tokens/sec)\n2025-02-10 20:49:05,066 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:05,066 - INFO - Round 11 of 20.\n2025-02-10 20:49:05,688 - INFO - Generated 91 tokens in 0.62 seconds (146.45 tokens/sec)\n2025-02-10 20:49:05,688 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:05,688 - INFO - Round 12 of 20.\n2025-02-10 20:49:06,310 - INFO - Generated 91 tokens in 0.62 seconds (146.37 tokens/sec)\n2025-02-10 20:49:06,311 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:06,311 - INFO - Round 13 of 20.\n2025-02-10 20:49:06,933 - INFO - Generated 91 tokens in 0.62 seconds (146.27 tokens/sec)\n2025-02-10 20:49:06,933 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:06,933 - INFO - Round 14 of 20.\n2025-02-10 20:49:07,554 - INFO - Generated 91 tokens in 0.62 seconds (146.56 tokens/sec)\n2025-02-10 20:49:07,555 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:07,555 - INFO - Round 15 of 20.\n2025-02-10 20:49:08,176 - INFO - Generated 91 tokens in 0.62 seconds (146.45 tokens/sec)\n2025-02-10 20:49:08,177 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:08,177 - INFO - Round 16 of 20.\n2025-02-10 20:49:08,799 - INFO - Generated 91 tokens in 0.62 seconds (146.39 tokens/sec)\n2025-02-10 20:49:08,799 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:08,799 - INFO - Round 17 of 20.\n2025-02-10 20:49:09,420 - INFO - Generated 91 tokens in 0.62 seconds (146.49 tokens/sec)\n2025-02-10 20:49:09,421 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:09,421 - INFO - Round 18 of 20.\n2025-02-10 20:49:10,043 - INFO - Generated 91 tokens in 0.62 seconds (146.24 tokens/sec)\n2025-02-10 20:49:10,044 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:10,044 - INFO - Round 19 of 20.\n2025-02-10 20:49:10,665 - INFO - Generated 91 tokens in 0.62 seconds (146.62 tokens/sec)\n2025-02-10 20:49:10,665 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:10,665 - INFO - Round 20 of 20.\n2025-02-10 20:49:11,286 - INFO - Generated 91 tokens in 0.62 seconds (146.54 tokens/sec)\n2025-02-10 20:49:11,286 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:11,287 - INFO - Run details saved to gpt2-rates-2025-02-10-20-49-11-287012-torch-compile-reduce-overhead-fullgraph-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m19.626s\nuser\t0m22.831s\nsys\t0m1.406s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n2025-02-10 20:49:17,458 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.491 seconds.\n2025-02-10 20:49:17,465 - INFO - Initialized GPT model in 0.006 seconds.\n2025-02-10 20:49:17,704 - INFO - Loaded model weights in 0.239 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:49:18,069 - INFO - Created GPT model in 0.611 seconds.\n2025-02-10 20:49:18,153 - INFO - Moved model to cuda:3 in 0.084 seconds.\n2025-02-10 20:49:18,153 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:49:18,153 - INFO - Loaded gpt2 on cuda:3 in 1.187 seconds.\n2025-02-10 20:49:18,157 - INFO - torch.compiled model in 0.003 seconds.\n2025-02-10 20:49:18,157 - INFO - Round 1 of 20.\n2025-02-10 20:49:18,973 - INFO - Generated 91 tokens in 0.82 seconds (111.60 tokens/sec)\n2025-02-10 20:49:18,973 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:18,973 - INFO - Round 2 of 20.\n2025-02-10 20:49:19,621 - INFO - Generated 91 tokens in 0.65 seconds (140.58 tokens/sec)\n2025-02-10 20:49:19,621 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:19,621 - INFO - Round 3 of 20.\n2025-02-10 20:49:20,266 - INFO - Generated 91 tokens in 0.64 seconds (141.26 tokens/sec)\n2025-02-10 20:49:20,266 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:20,266 - INFO - Round 4 of 20.\n2025-02-10 20:49:20,911 - INFO - Generated 91 tokens in 0.64 seconds (141.24 tokens/sec)\n2025-02-10 20:49:20,911 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:20,911 - INFO - Round 5 of 20.\n2025-02-10 20:49:21,555 - INFO - Generated 91 tokens in 0.64 seconds (141.46 tokens/sec)\n2025-02-10 20:49:21,555 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:21,555 - INFO - Round 6 of 20.\n2025-02-10 20:49:22,199 - INFO - Generated 91 tokens in 0.64 seconds (141.27 tokens/sec)\n2025-02-10 20:49:22,200 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:22,200 - INFO - Round 7 of 20.\n2025-02-10 20:49:22,846 - INFO - Generated 91 tokens in 0.65 seconds (140.98 tokens/sec)\n2025-02-10 20:49:22,846 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:22,846 - INFO - Round 8 of 20.\n2025-02-10 20:49:23,486 - INFO - Generated 91 tokens in 0.64 seconds (142.29 tokens/sec)\n2025-02-10 20:49:23,487 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:23,487 - INFO - Round 9 of 20.\n2025-02-10 20:49:24,129 - INFO - Generated 91 tokens in 0.64 seconds (141.71 tokens/sec)\n2025-02-10 20:49:24,130 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:24,130 - INFO - Round 10 of 20.\n2025-02-10 20:49:24,771 - INFO - Generated 91 tokens in 0.64 seconds (141.83 tokens/sec)\n2025-02-10 20:49:24,772 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:24,772 - INFO - Round 11 of 20.\n2025-02-10 20:49:25,413 - INFO - Generated 91 tokens in 0.64 seconds (142.00 tokens/sec)\n2025-02-10 20:49:25,413 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:25,414 - INFO - Round 12 of 20.\n2025-02-10 20:49:26,056 - INFO - Generated 91 tokens in 0.64 seconds (141.66 tokens/sec)\n2025-02-10 20:49:26,056 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:26,057 - INFO - Round 13 of 20.\n2025-02-10 20:49:26,703 - INFO - Generated 91 tokens in 0.65 seconds (140.87 tokens/sec)\n2025-02-10 20:49:26,703 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:26,703 - INFO - Round 14 of 20.\n2025-02-10 20:49:27,344 - INFO - Generated 91 tokens in 0.64 seconds (141.96 tokens/sec)\n2025-02-10 20:49:27,345 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:27,345 - INFO - Round 15 of 20.\n2025-02-10 20:49:27,986 - INFO - Generated 91 tokens in 0.64 seconds (142.08 tokens/sec)\n2025-02-10 20:49:27,986 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:27,986 - INFO - Round 16 of 20.\n2025-02-10 20:49:28,628 - INFO - Generated 91 tokens in 0.64 seconds (141.93 tokens/sec)\n2025-02-10 20:49:28,628 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:28,628 - INFO - Round 17 of 20.\n2025-02-10 20:49:29,269 - INFO - Generated 91 tokens in 0.64 seconds (142.10 tokens/sec)\n2025-02-10 20:49:29,269 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:29,269 - INFO - Round 18 of 20.\n2025-02-10 20:49:29,913 - INFO - Generated 91 tokens in 0.64 seconds (141.43 tokens/sec)\n2025-02-10 20:49:29,913 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:29,913 - INFO - Round 19 of 20.\n2025-02-10 20:49:30,557 - INFO - Generated 91 tokens in 0.64 seconds (141.46 tokens/sec)\n2025-02-10 20:49:30,557 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:30,557 - INFO - Round 20 of 20.\n2025-02-10 20:49:31,199 - INFO - Generated 91 tokens in 0.64 seconds (141.84 tokens/sec)\n2025-02-10 20:49:31,199 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:31,200 - INFO - Run details saved to gpt2-rates-2025-02-10-20-49-31-199651-torch-compile-max-autotune-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m19.900s\nuser\t0m23.112s\nsys\t0m1.409s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n2025-02-10 20:49:37,383 - INFO - Loaded /mnt/raid1/trent/src/parallelopedia/data/model_19072.pt checkpoint in 0.482 seconds.\n2025-02-10 20:49:37,390 - INFO - Initialized GPT model in 0.006 seconds.\n2025-02-10 20:49:37,613 - INFO - Loaded model weights in 0.223 seconds.\n<frozen importlib._bootstrap>:488: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n2025-02-10 20:49:37,976 - INFO - Created GPT model in 0.592 seconds.\n2025-02-10 20:49:38,058 - INFO - Moved model to cuda:3 in 0.082 seconds.\n2025-02-10 20:49:38,058 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n2025-02-10 20:49:38,058 - INFO - Loaded gpt2 on cuda:3 in 1.157 seconds.\n2025-02-10 20:49:38,061 - INFO - torch.compiled model in 0.002 seconds.\n2025-02-10 20:49:38,061 - INFO - Round 1 of 20.\n2025-02-10 20:49:38,896 - INFO - Generated 91 tokens in 0.83 seconds (109.09 tokens/sec)\n2025-02-10 20:49:38,896 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:38,896 - INFO - Round 2 of 20.\n2025-02-10 20:49:39,552 - INFO - Generated 91 tokens in 0.66 seconds (138.68 tokens/sec)\n2025-02-10 20:49:39,553 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:39,553 - INFO - Round 3 of 20.\n2025-02-10 20:49:40,199 - INFO - Generated 91 tokens in 0.65 seconds (140.90 tokens/sec)\n2025-02-10 20:49:40,199 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:40,199 - INFO - Round 4 of 20.\n2025-02-10 20:49:40,851 - INFO - Generated 91 tokens in 0.65 seconds (139.61 tokens/sec)\n2025-02-10 20:49:40,852 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:40,852 - INFO - Round 5 of 20.\n2025-02-10 20:49:41,496 - INFO - Generated 91 tokens in 0.64 seconds (141.24 tokens/sec)\n2025-02-10 20:49:41,497 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:41,497 - INFO - Round 6 of 20.\n2025-02-10 20:49:42,143 - INFO - Generated 91 tokens in 0.65 seconds (140.79 tokens/sec)\n2025-02-10 20:49:42,144 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:42,144 - INFO - Round 7 of 20.\n2025-02-10 20:49:42,794 - INFO - Generated 91 tokens in 0.65 seconds (139.98 tokens/sec)\n2025-02-10 20:49:42,794 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:42,795 - INFO - Round 8 of 20.\n2025-02-10 20:49:43,442 - INFO - Generated 91 tokens in 0.65 seconds (140.59 tokens/sec)\n2025-02-10 20:49:43,442 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:43,442 - INFO - Round 9 of 20.\n2025-02-10 20:49:44,088 - INFO - Generated 91 tokens in 0.65 seconds (141.07 tokens/sec)\n2025-02-10 20:49:44,088 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:44,088 - INFO - Round 10 of 20.\n2025-02-10 20:49:44,738 - INFO - Generated 91 tokens in 0.65 seconds (140.10 tokens/sec)\n2025-02-10 20:49:44,738 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:44,738 - INFO - Round 11 of 20.\n2025-02-10 20:49:45,382 - INFO - Generated 91 tokens in 0.64 seconds (141.34 tokens/sec)\n2025-02-10 20:49:45,383 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:45,383 - INFO - Round 12 of 20.\n2025-02-10 20:49:46,026 - INFO - Generated 91 tokens in 0.64 seconds (141.47 tokens/sec)\n2025-02-10 20:49:46,027 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:46,027 - INFO - Round 13 of 20.\n2025-02-10 20:49:46,695 - INFO - Generated 91 tokens in 0.67 seconds (136.23 tokens/sec)\n2025-02-10 20:49:46,695 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:46,695 - INFO - Round 14 of 20.\n2025-02-10 20:49:47,339 - INFO - Generated 91 tokens in 0.64 seconds (141.40 tokens/sec)\n2025-02-10 20:49:47,340 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:47,340 - INFO - Round 15 of 20.\n2025-02-10 20:49:47,988 - INFO - Generated 91 tokens in 0.65 seconds (140.35 tokens/sec)\n2025-02-10 20:49:47,989 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:47,989 - INFO - Round 16 of 20.\n2025-02-10 20:49:48,649 - INFO - Generated 91 tokens in 0.66 seconds (137.84 tokens/sec)\n2025-02-10 20:49:48,649 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:48,649 - INFO - Round 17 of 20.\n2025-02-10 20:49:49,295 - INFO - Generated 91 tokens in 0.64 seconds (141.12 tokens/sec)\n2025-02-10 20:49:49,295 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:49,295 - INFO - Round 18 of 20.\n2025-02-10 20:49:49,939 - INFO - Generated 91 tokens in 0.64 seconds (141.44 tokens/sec)\n2025-02-10 20:49:49,939 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:49,939 - INFO - Round 19 of 20.\n2025-02-10 20:49:50,582 - INFO - Generated 91 tokens in 0.64 seconds (141.49 tokens/sec)\n2025-02-10 20:49:50,583 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:50,583 - INFO - Round 20 of 20.\n2025-02-10 20:49:51,232 - INFO - Generated 91 tokens in 0.65 seconds (140.31 tokens/sec)\n2025-02-10 20:49:51,232 - INFO - Output:\nEinstein's Theory of Relativity states that the speed of\nlight in a vacuum is simply the speed of the electrons in\nthat vacuum, since light has a speed. Since the speed of\nlight in a vacuum is equal to the speed of the electrons in\na solid, the light from this source has a speed of\napproximately 1/299,792 m/s. Einstein's theory of relativity\nexplains this speed by a phenomenon known as the speed at\nthe end of time. In other words, this speed is\n2025-02-10 20:49:51,232 - INFO - Run details saved to gpt2-rates-2025-02-10-20-49-51-232526-torch-compile-max-autotune-fullgraph-py313t.json.\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported\ntorch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported\n\nreal\t0m20.061s\nuser\t0m23.272s\nsys\t0m1.387s\nException ignored in: <_io.BufferedWriter name=41>\nBrokenPipeError: [Errno 32] Broken pipe\n\n```\n:::\n\nAfter each run, a JSON file is saved capturing details about the run.  These\ncan all be found in this [`json`](\nhttps://github.com/tpn/website/tree/main/articles/pytorch-and-python-free-threading/json)\ndirectory.\n\nUsing [`gpt2-rates-2025-02-09-18-19-29-116321-py313t.json`](\nhttps://github.com/tpn/website/blob/main/articles/pytorch-and-python-free-threading/json/gpt2-rates-2025-02-09-18-19-29-116321-py313t.json)\nas an example:\n\n```json\n{\n    \"rates\": [\n        103.45683188097036,\n        133.69021647689885,\n        134.64711111769682,\n        134.99572668445947,\n        135.07252279473053,\n        134.93995013027606,\n        134.8097791019913,\n        134.97874539755563,\n        134.96994164914173,\n        134.2781961646748,\n        133.36089184927351,\n        134.91936892711604,\n        134.99927221213662,\n        134.99574871578727,\n        134.84820822748674,\n        134.90753647890656,\n        134.84542991060954,\n        134.76236728440452,\n        134.939303436755,\n        134.95605543169106\n    ],\n    \"model_config\": {\n        \"block_size\": 1024,\n        \"vocab_size\": 50304,\n        \"n_layer\": 12,\n        \"n_head\": 12,\n        \"n_embd\": 768\n    },\n    \"args\": {\n        \"log_level\": \"INFO\",\n        \"model\": \"gpt2\",\n        \"device\": \"cuda:3\",\n        \"max_length\": 100,\n        \"top_k\": 50,\n        \"seed\": 42,\n        \"prompt\": \"Einstein's Theory of Relativity states that\",\n        \"torch_compile\": false,\n        \"torch_jit\": false,\n        \"torch_compile_fullgraph\": false,\n        \"torch_compile_reduce_overhead\": false,\n        \"torch_compile_max_autotune\": false,\n        \"generate_slim\": false,\n        \"rounds\": 20,\n        \"wrap\": 60,\n        \"note\": \"\"\n    },\n    \"start_timestamp\": \"2025-02-09T18:19:14.156224\",\n    \"end_timestamp\": \"2025-02-09T18:19:29.116383\",\n    \"elapsed\": \"14.960\",\n    \"device_name\": \"Tesla V100-DGXS-32GB\",\n    \"conda_env_name\": \"py313t\",\n    \"is_gil_enabled\": false,\n    \"note\": \"\"\n}\n```\n\nAll tests were done for 20 rounds, and the `rates` key contains an array of\nfloats, representing the tokens/sec generation rate achieved by the call to\n`generate()` (or `generate_slim()`, which we'll discuss shortly) after\noptionally compiling the model with the requested parameters.\n\n#### Free-Threaded Python (py313t)\n\nI ended up doing multiple runs because... well, as you're about to see from\nthe data visualization below... it was a pretty noisy test.  Loads of\nvariance and generally everything was all over the shop.  The total run\ntimes for each different permutation were all about the same, around 14\nseconds or so, but definitely no clear winner regarding whether or not\n`torch.compile()` or any particular permutation was having a repeatable\nspeedup.  Interesting.\n\nLine plots and box-plots follow.  The box plots omitted the first *\"warmup\"*\nrun (which was always slower and skewed the data unnecessarily).  If you\nclick on the image you should get a nice *gallery* presentation mode that\nallows you to flip between images nicely.\n\n::: {.theme-light}\n![Generation Performance - Run 1 (py313t)](py313t-run1-combined-side-by-side-light.svg){.lightbox group=\"gal1\"}\n![Generation Performance - Run 2 (py313t)](py313t-run2-combined-side-by-side-light.svg){.lightbox group=\"gal1\"}\n![Generation Performance - Run 3 (py313t)](py313t-run3-combined-side-by-side-light.svg){.lightbox group=\"gal1\"}\n![Generation Performance - Run 4 (py313t)](py313t-run4-combined-side-by-side-light.svg){.lightbox group=\"gal1\"}\n:::\n\n::: {.theme-dark}\n![Generation Performance - Run 1 (py313t)](py313t-run1-combined-side-by-side-dark.svg){.lightbox group=\"gal2\"}\n![Generation Performance - Run 2 (py313t)](py313t-run2-combined-side-by-side-dark.svg){.lightbox group=\"gal2\"}\n![Generation Performance - Run 3 (py313t)](py313t-run3-combined-side-by-side-dark.svg){.lightbox group=\"gal2\"}\n![Generation Performance - Run 4 (py313t)](py313t-run4-combined-side-by-side-dark.svg){.lightbox group=\"gal2\"}\n:::\n\n#### Normal Python (py313)\n\nNormal Python (not just free-threaded Python with the GIL enabled, but\nfull-blown old-school normal Python with no knowledge of GIL removal---i.e.\nour `py313` environment) was pretty similar:\n\n::: {.theme-light}\n![Generation Performance (py313)](py313-run1-combined-side-by-side-light.svg){.lightbox group=\"gal3\"}\n:::\n\n::: {.theme-dark}\n![Generation Performance (py313)](py313-run1-combined-side-by-side-dark.svg){.lightbox group=\"gal4\"}\n:::\n\n#### Explicit @torch.compile Decorator\n\nHere's where it gets interesting.  In a final, last-ditch effort to see if I\ncould see *any* sort of speedup from `torch.compile()`, I introduced a\nslimmer `generate()` routine, aptly named `generate_slim()`, which was\nstripped of any superfluous code that would have otherwise impeded Torch\nDynamo's ability to optimize the graph.  That function looked like this:\n\n```python\nclass GPT:\n    ...\n\n    # @torch.compile\n    def generate_slim(\n        self, text_tokens: torch.Tensor, max_length: int = 1024,\n        top_k: int = 50, seed: int = None,\n    ) -> str:\n        \"\"\"\n        Generate text from the model.  This version differs from\n        `generate()` in that it does not use any Python code that\n        causes a torch graph break.\n\n        Args:\n\n            text (str): Supplies the prompt.\n\n            max_length (int): Supplies the maximum total length,\n                including prompt.\n\n            top_k (int): Supplies the number of tokens to consider\n                at each generation step.\n\n            seed (int): Ignored!\n\n        Returns:\n\n            str: The generated text (including the initial prompt).\n        \"\"\"\n        # Initialize alias.\n        device = self.device\n        stop_token = self.stop_token\n\n        # Create the tensor for capturing predicted tokens.\n        x = torch.tensor(\n            text_tokens,\n            dtype=torch.long,\n            device=device\n        ).unsqueeze(0)\n\n        # Create a random generator for reproducibility.\n        # sample_rng = torch.Generator(device=device)\n        # if seed is None:\n        #     seed = self.manual_seed\n        # sample_rng.manual_seed(seed)\n\n        # Generate tokens up to our max length.\n        for _ in range(max_length):\n            with torch.no_grad():\n                # Forward pass, ignoring the returned loss.\n                (logits, _) = self(x)\n\n            # Take the logits at the last time-step (shape:\n            # (1, vocab_size)).\n            logits = logits[:, -1, :]\n\n            # Convert to probabilities.\n            probs = F.softmax(logits, dim=-1)\n\n            # Top-k sampling.\n            topk_probs, topk_indices = torch.topk(\n                probs, k=top_k, dim=-1,\n            )\n\n            # Sample the next token.\n            next_idx = torch.multinomial(\n                topk_probs,\n                num_samples=1,\n                # generator=sample_rng,\n            )\n            next_token = torch.gather(topk_indices, -1, next_idx)\n\n            # If the next token is the stop token, we're done.\n            # next_token_item = next_token.item()\n            # if next_token_item == stop_token:\n            #    break\n\n            # Append token to current sequence.\n            x = torch.cat((x, next_token), dim=1)\n\n        return x\n```\n\nNote that we had to make a number of sizable modifications.  No more\nrandom number generator---that was causing graph breaks.  No more\nexplicitly checking for the stop token, again, that makes the dynamic\noptimizer's job much harder at runtime without extra tracing overhead\nfor tracking scalars.  So we now generate tokens up to the maximum\nspecified, ignorant of any stop tokens, and return that.\n\nI wanted to do two runs here, one where we call everything as normal\nwith all the different `torch.compile()` invocations we'd used in\nprior runs, and then a second one where I explicitly mark the\n`generate_slim()` routine with a `@torch.compile` decorator.\n\nThe latter absolutely does not work in free-threaded Python, it\nsegfaults after about ten seconds or so, thus, I couldn't test it.\n\nHowever, on the normal Python version... we finally saw some\ninteresting results.\n\nFirst, let's look at our baseline: normal `generate_slim()` with no\n`@torch.compile` generator (the bash scripts verify that the decorator\nis uncommented and commented as necessary):\n\n::: {.theme-light}\n![Generation Performance - generate_slim() - No @torch.compile Decorator (py313)](py313-run2-generate_slim-combined-side-by-side-light.svg){.lightbox}\n:::\n\n::: {.theme-dark}\n![Generation Performance - generate_slim() - No @torch.compile Decorator (py313)](py313-run2-generate_slim-combined-side-by-side-dark.svg){.lightbox}\n:::\n\nWell we finally see one configuration break out from the pack:\napparently `torch.compile(model, {'fullgraph': True})` yielded the\nbest generation rate we've seen yet, hovering around 160 tokens/sec.\nNote that all the total run times are still pretty similar, hovering\naround that 14-15s mark.\n\nNow, let's uncomment the `@torch.compile` decorator above `def\ngenerate_slim()` and do another full run:\n\n::: {.theme-light}\n![Generation Performance - generate_slim() - With @torch.compile Decorator (py313)](py313-run3-generate_slim-torch-compiled-combined-side-by-side-light.svg){.lightbox}\n:::\n\n::: {.theme-dark}\n![Generation Performance - generate_slim() - With @torch.compile Decorator (py313)](py313-run3-generate_slim-torch-compiled-combined-side-by-side-dark.svg){.lightbox}\n:::\n\nOh man.  That first compilation took *forever*.  But once compiled,\nour tokens/sec generation rate shoots up significantly to the 250+\nrange instead of the 150+ range!  But at a crazy up-front cost---as\nyou can see with all of the run times (the values in parenthesis in\nthe labels and x-axis in first and second plots, respectively) were in\nexcess of four minutes.\n\nAs we're only doing twenty runs of generating ~90-100 characters, that\nstartup cost is brutal, however, if we were doing model training or\nlaunching a long-running inference service like our HTTP server, the\nstartup cost would be quickly amortized away as we benefit from about\na 75% speedup.\n\nSo, interesting stuff, kind of.  To be fair... PyTorch clearly\nindicates free-threaded Python isn't supported, per our hacks earlier,\nso, this should be an interesting area of development in future\nreleases, especially now that we can see how much benefit there is to\ndoing multi-threaded run-time inference in a single Python process.\n\n# Conclusion\n\nIn this article, we've explored the world of free-threaded Python and\nPyTorch, and demonstrated that you can now do parallel inference on\nPyTorch models, and that it all plays very nicely together when wrapped\nup with an `asyncio`-based HTTP server.\n\nHopefully this encourages more folks to experiment with free-threaded\nPython, or perhaps port their existing Python packages to play nicely\nwhen installed in a free-threaded Python environment.  I personally\ncan't wait until free-threaded Python is the default!  Although that's\nprobably at least five or so years out at this point.\n\n<!-- vim:set ts=8 sw=2 sts=2 expandtab textwidth=78 -->\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}