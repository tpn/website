---
title: "PyTorch and Python Free-Threading"
subtitle: |
    Unlocking multi-threaded parallel inference on PyTorch models
categories:
  - PyTorch
  - Python
  - Free-Threading
  - No-GIL
  - LLM
  - GPT2
author: "Trent Nelson"
date: 02/13/2025
#draft: true
#draft-mode: visible
image: "images/pytorch-and-python-free-threading.png"
description: |
  This post examines multi-threaded parallel inference on PyTorch models
  using the new *No-GIL*, free-threaded version of Python.  Using a simple
  124M parameter GPT2 model that we train from scratch, we explore the novel
  new territory unlocked by free-threaded Python: parallel PyTorch model
  inference, where multiple threads, unimpeded by the Python GIL, attempt to
  generate text from a transformer-based model in parallel.
jupyter:
  kernelspec:
    display_name: "Python 3.13t (Free-Threaded)"
    language: python
    name: py313t
execute:
  echo: true
  eval: true
  output: true
  error: true
  cache: true
  freeze: auto
lightbox:
  match: auto
comments: false
format:
  html:
    grid:
      gutter-width: 1rem
notebook-view:
  - notebook: torch-compile-analysis.ipynb
    title: "Data Visualization: torch.compile"
  - notebook: wrk-analysis.ipynb
    title: "Data Visualization: wrk"
  - notebook: gpt2-v1.ipynb
    title: "GPT2 v1"
---

This post is sponsored in part by [Meta](https://meta.com) in
collaboration with [Quansight](https://quansight.com) and
[OpenTeams](https://openteams.com).

::: {.callout-tip}

Toggle the little radio button in the navigation bar at the top to switch
between native *Light Mode* and *Dark Mode* themes.

:::

# Introduction

Python 3.13, released in October 2024, is the first version of Python to
introduce support for a "no-GIL" *free-threaded* mode, per
[PEP-703 Making the Global Interpreter Lock Optional in CPython](
https://peps.python.org/pep-0703/), unlocking the ability for multiple Python
threads to run simultaneously.

This allows, for the first time since the language's inception in December
1989, a single Python process to saturate all CPU cores in parallel with
pure Python code (i.e. not farming out to extension modules written in C, C++,
or, more recently, Rust).

A handful of the [motivations](https://peps.python.org/pep-0703/#motivation)
captured in that PEP opine on how the GIL impedes Python AI workflows,
particularly as it relates to GPU programming.

This blog post explores what can be done with [PyTorch](https://pytorch.org)
now with the new free-threaded version of Python, specifically focusing on
run-time inference on transformer-based generative models.

::: {.callout-note}

I didn't focus on how *training* PyTorch models might look in the new
free-threaded Python world for a couple of reasons.

Primarily, training is
a lot more complex if you're involving multiple nodes---as gradients need
to be carefully synchronized at critical points, for example---and is well
outside the scope of a simple blog post.

Additionally, there's already a huge body of existing work tackling
multi-node training in PyTorch by way of the [Distributed Data Parallel](
https://pytorch.org/tutorials/intermediate/ddp_tutorial.html),
`multiprocessing`-based facilities exposed by [`torch.distributed`](
https://pytorch.org/tutorials/intermediate/dist_tuto.html).

Whereas, on the flip side, no one has really explored what parallel
inference might look like in a single-threaded Python because the GIL has
prevented that from even being an option until now.

:::

# Getting Started

All of this work was done on Linux (Ubuntu 22.04) with Python 3.13t, PyTorch
2.6, and CUDA 12.6.

Full source code is provided to everything captured in this post.  It is
worth noting that in a few cases, I am rolling my own solutions for things
that have existing solutions in the broader Python ecosystem.  For example,
in the tail end of the post, I leverage a multi-threaded `asyncio`-based
HTTP server I wrote instead of using existing solutions like
[FastAPI](https://fastapi.tiangolo.com/).

The reason for this is that, as free-threaded Python is still in its
infancy, a lot of packages do not work with it yet, especially those that
rely on Cython, C or C++ code, or Rust.

In fact, Rust dependencies are particularly problematic due to the
proliferation of Python projects leveraging [PyO3](
https://github.com/PyO3/pyo3) (Rust bindings for Python), especially
prominent projects such as [TikToken](https://github.com/openai/tiktoken)
and [Pydantic](https://docs.pydantic.dev/latest/), upon which a lot of the
Python AI ecosystem is built.  PyO3 only recently grew support for
free-threaded Python in their 0.23.3 release, which came out in December,
2024, and many dependent projects are yet to update to it.

Thus, this post and its supporting code should not be considered the state
of the art for production deployments---its primary goal is exploratory in
nature, and minimizing the number of moving pieces in the stack helps
achieve this goal.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Environment Setup Details"}

## Environments

It is fiddly getting the environments set up in support of this post.
Again, this is due to the infancy of free-threaded Python.  So I apologize
in advance for how long this environment setup section is.

I reference two `conda` environments in this post: a Python 3.13
free-threaded one named `py313t`, and a normal, not-free-threaded Python
3.13 one named `py313`.

The primary motivation behind the second `py313` environment is it allows
us to install Jupyter Lab, which, at the time of writing, still isn't
compatible with a Python free-threaded installation.  However, we can still
register a free-threaded Python kernel with Jupyter, which is all we really
care about when running the code in this post in a free-threaded environment.

Details on creating the `conda` environments follow.

### Free-Threaded 3.13 Env (py313t)

I use `conda` to create the Python 3.13 free-threaded environment plus
initial dependencies, activate it, then install the remaining dependencies
via pip, as follows:

```bash
conda create -n py313t python=3.13 python-freethreading \
    nodejs pip tqdm flake8 rust requests \
        -c conda-forge
conda activate py313t
pip install numpy setuptools_rust regex safetensors
```

`nodejs` is required for the UI component we'll introduce later.  `regex`,
`rust`, and `setuptools_rust` are needed for `tiktoken`, described next.
Finally, `numpy` is for `torch`, which we install later, too.

#### TikToken

[TikToken](https://github.com/openai/tiktoken) is a fast BPE tokenizer from
OpenAI that is used extensively in the emerging Python LLM landscape.  At the
time of writing, the latest TikToken release was [0.8.0](
https://github.com/openai/tiktoken/releases/tag/0.8.0), which was built
against PyO3 0.22.2, which isn't compatible with free-threaded Python.

Thankfully, it was trivial to get a local installation of `tiktoken` working
by cloning the Github repo, bumping the PyO3 version in `Cargo.toml`, then
rebuilding and installing.

::: {.callout-note}

This is a perfect example of the type of fiddling around I wanted to avoid
by not depending on any external packages other than the bare necessities,
such as PyTorch.  I made an exception for `tiktoken` because a) it's arguably
an equally-important part of the LLM stack as `torch`, and b) it thankfully
wasn't *too* difficult getting a compatible version of `tiktoken` installed
locally.

:::

Clone the tiktoken git repo and cd into it as follows:

```bash
git clone https://github.com/openai/tiktoken
cd tiktoken
```

Edit the `Cargo.toml` file and change the `pyo3` dependency version to at
least 0.23.3^[I used 0.23.3, as that was the latest version available at the
time, however, 0.23.4 has since been released, so you could try that too.]:

```diff
diff --git a/Cargo.toml b/Cargo.toml
index 2eed0c1..6be5f63 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -9,7 +9,7 @@ name = "_tiktoken"
 crate-type = ["cdylib"]
 
 [dependencies]
-pyo3 = { version = "0.22.2", default-features = false, features = ["extension-module", "macros"] }
+pyo3 = { version = "0.23.3", default-features = false, features = ["extension-module", "macros"] }
 
 # tiktoken dependencies
 fancy-regex = "0.13.0"
```

With this patch applied, and the `py313t` conda environment active (with
`rust` and `setuptools_rust` installed):

```bash
python setup.py build
python setup.py install
```

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Show Build & Install Output"}

{{< include tiktoken-setup.py-build-and-install-output.md >}}

:::

After this, you should be able to import the `tiktoken` module in Python:

```bash
% cd ..
% python -Xgil=0
Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import tiktoken
>>>
```

#### Torch

Install PyTorch 2.6 via `pip` with the conda `py313t` environment active:

```bash
% conda activate py313t
% pip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126
```

If you have trouble installing PyTorch, consult their [Getting Started](
https://pytorch.org/get-started/locally/) guide.

You can verify torch installed correctly as follows:

```bash
% python -Xgil=0
Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> torch.cuda.is_available()
True
```

#### IPython Kernel

Installing IPython Kernel allows us to use our free-threaded Python
installation via the Jupyter Lab instance we install in the `py313`
environment.

```bash
conda activate py313t
pip install ipykernel
```

Once `ipykernel` is installed, run the following:

```bash
% python3.13t -m ipykernel --install --name py313t --user
Installed kernelspec py313t in /home/trent/.local/share/jupyter/kernels/py313t
```

This will install a kernel configuration file, `kernel.json`, which we need
to tweak by adding the `-Xgil=0` startup flag to the Python interpreter:

```bash
% cd ~/.local/jupyter/share/kernels/py313t
% cp kernel.json kernel.json.orig
% vi kernel.json
# Edit kernel.json to make it look like the diff below.
% diff -u kernel.json.orig kernel.json
```

```diff
--- kernel.json.orig    2025-02-04 15:02:21.814112004 -0800
+++ kernel.json 2025-02-04 15:02:36.553806199 -0800
@@ -1,6 +1,7 @@
 {
  "argv": [
   "/home/trent/mambaforge/envs/py313t/bin/python3.13t",
+  "-Xgil=0",
   "-Xfrozen_modules=off",
   "-m",
   "ipykernel_launcher",
@@ -12,4 +13,4 @@
  "metadata": {
   "debugger": true
  }
```

::: {.hidden}

(Hidden for now as we removed the immediate need for `datrie` by fixing the
HTTP server routing logic.)

#### Datrie and Cython

[datrie](https://github.com/pytries/datrie) is a Python library that provides
a *trie* (or *digital search tree*) data structure by way of the [libdatrie](
https://linux.thai.net/~thep/datrie/datrie.html) C library.  The Python
`datrie` library isn't strictly necessary to run `parallelopedia.gpt2`, but
other components rely on it, so it's handy to get installed now, if possible.

It relies upon Cython, and thus, for now, you need to install a free-threaded
compatible version of Cython first, as follows:

```bash
conda activate py313t
pip install git+https://github.com/cython/cython
```
Then, clone the `datrie` repo and install as follows:

```bash
conda activate py313t
git clone https://github.com/pytries/datrie --recursive
cd datrie
python setup.py build
python setup.py install
```

If everything goes well, you should see something like this when you launch
Python and import `datrie`:

```bash
% python
Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import datrie
>>>
```

:::

### Normal 3.13 Env (py313)

The second `py313` environment is almost identical to `py313t`, except it is
not a `python-freethreading` installation, and, additionally, we install
Jupyter Lab.  We can install `tiktoken` directly via `pip`, so we don't need
the supporting Rust cruft.

```bash
conda create -n py313 python=3.13 \
    nodejs pip tqdm flake8 jupyterlab requests \
        -c conda-forge
conda activate py313
pip install numpy tiktoken safetensors
pip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126
```

## Parallelopedia

All of the code in this article is available in the [Parallelopedia](
https://github.com/tpn/parallelopedia) repository on Github.  The code we'll
be focusing on in this post lives in the [parallelopedia.gpt2](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)
module.

There is also a web user interface component named [Parallelopedia-UI](
https://github.com/tpn/parallelopedia-ui), which we will use later in the
post.

Clone the repositories as follows:

```bash
% git clone https://github.com/tpn/parallelopedia
% git clone https://github.com/tpn/parallelopedia-ui
```

::: {.callout-important .env-vars collapse="false" title="Important Environment Variables"}

The code and command examples in this post will assume you've added the
`src` directory to your `PYTHONPATH`, the `bin` directory to your `PATH`,
and set the `PARALLELOPEDIA_ROOT` environment variable to the root of the
repository.  You can do this as follows:

```bash
cd parallelopedia
export PYTHONPATH=$(pwd)/src:$PYTHONPATH
export PATH=$(pwd)/bin:$PATH
export PARALLELOPEDIA_ROOT=$(pwd)
cd ..
cd parallelopedia-ui
export PARALLELOPEDIA_UI=$(pwd)
```

It is recommended that you add these to your shell.  For me, using zsh, I
use the following:

```zsh
export PARALLELOPEDIA_ROOT=~s1/parallelopedia
export PARALLELOPEDIA_UI=~s1/parallelopedia-ui
export PYTHONPATH=$PARALLELOPEDIA_ROOT/src:$PYTHONPATH
export PATH=$PARALLELOPEDIA_ROOT/bin:$PATH
```

:::

You can perform a quick sanity check that things are working as follows:

```bash
% python -Xgil=0 -m parallelopedia.http.server --help
usage: server.py [-h] [--ip IP] [--port PORT] [--debug] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--threads THREADS] [--protocol-class PROTOCOL_CLASS] [--app-classes APP_CLASSES [APP_CLASSES ...]] [--listen-backlog LISTEN_BACKLOG]

Run the HTTP server.

options:
  -h, --help            show this help message and exit
  --ip IP               IP address to bind the server to.
  --port PORT           Port number to bind the server to.
  --debug               Enable debug mode for asyncio.
  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}
                        Set the logging level.
  --threads THREADS     Number of threads to use.
  --protocol-class PROTOCOL_CLASS
                        The protocol class to use for the server.
  --app-classes APP_CLASSES [APP_CLASSES ...]
                        Space-separated list of HTTP application classes.
  --listen-backlog LISTEN_BACKLOG
                        The listen backlog for the server.

```
:::

# PyTorch and LLM Crash Course

My involvement with PyTorch and Large Language Models (LLMs) started around
late November last year, 2024.  Going in, I knew nothing about PyTorch, nor
deep neural networks, nor LLMs---other than having enjoyed using LLMs
thoroughly the past couple of years.  I had never trained an AI model of
any kind.  I did have a bit of NumPy and data science exposure up my sleeve,
plus general familiarity with Python.

Thanks to [Andrej Karpathy](https://karpathy.ai/)'s phenomenal YouTube series
on deep neural networks and LLMs titled [Neural Networks: From Zero to Hero](
https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ),
over the course of about 3 weeks or so I went from zero to... well I
wouldn't necessarily say *hero*---perhaps zero to *not-completley-clueless*
is more apropos.

Andrej's content is a fantastic resource to learn everything you need to
know to understand how modern LLMs work from the ground-up.  It's not a
short series---there are 19 hours, 21 minutes and two seconds of content
across ten videos---and you'll probably spend double that if you *really*
want to properly absorb the content.

None of the work presented in this post would have been possible had I not
invested the time in Andrej's series.  If you're reading this Andrej, thanks,
and keep up the brilliant work!

## Training GPT-2 (124M) Locally

Equipped with my new knowledge about LLMs, PyTorch, and, thanks to Andrej's
final video in the series titled [Let's reproduce GPT-2 (124M)](
https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)
and the accompanying [build-nanogpt](
https://github.com/karpathy/build-nanogpt) Github repo, I was able to train
a local GPT-2 model via PyTorch, from scratch, using the [edu_fineweb10B](
https://huggingface.co/rhysjones/gpt2-124M-edu-fineweb-10B) dataset.

I only had to make one change in order to run locally:
[Use 8 for micro batch size instead of 64](
https://github.com/tpn/build-nanogpt/commit/0069c4a35d1a362c1fd50f9f5bce00a170e15904).
With that change in place, I was able to train GPT-2 from scratch as follows:

```bash
% conda activate py313
% git clone gh:tpn/build-nanogpt
% cd build-nanogpt
# Download the fineweb dataset.
% python fineweb.py
# Train!
% torchrun --standalone --nproc_per_node=4 train_gpt2.py
```

This was run on an NVIDIA DGX workstation from 2017, which has an Intel
Xeon(R) CPU E5-2698 v4 @ 2.20GHz (20 cores, 40 threads), and four Tesla
V100-DGXS-32GB GPUs.

Training in parallel across all four GPUs yielded around 36,000 tokens/sec,
with an average time of about 14.5 seconds per loop iteration.  Training
took about 3 days and 5 hours for 19,072 steps.  All four GPUs were pegged
close to their 300W power limit for those three days.

::: {.callout-note}

Amusingly, well after the fact, I decided to see what kind of training
performance I'd get on my Windows 11 gaming box (via WSL2 and Ubuntu 22.04),
which has an AMD Ryzen 9 7950X3D (16 cores, 32 threads) and NVIDIA RTX 4090.
Training via `python train_gpt2.py` (`torchrun` wasn't needed as I wasn't
using multiple GPUs) yielded about 45,000 tokens/sec, which is a nice bump,
but what was most impressive was the reduction to the loop iteration
duration, which averaged out to about 180ms!

So, I could have completed the same training process in about an hour or so,
at a vastly reduced impact on my electricity bill that month :-)

:::

Once training completes, a `log/model_19072.pt` file is produced, which is
the checkpoint of the model at that final step, obtained via a call to
`torch.save()`.  The model has 124M parameters---which is tiny by modern
standards---and is just under 500MB on disk.

You can download that very model I trained via the HuggingFace dataset I set
up here: [model_19072.pt](
https://huggingface.co/datasets/trentnelson/parallelopedia-data-gpt2/blob/main/model_19072.pt).
Once downloaded, place the file in `$PARALLELOPEDIA_ROOT/data`;
alternatively, if you run the Jupyter Notebook below, it'll automatically
download the model from HuggingFace on first run.

# PyTorch GPT-2 Implementation

Let's introduce the first version of the Python code we're going to use.
Again, all of this has been made possible thanks to Andrej Karpathy's work
with his [YouTube series](
https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)
and [build-nanogpt](
https://github.com/karpathy/build-nanogpt) repo, so any and all code you see
in this post can typically be traced back to something equivalent that appears
in [train_gpt2.py](
https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py).  None
of this code would have made any sense to me a month or two ago---but I can
promise you that if you devote sufficient time to watching and understanding
the entire series, you'll have a comprehensive understanding of all of the
pieces present in this post.

You can follow along in a Jupyter Lab notebook if you activate the `py313`
environment and launch `jupyter lab`.  If you correctly registered your
`py313t` kernel per the instructions earlier, you should see an option when
creating a new notebook to use the `py313t` Python kernel, which will be the
free-threaded version.  On the right of this page you should see all of the
notebooks referenced.

## Initial Implementation

The code below roughly corresponds to my first version of the code in the
commit [3ed4fe6: Add gpt2.py](
https://github.com/tpn/parallelopedia/blob/3ed4fe60a767a12b31fca183fed00fef43c65827/src/parallelopedia/gpt2.py),
with some formatting and style tweaks to ensure the code is viewable on mobile
devices without requiring horizontal scrolling.

We'll revise this code later in the post, but for now, it's a good starting
point to get a feel for how we can use PyTorch to load a GPT-2 model
checkpoint, tokenize some input text, and generate some output text.

::: {#initial-model-setup-code}
{{< embed gpt2-v1.ipynb#gpt2-v1-setup >}}
:::

## Loading the Model

With the code above executed in a preceding Jupyter Notebook cell, we can
load the model as follows:

{{< embed gpt2-v1.ipynb#gpt2-v1-load-model >}}

## Generating Text

Once we've got a model instance, text can be generated by simply calling
the model's `generate()` function with a prompt, and, optionally, some
additional parameters like seed and max length.  This is also referred to
as inference---the two terms are interchangeable, and mean the same thing:
the act of providing some input tokens---your encoded prompt---to your
trained model, and having it generate tokens in response.

Note that this isn't a *chat* or *instruction* model, nor has it been
fine-tuned to remotely resemble something actually usable.  So you can't
ask it questions or have it write code for you.

What you can do, though, is provide it with a half written sentence, and
then laugh at the ridiculous content it generates in response.  Although
note that its syntax is pretty good---the model has clearly learned enough
during training about how the English language is structured, which words
make sense when placed together, that sort of thing.  It just has no clue
about underlying semantics.

::: {.desktop-only}
{{< embed gpt2-v1.ipynb#gpt2-v1-generate-1-desktop >}}
:::

::: {.tablet-only}
{{< embed gpt2-v1.ipynb#gpt2-v1-generate-1-tablet >}}
:::

::: {.phone-only}
{{< embed gpt2-v1.ipynb#gpt2-v1-generate-1-phone >}}
:::

Now, it's worth noting at this point that a 124 million parameter GPT2 model,
trained from scratch for 19,072 iterations on the `edu_fineweb10b` data set,
with a final loss score of 3.052, is, quite frankly, hot garbage :-)

Do not be expecting output from this model to rival anything close to what
you'd expect from a contemporary LLM.  In fact, we can't even rely on it to
even remotely generate content that is factual in nature.  It spews hot
probabilisitic garbage that mostly conforms to the structure of the English
language.

But at least it's *our* hot garbage that we trained from nothing, and it's
all we need to start playing around with generating text in parallel.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Hot Garbage Example"}

Provide a slightly different seed to the prompt above and you'll precisely
see some hot garbage generation in action.  In the following example, with
an identical prompt as the one prior, it just streams nonsense until hitting
its max token limit, nary a stop token in sight.

::: {.desktop-only}
{{< embed gpt2-v1.ipynb#gpt2-v1-generate-2-desktop >}}
:::

::: {.tablet-only}
{{< embed gpt2-v1.ipynb#gpt2-v1-generate-2-tablet >}}
:::

::: {.phone-only}
{{< embed gpt2-v1.ipynb#gpt2-v1-generate-2-phone >}}
:::

lolwat.

:::

# Parallel PyTorch Inference

Now that we've got generation working, let's tackle the fun part:
investigating whether or not PyTorch model inference can be done
simultaneously, in parallel, by multiple threads, running on multiple cores
at the same time, in a single free-threaded Python process.  And ideally, we
should only need one GPU, with all of these threads *sharing* it as fairly
as possible.  Although if we have multiple GPUs, we should be able to
distribute the incoming work evenly across those too, if we want.

This is novel, uncharted territory we're able to now explore thanks to
the free-threaded version of Python.

::: {.callout-note title="Simultaneous vs Parallel vs Concurrent"}

The phrasing I've used above---*"simultaneously, in parallel"*---is a bit
redundant.  They both imply the same thing.  When I use either word in this
post, I'm explicitly referring to the new ability unlocked by free-threaded
Python, where multiple threads can be running Python code on different CPU
cores at the same time---i.e. *simultaneously*.  And thus, you're performing
work in *parallel*.

When I use the term *concurrent* or *concurrency*, I'm using it in the
traditional sense within the context of Python: making progress on multiple
things at a time.  This term is well suited to describe things like
web servers, where a single Python process, with a single thread, running on
a single CPU core, can service multiple clients at any given time (by way
of [non-blocking socket I/O and event multiplexing](
https://speakerdeck.com/trent/pyparallel-how-we-removed-the-gil-and-exploited-all-cores?slide=27)),
but it's not servicing any of those clients *simultaneously* on different
cores, because that would require multiple threads running in *parallel*.

Some more details regarding *parallelism* vs *concurrency* can be found on
[slide
8](
https://speakerdeck.com/trent/parallelism-and-concurrency-with-python?slide=8)
of a deck I put together many years ago titled
[Parallelism and Concurrency in Python](
https://speakerdeck.com/trent/parallelism-and-concurrency-with-python).

:::

So how do we test this out?  I guess we could spin up some threads and have
them all call `model.generate()`, but that's a little boring.

Why not try implement a pure Python, multi-threaded `asyncio`-based HTTP
server, expose a `/generate`-esque style `GET` endpoint, and wire that up
to the model generation code we saw above, allowing us to serve web requests
for generation in parallel, ideally in an `asyncio`-friendly manner, where
we can stream individual tokens back one-by-one via HTTP's chunked-encoding
transfer protocol, giving each thread's event loop the ability to service
multiple clients *concurrently* in a reasonably fair manner, whilst also
servicing many clients in *parallel* across all threads, and for kicks, get
AI to whip up a janky little React Bootstrap UI that we can slap in front of
it all to test it?

## Pure Python Multi-threaded HTTP Server

First thing we need is a nice and simple `asyncio`-based HTTP server, that
also happens to work with multi-threading now that we have a free-threaded
Python at our disposal.

Luckily, I have one of those laying around already!  In support of another
article I'm actively working on (which was meant to get published before
this post), I ported the [HTTP Server](
https://github.com/pyparallel/pyparallel/blob/branches/3.3-px/Lib/async/http/server.py)
I wrote many years ago for [PyParallel](https://pyparallel.org)^[And I'm
sure I used the existing Python stdlib `http.server` code at the time as the
basis; ain't nobody got time to be writing new web servers from scratch.] to
use the new `asyncio` facilities available with Python, and then slapped
multiple threads on it, where each thread gets its own `asyncio` event loop.

Turns out, thankfully, that this Just Works, at least on
Linux^[Unfortunately, it doesn't appear to work on Windows as-is; using the
exact same code, only one thread can be seen running when the server is
loaded.  It's not doing a round-robin between *all* threads, like you'd
expect to see with the GIL enabled, there's just a single sole thread
attempting to service all incoming requests, with all other threads sitting
idle.  I don't know if it's because of something quirky with regards to
additional, non-main-thread threads not getting their own event loop
(hopefully easy to fix), or something more insidious related to how we're
misuing I/O completion ports behind the scenes in `IocpProactor()` now that
we have free-threading (much harder to fix).  I haven't had time to
investigate in more detail.]---we can now have a pure Python HTTP server,
running in a single Python process, that'll happily saturate every CPU core
under load.

The server code lives in [`parallelopedia.http.server`](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/http/server.py),
and it includes a super janky little notion of *"HTTP Apps"*, the purpose of
which can be best demonstrated with a simple example:

```python
class PlaintextApp(HttpApp):
    @route
    def plaintext(self, request):
        response = text_response(request, 'Hello, World!')
        self.server.send_response(response)

class SleeperApp(HttpApp):
    @route
    def sleep(self, request, seconds):
        time.sleep(int(seconds))
        msg = f'Slept for {seconds} seconds.')
        response = text_response(request, msg)
        self.server.send_response(response)

# Create a server with the two apps.
app_classes=[PlaintextApp, SleeperApp]
server = HttpServer(app_classes=app_classes)
```

In the above example, the HTTP server will route requests for the
`/plaintext` endpoint to an instance of the `PlaintextApp`'s
`plaintext()` routine, and `/sleep` requests get routed to the
`SleeperApp`'s `sleep()` routine.

The *"slapped multiple threads on it"* activity I refered to earlier is
handled by some new [async and threading scaffolding](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/http/server.py#L1344)
added to the bottom of that module, with the pertinent pieces reproduced
below:

```python

async def main_async(
    args: argparse.Namespace,
    protocol_class: type,
    *protocol_args: Tuple
) -> None:
    """
    This is the main function for the server when it is running in
    asynchronous mode.  It will create a server instance and then
    call serve_forever() on it.

    Arguments:

        args (argparse.Namespace): Supplies the command-line arguments.

        protocol_class (type): Supplies the protocol class to use.

        protocol_args (tuple): Supplies the arguments to pass to the
            protocol class constructor.

    """
    loop = asyncio.get_running_loop()

    if os.name in ('nt', 'cygwin'):
        reuse_port = False
    else:
        reuse_port = True

    reuse_address = True

    server = await loop.create_server(
        lambda: protocol_class(*protocol_args),
        args.ip,
        args.port,
        backlog=args.listen_backlog,
        reuse_address=reuse_address,
        reuse_port=reuse_port,
    )

    async with server:
        await server.serve_forever()


def start_event_loop(
    args: argparse.Namespace,
    protocol_class: type,
    *protocol_args: Tuple
) -> None:
    """
    This function will start the asyncio event loop and run
    the main_async() function.  It is intended to be the
    target of a threading.Thread.

    Arguments:

        args (argparse.Namespace): Supplies the command-line arguments.

        protocol_class (type): Supplies the protocol class to use.

        protocol_args (tuple): Supplies the arguments to pass to the
            protocol class constructor.
    """
    asyncio.run(
        main_async(
            args,
            protocol_class,
            *protocol_args,
        ),
        debug=args.debug,
    )


def main_threaded_multi_accept(
    args: argparse.Namespace,
    protocol_class: type,
    *protocol_args: Tuple
) -> None:
    """
    This is the main function for the server when it is running in
    multi-threaded mode with multiple accept sockets.  Each thread
    will have its own asyncio loop issue a create_server() call for
    the given host/port and protocol.

    Arguments:

        args (argparse.Namespace): Supplies the command-line arguments.

        protocol_class (type): Supplies the protocol class to use.

        protocol_args (tuple): Supplies the arguments to pass to the
            protocol class constructor.
    """
    import threading

    threads = []
    for _ in range(args.threads):
        thread = threading.Thread(
            target=start_event_loop,
            args=(args, protocol_class, *protocol_args),
        )
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()


def main(args: Optional[argparse.Namespace] = None):
    """
    Main entry point for parallelopedia.http.server module.
    """
    args = parse_arguments()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

    # Use multiple threads to load the application classes.
    app_classes = get_classes_from_strings_parallel(
        args.app_classes,
    )

    protocol_class = get_class_from_string(args.protocol_class)
    protocol_args = (app_classes,)

    if args.threads == 1:
        asyncio.run(
            main_async(
                args,
                protocol_class,
                *protocol_args,
            ),
            debug=args.debug,
        )
    else:
        main_threaded_multi_accept(
            args,
            protocol_class,
            *protocol_args,
        )

if __name__ == '__main__':
    main()
```

## GPT2 HTTP App

With the HTTP server scaffolding in place, we can now whip up a little
`Gpt2App` class that has a `generate()` method.  Incoming requests to
the `/generate` endpoint will be routed to that routine by the server.

### Synchronous Up-Front Generation

Now, we could take the simple approach, where the `Gpt2App.generate()`
call goes off and calls `model.generate()` and then patiently waits for the
entire response to be generated before sending anything back to the user.

That code would look something like this:

```python
class Gpt2App(HttpApp):
    @route
    def generate(self, request: Request,
                 *args: List,
                 **kwds: Dict) -> None:
        prompt = args[0]
        model = get_model()
        result = model.generate(prompt=prompt)
        respose = text_response(request, result)
        self.server.send_response(response)
```

But when have you ever interacted with an LLM via a web interface where it
waits until it generates *all* of the response up-front before sending it
back to you?  Never; you can see it generate the response in real time, and
that's what we want to mimic here in this experiment.

### Our Goals

The high-level goals for our solution are thus:

1. We want to send a generated token back to the user as soon as it becomes
   available.

2. We want to ensure the client receiving the token can display it as soon
   as they receive it---so we need to be cognizant of what HTTP transfer
   protocol we use to send bytes back.  If we just used normal HTTP transfer
   encoding, the client would wait until we've sent *everything* before the
   user sees it, despite the fact that we've been trickling individual
   tokens to them the entire time.

3. We want to play nicely with the `asyncio` ecosystem upon which our
   hosting HTTP server is based---so we need to be cognizant of the current
   thread's event loop, and make sure we don't impede that thread's ability
   to *concurrently* serve other clients that are being handled by the event
   loop.

Thankfully, as we saw earlier with the implementation of the
`GPT.generate()` routine, generating tokens in response to a prompt is
inherently a *token-by-token* process.  So the algorithm at least provides
us with the means to obtain a single token at a time, which takes care of
the first point.

Second, HTTP's chunked-encoding transfer protocol will allow a HTTP client
to immediately *"see"* the tokens we send back to it as soon as we send
them, provided we enable `TCP_NODELAY` on the underlying socket to ensure
the operating system sends the bytes out to the client as soon as we send
them.

::: {.callout-note}

If we didn't do this, the default behavior of [Nagle's algorithm](
https://en.wikipedia.org/wiki/Nagle%27s_algorithm) would apply, and the
operating system would delay sending individual bytes back when we request,
in the hope that it can accumulate more bytes to send all at once at a
slightly later point in time.  This is advantageous for maximizing
throughput, but it impedes latency, and in our case, we want the lower
latency afforded by immediately sending the bytes back to the client as soon
as we generate them.

:::

[Chunked-encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding)
works by setting an HTTP response header `Transfer-Encoding: chunked`, and
then in the body, each chunk is transmitted by its length and then the chunk
itself.  The server communicates to the client that the transfer has
completed once a zero-length chunk is received.

So, as long as we send our tokens back via chunked-encoding, any HTTP/1.1
client will be able to reassemble them back into the generated text, giving
the visual appearance of real time model generation.  That will take care
of the second point.

Lastly, in order to play nice within the greater `asyncio` ecosystem, we
need to give control back to the underlying thread's `asyncio` event loop
after we generate a token and yield a decoded text fragment, which can
thankfully be done via a simple call to `await asyncio.sleep(0)`, provided
we're generating text from the model from within an `async` callback.

This ensures multiple *concurrent* clients being handled by our thread's
event loop will be handled fairly; they'll all receive generated tokens
at the same rate.

### Asynchronous Token-by-Token Generation

The first thing we need to do is to change our `Gpt2App.generate()` call
into something that is `async` compatible, in anticipation of some later
code that we write needing to issue an `await asyncio.sleep(0)`, which
can only be done within a call frame of an asynchronous method.

When our `Gpt2App.generate()` routine is called, we're still within the
context of the `asyncio` protocol's `data_received()` routine, which is a
normal, synchronous method, *not* an enlightened `async` method that can
participate in an `asyncio` event loop.

So, in order to transition from a synchronous callback to an asynchronous
one, we can use the current event loop's `create_task()` routine to enqueue
an `async` method for execution.

#### Step 1: Have generate() enqueue an async generate_response().

Thus, our `Gpt2App.generate()` call will look something like this:

```python

class Gpt2App(HttpApp):

    ...

    @route
    def generate(self, request: Request,
                 *args: List,
                 **kwds: Dict) -> None:

        # Extract the "prompt" provided in the incoming request.
        text = args[0]

        # Obtain the event loop and schedule the response
        # generation via our async generation coroutine.
        # We have to do it like this as at this point we're
        # still within the call frame of the data_received()
        # protocol callback, which isn't an async function.
        loop = asyncio.get_running_loop()
        async_task = self.generate_response(request, text, **kwds)
        loop.create_task(async_task)
```

#### Step 2: Implement an async generate_response()

Our asynchronous `generate_response()` routine will be the bridge between
generating tokens from the model, and sending those tokens back to the
client via chunked-encoding.

It is responsible for preparing the response to use chunked-encoding,
and then enabling `TCP_NODELAY` on the socket.

Then, assuming that our model has an `async_generate_for()` routine,
which we'll implement in the next step, we perform an `async for` loop
over that routine in order to obtain individual *decoded* tokens.  As
soon as we receive a token, we can send it back to the client via the
`response` object's `send_chunk()` routine.

Once we've exhausted the async generator (i.e. it either generated the
maximum number of requested tokens, or it encountered a stop token), we
can re-enable `TCP_NODELAY`, and then return.

A simplified version of the Python code is presented below.  I have omitted
most of the error handling and query parameter parsing code for simplicity;
see the expandable code block at the end for the full version.

```python

class Gpt2App(HttpApp):

    ...

    async def generate_response(
        self, request: Request,
        prompt: str,
        **kwds: Dict
    ) -> None:

        # Prepare a chunked-encoding response.
        response = request.response
        response.code = 200
        response.message = 'OK'
        response.chunked_response = True
        response.content_type = 'text/plain'

        # Obtain the model.
        model = get_model()

        # We want to enable TCP_NODELAY for the duration of
        # the response.  This ensures packets are sent
        # immediately without any internal buffering.
        try:
            response.enable_tcp_nodelay()
            enabled_nodelay = True
        except Exception as e:
            logging.error(f'Error enabling TCP_NODELAY: {e}')
            enabled_nodelay = False

        # Write the chunked header immediately.
        response_bytes = bytes(response)
        if not self.write(response_bytes):
            # Encountered a disconnect, return.
            return

        # N.B. From herein, all data must be transferred to
        #      the client via chunked encoding with the
        #      `response.send_chunk()` routine.

        # Send the initial prompt text.
        response.send_chunk(prompt)

        # Obtain decoded tokens from the model one at a time
        # via an `async for` loop, sending the token back to
        # the client as soon as it's available.
        async for decoded_token in model.generate_async_for(prompt):
            response.send_chunk(decoded_token)

        # Terminate the chunked-encoding response.
        response.end_chunks()

        # Disable TCP_NODELAY now that the response is complete.
        # The reasoning behind this is that the client may have
        # issued the HTTP request with a keep-alive header, and
        # plans on reusing this connection for a different request
        # next, which won't necessarily want `TCP_NODELAY` active.
        if enabled_nodelay:
            response.disable_tcp_nodelay()
```

::: {.callout-note #gpt2app-generate-response-full collapse="true" appearance="simple" icon="false" title="Full Code for async Gpt2App.generate_response()"}

The actual code has more robust error-handling facilities and support for
extracting the query string parameters from the incoming request URI and
converting them into keyword arguments suitable for passing to the model.

Additionally, we haven't touched on how we initialize or obtain instances of
our models yet, so the model-related code won't make much sense until later
in the article.

```python

class Gpt2App(HttpApp):
    routes = make_routes()
    route = router(routes)

    def __init__(self, server: HttpServer) -> None:
        super().__init__(server)
        self.printable = PRINTABLE

    def is_connected(self):
        # server.transport will be severed when the client
        # disconnects, so we can use this to determine if
        # the client is still connected.
        server = self.server
        transport = None
        try:
            transport = server.transport
        except AttributeError:
            pass
        return transport is not None

    def write(self, response_bytes):
        server = self.server
        transport = None
        try:
            transport = server.transport
        except AttributeError:
            pass
        if transport is not None:
            transport.write(response_bytes)
            return True
        else:
            return False

    async def generate_response(
        self, request: Request, prompt: str, **kwds: Dict
    ) -> None:

        response = request.response

        response.code = 200
        response.message = 'OK'
        response.chunked_response = True
        response.content_type = 'text/plain'

        if kwds is None:
            kwds = {}
        max_length = min(int(kwds.get('max_length', 100) or 100), 1024)
        top_k = min(int(kwds.get('top_k', 50) or 50), 50)
        seed = kwds.get('seed', None)
        if seed:
            try:
                seed = int(seed)
            except ValueError:
                seed = None
        if not seed:
            seed = random.randint(0, 2**32 - 1)

        device = kwds.get('device', None)

        model_name = kwds.get('model', None)
        if model_name == 'gpt2-xl':
            models = PRETRAINED_MODELS
            get_next = get_next_pretrained_model
        else:
            model_name = 'gpt2'
            models = MODELS
            get_next = get_next_model

        model = None
        if device is not None:
            if device == 'cpu':
                model = models[-1]
            elif device.startswith('cuda:'):
                try:
                    index = int(device[5:])
                except ValueError:
                    index = -1
                if index < 0 or index >= NUM_GPUS:
                    index = -1
                if index != -1:
                    model = models[index]
            elif device == 'cuda':
                model = models[random.randint(0, NUM_GPUS - 1)]

        if not model:
            # Get a model.  If there are multiple models available, e.g. if we
            # have multiple GPUs, this will balance the load a bit.
            model = get_next()

        expose_headers = (
            'Access-Control-Expose-Headers: '
            'X-Max-Length, '
            'X-Top-K, '
            'X-Seed, '
            'X-Model-Name, '
            'X-Model-Device'
        )
        response.other_headers.extend([
            expose_headers,
            f'X-Max-Length: {max_length}',
            f'X-Top-K: {top_k}',
            f'X-Seed: {seed}',
            f'X-Model-Name: {model_name}',
            f'X-Model-Device: {model.device}',
        ])

        # We want to enable TCP_NODELAY for the duration of
        # the response.  This ensures packets are sent
        # immediately without any internal buffering.
        try:
            response.enable_tcp_nodelay()
            enabled_nodelay = True
        except Exception as e:
            logging.error(f'Error enabling TCP_NODELAY: {e}')
            enabled_nodelay = False

        # Write the chunked header immediately.
        response_bytes = bytes(response)
        if not self.write(response_bytes):
            # Encountered a disconnect, return.
            return

        # N.B. From herein, all data must be transferred to
        #      the client via chunked encoding with the
        #      `response.send_chunk()` routine.

        # Send the initial prompt text.
        response.send_chunk(prompt)

        # Obtain an async generator instance to the model's
        # new async token generation routine.
        generate_tokens = model.generate_async_for(
            prompt,
            max_length=max_length,
            top_k=top_k,
            seed=seed,
        )
        async for decoded_token in generate_tokens:
            if decoded_token == -1:
                # A non-printable token was generated,
                # terminating generation.
                response.send_chunk(
                    OOPS_NON_PRINTABLE_ENCOUNTERED
                )
                break

            # If the client has forcibly disconnected,
            # terminate generation.
            if not self.is_connected():
                break

            # Otherwise, send the decoded token to the client
            # via chunked encoding.
            response.send_chunk(decoded_token)

        # Send the termination chunk.  This may fail at the
        # socket.send() level if the client has already
        # disconnected, which is harmless.
        response.end_chunks()

        # Disable TCP_NODELAY now that the response is complete.
        # Again, this may fail at the socket level if the client
        # has already disconnected, which is harmless.
        if enabled_nodelay:
            try:
                response.disable_tcp_nodelay()
            except Exception as e:
                msg = f'Error disabling TCP_NODELAY: {e}'
                logging.error(msg)

```

:::


#### Step 3: Implement an async GPT.async_generate_for()

In the code example above, we assumed the `GPT` model we've been using had
grown a new `async` routine named `async_generate_for()`, which we'll cover
now.

This routine is essentially an `asyncio`-friendly version of the original
`generate()` routine we wrote.  It shares a lot of the same code, with a few
notable tweaks in order to support the fact that it is being called from a
callback that was enqueued on a thread's `asyncio` event loop, and it is
expected to yield a token as soon as it is available, and then pass control
back to the event loop in order for it to service other clients before it
continues with generating the next token.

It also has the notion of checking for *"printable"* characters.  This came
about when I was initially testing this code via `curl` which would
sometimes balk and exit in the middle of streaming the response, citing that
it encountered corrupted data or something like that.

After investigation, it turned out that sometimes, for whatever reason, the
model just generates a junk, nonsensical token (like 65534, which is well
outside the highest token number of 50384).  I have no idea why it happens,
although I'll note it happens on the OpenAI GPT2 XL model available on
HuggingFace (which we'll discuss later) too, so, eh.

I deal with this by checking if we've generated a non-printable token after
decoding it, and, if so, return -1 and terminate the loop.  The
[full version](#gpt2app-generate-response-full) of the
`Gpt2App.generate_response()` routine that we introduced above checks if
we return -1, and if so, terminates generation with an oopsie message, e.g.:

```python
OOPS_NON_PRINTABLE_ENCOUNTERED = (
    'Oops! Non-printable token encountered.  Generation terminated.'
)
...

async for decoded_token in generate_tokens:
    if decoded_token == -1:
        # A non-printable token was generated,
        # terminating generation.
        response.send_chunk(
            OOPS_NON_PRINTABLE_ENCOUNTERED
        )
        break
```

After yielding a valid decoded token, we issue an `await asyncio.sleep(0)`
call, which returns control back to the event loop for it to potentially
handle other concurrent clients.  If there are no other clients, or after
it has handled all other enqueued work, generation resumes.

The full code follows, it is simple enough as-is that I didn't feel the need
to omit any details like in the prior example.

```python

class GPT:

    ...

    async def generate_async_for(
        self, text: str, max_length: int = 1024, top_k: int = 50,
        seed: int = None,
    ):
        """
        Asynchronously generate text from the model, yielding tokens
        one at a time as soon as they are available.

        Args:

            text (str): Supplies the prompt.

            max_length (int): Supplies the maximum total length,
                including prompt.

            top_k (int): Supplies the number of tokens to consider
                at each generation step.

            seed (int): Optionally supplies the manual seed to use
                for the generator.  If None, the model's manual
                seed will be used.

        Yields:

            byte: The newly generated decoded token.  If -1, a
            non-printable token was generated, and generation
            was terminated.
        """

        enc = self.enc
        stop_token = self.stop_token

        # Encode the prompt -> tensor of shape (1, T)
        tokens = enc.encode(text)
        x = torch.tensor(
            tokens, dtype=torch.long, device=self.device
        ).unsqueeze(0)

        sample_rng = torch.Generator(device=self.device)
        if seed is None:
            seed = self.manual_seed
        sample_rng.manual_seed(seed)

        logging.debug(
            f'[generate_async_for] Starting generation loop for {text} '
            f'with seed {seed}.'
        )

        start_time = time.perf_counter()
        count = 0
        while x.size(1) < max_length:
            count += 1
            with torch.no_grad():
                # Forward pass, ignoring the returned loss.
                (logits, _) = self(x)

            # Take the logits at the last time-step (shape:
            # (1, vocab_size)).
            logits = logits[:, -1, :]

            # Convert to probabilities.
            probs = F.softmax(logits, dim=-1)

            # Top-k sampling.
            topk_probs, topk_indices = torch.topk(
                probs, k=top_k, dim=-1,
            )

            # Sample the next token.
            next_idx = torch.multinomial(
                topk_probs,
                num_samples=1,
                generator=sample_rng,
            )
            next_token = torch.gather(topk_indices, -1, next_idx)

            # If the next token is the stop token, we're done.
            next_token_item = next_token.item()
            if next_token_item == stop_token:
                break

            # Append token to current sequence.  Although we only
            # yield a singular decoded token below, we still need
            # to keep track of the entire sequence for subsequent
            # generation steps.
            x = torch.cat((x, next_token), dim=1)

            # Decode the newly-generated token.  Note that a single
            # token will often be decoded to multiple characters.
            new_text_fragment = enc.decode([next_token.item()])

            # If any of the characters in the decoded text
            # representation aren't printable, terminate
            # generation.
            if not all(c in self.printable for c in new_text_fragment):
                yield -1
                break

            yield new_text_fragment

            # Yield control back to the event loop before continuing
            # generation.  If we didn't do this, this client would
            # hog the thread's event loop, preventing other clients
            # associated with the loop from getting serviced.  (As
            # we're now running multiple threads in parallel, other
            # clients associated with event loops on other threads
            # would not be impacted.)
            await asyncio.sleep(0)

        elapsed = time.perf_counter() - start_time
        logging.debug(
            f"[generate_async_for] Generated {count} tokens in "
            f"{elapsed:.2f} seconds (~{count / elapsed:.2f} tok/s)"
        )
```

This routine was the last piece we needed to implement to satisfy
[our three goals](#our-goals) captured earlier, so, we're now ready to test
it out!

## Test Drive!

### Launching the HTTP Server

We can launch an instance of our multi-threaded HTTP web server with our new
`Gpt2App` HTTP application via the command line as follows:

```bash
% python -Xgil=0 -m parallelopedia.http.server  \
    --threads 40 --ip 0.0.0.0 --port 4444       \
    --app-classes parallelopedia.gpt2.Gpt2App   \
                  parallelopedia.http.server.PlaintextApp
```

This will start up a multi-threaded HTTP server listening on all interfaces
on port 4444, with 40 threads, and two HTTP applications: our `Gpt2App`,
which will service requests to the `/generate` endpoint, and a
`PlaintextApp` that just returns "Hello, World!" to any incoming request
received for the `/plaintext` endpoint.

### Visualizing Chunked-Encoding

Let's visualize the generation response in a way that shows us the raw
chunked-encoding, without doing any client-side reassembly.  We can achieve
that with `echo` and `netcat` (`nc`):

```bash
% echo -en \
 'GET /generate/The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42 ' \
 'HTTP/1.1\r\nConnection: close\r\nHost: dgx\r\n\r\n' | nc dgx 4444

```

That should yield the following raw output:

```
HTTP/1.1 200 OK
Server: Parallelopedia Web Server v1.0
Date: Fri, 07 Feb 2025 23:32:02 GMT
Accept-Ranges: bytes
Content-Type: text/plain
Access-Control-Allow-Origin: *
Connection: close
Transfer-Encoding: chunked
Access-Control-Expose-Headers: X-Max-Length, X-Top-K, X-Seed, X-Model-Name, X-Model-Device
X-Max-Length: 20
X-Top-K: 50
X-Seed: 42
X-Model-Name: gpt2
X-Model-Device: cuda:0

13
The quick brown fox
3
 is
2
 a
4
 sub
7
species
5
 that
B
 originated
3
 in
9
 southern
9
 Scotland
3
 as
2
 a
8
 variety
3
 of
4
 fox
1
.
5
 This
0

```

As you can see, we've enabled chunked-encoding by way of the
`Transfer-Encoding: chunked` header.  And the body of the response is
comprised of these *"chunks"*; specifically, each bit of decoded text is
preceded by its length, in bytes, then followed by `\r\n`, then followed by
the text itself.

The zero-length chunk at the end indicates completion of the transfer, and
as we requested `Connection: close` in our headers, our HTTP server closes
the connection once the generation has completed.

::: {.callout-note}

In HTTP/1.1, *"keep-alive"* is the default behavior---i.e., a server won't
close a connection unless the client specifically requests it.  This is
the opposite of HTTP/1.0 behavior, where the server will close a connection
by default unless a client furnishes a `Connection: keep-alive` header.

:::

### Verifying via Curl

If we switch over to `curl` and run the same generation request, we'll see
the *reassembled* text, and, provided we supply the `--no-buffer` argument,
`curl` will also display decoded text as soon as it receives it.

```bash
% curl --no-buffer --verbose \
    'http://dgx:4444/generate/The%20quick%20brown%20fox?' \
        'max_length=20&seed=42&device=cuda'
*   Trying 10.0.132.48:4444...
* Connected to dgx (10.0.132.48) port 4444 (#0)
> GET /generate/The%20quick%20brown%20fox?max_length=20&seed=42&device=cuda HTTP/1.1
> Host: dgx:4444
> User-Agent: curl/7.81.0
> Accept: */*
> 
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: Parallelopedia Web Server v1.0
< Date: Fri, 07 Feb 2025 23:05:34 GMT
< Accept-Ranges: bytes
< Content-Type: text/plain
< Access-Control-Allow-Origin: *
< Transfer-Encoding: chunked
< Access-Control-Expose-Headers: X-Max-Length, X-Top-K, X-Seed, X-Model-Name, X-Model-Device
< X-Max-Length: 20
< X-Top-K: 50
< X-Seed: 42
< X-Model-Name: gpt2
< X-Model-Device: cuda:2
< 
The quick brown fox is a subspecies that originated in southern Scotland as a variety of fox. This* Connection #0 to host dgx left intact
```

### Launching the React Bootstrap UI

The React Bootstrap UI can be launched as follows:

```bash
% cd $PARALLELOPEDIA_UI # i.e. root of gh:tpn/parallelopedia-ui
% conda activate py313t # or whatever has recent nodejs/npm
% npm start run
```

::: {.callout-note}

Full disclaimer: I'm not a web developer.  I don't know JavaScript, React,
or Bootstrap, or anything else in the modern web stack.  So like any good
developer in 2025 when confronted with a task they have absolutely no
business doing... I palmed it off to AI---either ChatGPT, local LLMs via
[LM Studio](https://lmstudio.ai/), or, more recently, [Aider](
https://aider.chat).

TL;DR: the web interface code probably sucks.

:::

Running `npm start run` should open up a browser window pointing at
`http://localhost:3000`.  Ignore the *Wiki* tab---that's for another
article---and switch to the *GPT2* tab.  You should see something similar
to the GIF below, which is a demo of me using the interface to generate
some text:

::: {.callout-note #parallelopedia-ui-gpt2-example collapse="false" appearance="simple" icon="false" title="Parallelopedia UI Demo"}

![Parallelopedia UI Demo](images/parallelopedia-ui-example.gif)

[GPT2.jsx Code](
https://github.com/tpn/parallelopedia-ui/blob/main/src/GPT2.jsx)

:::

Not too shabby!  Granted, it's a tad jarring seeing characters per second
instead of tokens per second.  If we wanted to display a live tokens/sec
rate, some possible options might be:

1. Alter `GPT.async_generate_for()` to calculate how long it took to
   generate each token (whilst we're in the main generation loop),
   convert that into a tokens/sec rate, and then change our API on both the
   server side and the JavaScript UI side such that each chunk that gets
   sent back is encoded with both the rate *and* the actual chunk.  A
   drawback of this approach is that hitting the `/generate` endpoint with
   `curl` or `netcat` would look wacky, as you'd see token/sec floats in
   between each generated set of decoded characters.

2. Send the raw integer tokens back as they're generated, instead of first
   decoding them.  That would allow client-side JavaScript to calculate
   tokens/sec, but it would make it impossible to easily inspect the output
   with our command line tools like `curl` or `echo ... | nc`.  It would
   also require adding a JavaScript decoding library on the client side
   (although that's not such a big deal, I think you can just do `npm
   install tiktoken` these days).

3. Have the JavaScript client re-encode the decoded characters received back
   into their GPT2-tokenizer token representation in order to determine
   actual underlying token count, and recalculate the rate based off that.
   This rate would differ from the tokens/sec rate observed on the server
   because it would also include network latency.

4. Do something more advanced with a separate WebSocket or something,
   where the live tokens/sec generation rate can be displayed independently
   to the generated decoded tokens.

4. Just YOLO it and have the JavaScript code divide the characters per
   second by, say, three, and pretend that's roughly the tokens/sec rate
   (assuming that on average, one token represents three characters in any
   given response).

I don't want to do any of that!  So characters per second it is, for now,
however jarring it may be.

## Parallel Generation

Alright, we've exercised the `/generate` endpoint a few different ways via a
single client connection, and it looks like it's behaving the way we hoped
it would, albeit with a whopping simultaneous client count of, precisely,
one.

Let's look at introducing some simultaneous load by way of multiple machines
all issuing `/generate` requests at the same time, first via the
`echo ... | nc` approach, and then via `curl`.

I'll leverage `tmux` and the `:setw synchronize-panes` command, which sends
my console keystrokes to all active panes within a given `tmux` session
window.

### Netcat Example

I captured a 15 frames-per-second GIF of this in action, which you can view
below.  It shows terminal sessions to six machines, arranged vertically, all
executing the same `echo ... | nc` command depicted above.

::: {.callout-note}

This might seem like a bit of a silly test, especially when it's working
correctly, because the output is pretty benign, and we don't *really* know
that these requests were actually being served simultaneously by different
threads on the server side.

When I first tried it, though, it absolutely did *not* work correctly---one
server would get correct output, another would get the HTTP headers in
triplicate, whilst two other sessions just went straight into the chunked
responses, with no preceding HTTP headers in sight.  I had a bug in my HTTP
routing code (that exists in the PyParallel code I wrote ten years ago upon
which I based the new HTTP server on!) which was entirely to blame.  With
that code ripped out and `@route` reimplemented in a much simpler fashion,
everything worked well.

:::

::: {.callout-note #parallel-netcat-generation collapse="false" appearance="simple" icon="false" title="Parallel Netcat Generation"}

![Parallel Netcat Generation](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-nc.gif)

:::

I extracted the last frame of the GIF, below, where you can see that at
least there was some variance between which GPUs were selected:

![Parallel Netcat Generation - Last Frame](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-nc-last-frame-2.png)

### Curl Example

I did the same thing with the panes arranged horizontally, using `curl`
instead and a `max_length=1000` and no seed, which helps in visualizing
the parallel generation, as you can clearly see different clients are
receiving completely different outputs.

::: {.callout-note #parallel-curl-generation collapse="false" appearance="simple" icon="false" title="Parallel Curl Generation"}

![Parallel Curl Generation](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-curl-1000.gif)

:::

## Let's Load Test This Sucker!

So far so good!  Looks like we can absolutely do PyTorch model generation in
parallel via multiple threads thanks to free-threaded Python.

Let's ramp it up a notch and see what happens if we attempt to load test
the `/generate` endpoint from a different server.  The `leopard` server
you see in the next example is an Intel(R) Xeon(R) W-2275 CPU @ 3.30GHz
(14 cores, 28 threads) and is connected to the `dgx` box via a 10G Ethernet
link.

Using the fork of [`wrk`](https://github.com/tpn/wrk) I hacked together some
ten years ago or so whilst load testing PyParallel, let's kick off a run
with 14 threads for 30 seconds.  There will be one connection per thread,
and HTTP/1.1 will be used, so the implicit *keep-alive* behavior means that
as soon as one `/generate` request completes, another one is immediately
dispatched.

For posterity, the console command being used is as follows:

```bash
% time ./wrk -v --latency -c14 -t14 -d30s \
    'http://dgx:4444/generate/'
    'The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42'
```

In the GIF below you'll see two terminal windows.  The smaller foreground
window on the left is `leopard`, the session doing the `wrk` load test.  The
background window that takes up the rest of the screen is logged in to
`dgx`, our server, and it's running a GPU-enabled build of [`btop`](
https://github.com/aristocratos/btop), which is a beautiful console app for
displaying resource usage.  I particularly like `btop` in this example as it
does a good job of conveying the CPU and GPU load that kicks in when the
load test starts.

The 40 CPU cores can be seen in the top pane, and, as expected, about 14 of
them whirr into life as soon as the load test begins, which tracks, as there
are now 14 clients attempting to hammer our `/generate` end point.

Below that, you can see the four Tesla V100-DGXS-32GB GPUs also whirr into
action, handling the generation between them with relative ease.

On the bottom right, I've filtered the process tree to just display our
`python` HTTP server invocation.  (I would love it if `btop` showed the
individual threads and their load, as it would beautifully depict Python
saturating cores in parallel now that there's no GIL impeding execution,
however, I don't believe that's currently possible with this version of
`btop`.)

### Parallel Load Testing (No GIL)

::: {.callout-note #parallel-generation-load-test-no-gil collapse="false" appearance="simple" icon="false" title="Parallel Generation Load Test: No GIL"}

![Parallel Generation Load Test - No GIL](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14.gif)

:::

I've extracted the last frame, below, to allow closer inspection at the end
of the run, without the annoyance of the GIF looping.

![Parallel Generation Load Test No GIL - Last Frame](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-last-frame-optimized.png)

The console output from `wrk` is captured in the callout below.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Console Output of wrk: No GIL"}

```
{{< include wrk-leopard-01-no-gil.txt >}}
```

:::

A visualization of the latencies is presented below.  All code for
visualization is from the [*Data Visualization*](wrk-analysis-preview.html)
Jupyter Notebook, which you can [preview here](wrk-analysis.ipynb), or
access directly on [Github](
https://github.com/tpn/website/blob/main/articles/pytorch-and-python-free-threading/wrk-analysis.ipynb).

::: {.theme-light}
![Parallel Load Test Latency Distribution (No GIL)](wrk-01-no-gil.svg)
:::

::: {.theme-dark}
![Parallel Load Test Latency Distribution (No GIL)](wrk-01-no-gil-dark.svg)
:::

### Ablation Test: Re-enable the GIL

Let's see what happens if we re-enable the GIL.  We should see poor resource
utilization on the server side, as only one Python thread can run at a time,
and the statistics reported on the client side should be equally dismal.

Expand the callout below to view the GIF.  I have used another terminal
window to launch the server with the `-Xgil=1` argument, which re-enables
the GIL.  I then switch back over to `leopard` and perform the `wrk` load
test like before.

::: {.callout-note #parallel-load-test-gil collapse="false" appearance="simple" icon="false" title="Parallel Generation Load Test: GIL Enabled"}

![Parallel Generation Load Test - GIL Enabled](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-GIL-enabled.gif)

:::

As with before, I've extracted the last frame, below, to allow closer
inspection at the end of the run, without the annoyance of the GIF looping.

![Parallel Generation Load Test - GIL Enabled - Last Frame](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-GIL-enabled-last-frame-optimized.png)

As we expected: re-enabling the GIL absolutely murders resource utilization.

The console output from `wrk` follows in the next callout.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Console Output of wrk: GIL Enabled"}

```
{{< include wrk-leopard-01-gil.txt >}}
```
:::

And a visualization of the latencies follows.  Note that there were 60
socket timeouts in this case, whereas no timeouts were encountered in the
prior run with the GIL disabled.

::: {.theme-light}
![Parallel Load Test Latency Distribution (GIL Enabled)](wrk-01-gil.svg)
:::

::: {.theme-dark}
![Parallel Load Test Latency Distribution (GIL Enabled)](wrk-01-gil-dark.svg)
:::

### Parallel Load Testing: No GIL vs GIL

Viewing them side by side:

::: {.theme-light}
![Parallel Load Test Latency Distribution (No GIL & GIL Enabled Combined)](
wrk-01-combined.svg)
:::

::: {.theme-dark}
![Parallel Load Test Latency Distribution (No GIL & GIL Enabled Combined)](
wrk-01-combined-dark.svg)
:::

Remember to keep in mind that the No GIL vs GIL requests/sec was 70.80 vs
7.35, nearly a ten-fold increase:

::: {.theme-light}
![Parallel Load Test Requests/sec (No GIL vs GIL)](
wrk-01-rps-combined.svg)
:::

::: {.theme-dark}
![Parallel Load Test Requests/sec (No GIL vs GIL)](
wrk-01-rps-combined-dark.svg)
:::

### Parallel Load Testing: How does Plaintext Fare?

As we launched our HTTP server invocation earlier with the `PlaintextApp`
class, which simply responds to the `/plaintext` endpoint with `b'Hello
World\n'`, let's throw some load at that too.

This doesn't have anything to do with PyTorch; it's an orthogonal load test
that's fun because it depicts the stark difference between GIL vs no GIL.

The console command was issued on `leopard`, like last time, as follows:

```bash
% time ./wrk -v --latency -c14 -t14 -d30s 'http://dgx:4444/plaintext'
```

Console outputs for both tests follow.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Console Output of wrk Plaintext: No GIL"}
```
{{< include wrk-leopard-02-no-gil.txt >}}
```
:::

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Console Output of wrk Plaintext: GIL Enabled"}
```
{{< include wrk-leopard-02-gil.txt >}}
```
:::

Combined requests/sec visualization depicts a similar 10x improvement:

::: {.theme-light}
![Plaintext Parallel Load Test Requests/sec (No GIL vs GIL)](
wrk-02-rps-combined.svg)
:::

::: {.theme-dark}
![Plaintext Parallel Load Test Requests/sec (No GIL vs GIL)](
wrk-02-rps-combined-dark.svg)
:::

The latencies are a bit misleading when viewed in isolation, as you can see
the tail end of the *No GIL* run incur higher latencies compared to the
*GIL* run---however, as can be seen above, the *No GIL* run was doing 10x
more requests/sec.

::: {.theme-light}
![Plaintext Parallel Load Test Latency Distribution (No GIL vs GIL)](
wrk-02-combined.svg)
:::

::: {.theme-dark}
![Plaintext Parallel Load Test Latency Distribution (No GIL vs GIL)](
wrk-02-combined-dark.svg)
:::

### Parallel Load Test Summary

I think it's safe to say we've achieved our original goals in a satisfactory
manner.  We can absolutely now do parallel model inference in a single
Python process, thanks to free-threading, and the whole thing works great
when wrapped around an `asyncio`-based HTTP server that yields tokens
one-by-one as soon as they're generated, which is a closer representation of
what you'd want in the real world if you were to deploy such a thing.

My takeaway from all of this: free-threading kicks ass.  Having devoted so
much time toward a parallel Python solution with [PyParallel](
https://pyparallel.org) over a decade ago, it makes me incredibly happy to
see a working, performant solution finally getting mainlined into the core
Python language.

It's important to consider how many other things are simultaneously unlocked
by Python free-threading.  The HTTP server we've demonstrated above also has
directory/file-serving behavior built-in, so it also functions like a normal
web server would.  In a separate article, I'll introduce the *Wiki* server
component, which features a `WikiApp` HTTP app that loads a 56GB XML file,
plus about 12GB of supporting index structures (by way of `datrie` digital
search tries, and NumPy arrays).  This app happily loads in the same single
Python process as our existing `Gpt2App`---demonstrating the power of
accessing huge data structures in parallel by multiple threads, something
that couldn't be done with `multiprocessing` without paying for the cost to
replicate that memory overhead in every process.

# Finishing Up

I'll tackle two more topics before we conclude this post.  First, I want to
briefly touch on a handful of the changes I made to the GPT-2 implementation
after the version we introduced in the [Initial Implementation](
#initial-implementation) section.

Second, let's see what happens if we throw the new graph compilation
optimizations in PyTorch 2.0 at our solution; specifically, can we still use
`model = torch.compile(model)` in our free-threading solution?

## Reviewing Initial Implementation

### Skipping Weight Initialization

The first version of the `GPT` class we introduced [here](
#initial-model-setup-code) looked like this:

```python
class GPT(nn.Module):

    def __init__(self, config, device):
        super().__init__()
        self.config = config
        self.device = device
        self.manual_seed = 42

        self.transformer = nn.ModuleDict(
            dict(
                wte=nn.Embedding(config.vocab_size, config.n_embd),
                wpe=nn.Embedding(config.block_size, config.n_embd),
                h=nn.ModuleList(
                    [Block(config) for _ in range(config.n_layer)]
                ),
                ln_f=nn.LayerNorm(config.n_embd),
            )
        )
        self.lm_head = nn.Linear(
            config.n_embd, config.vocab_size, bias=False
        )

        self.transformer.wte.weight = self.lm_head.weight
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            std = 0.02
            if hasattr(module, "NANOGPT_SCALE_INIT"):
                std *= (2 * self.config.n_layer) ** -0.5
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

As I mentioned earlier, I ripped that almost verbatim from Andrej's
`train_gpt2.py` code.  For about two weeks during development of this work,
I was using a local free-threaded build of PyTorch, but had forgotten that I
had built it in *debug* configuration.

Everything was dog slow, but I had no frame of reference for how long things
*should* have been taking, having had no real prior experience with PyTorch,
but I was getting annoyed enough at how long it seemed to take to load the
model---which I was doing frequently during development---so I added a bunch
of timing code around things to try and bisect where the overhead was being
introduced.

Looking at the `__init__()` code above, though, I was thoroughly perplexed
by the `_init_weights(self, module)` function.  Why would we be initializing
weights at the end of the `GPT` constructor if we were just about to
override them with the weights we were loading from the checkpoint?

Eliminating the call to `_init_weights()` entirely shaved off 15 seconds
from the time it took to simply create an *"empty"* `GPT()` instance---i.e.
before we'd even loaded the weights.  However, it was still taking 15
seconds just to execute this block of code:

```python

    self.transformer = nn.ModuleDict(
        dict(
            wte=nn.Embedding(config.vocab_size, config.n_embd),
            wpe=nn.Embedding(config.block_size, config.n_embd),
            h=nn.ModuleList(
                [Block(config) for _ in range(config.n_layer)]
            ),
            ln_f=nn.LayerNorm(config.n_embd),
        )
    )
    self.lm_head = nn.Linear(
        config.n_embd, config.vocab_size, bias=False
    )
```

I hoisted that code out into a separate `_init_transformer()` routine and
surrounded it with some optional `torch.profiler.profile()` glue:

```python

    timer = ElapsedTimer()

    with timer:
        if not self.torch_profile_activities:
            self._init_transformer()
        else:
            with torch.profiler.profile(
                activities=self.torch_profile_activities,
                with_stack=True,
            ) as prof:
                self._init_transformer()
            self.torch_profile_init_transformer = prof

    msg = f'Initialized GPT model in {timer.elapsed:.3f} seconds.'
    logging.info(msg)
```

The profiling data yielded some interesting insight:

```python
> print(model.torch_profile_init_transformer.key_averages().table(sort_by='cpu_time_total'))
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
           aten::uniform_        74.31%       10.861s        74.31%       10.861s     111.966ms            97
            aten::normal_        25.64%        3.747s        25.64%        3.747s        1.873s             2
             aten::detach         0.01%       1.408ms         0.03%       3.712ms      24.914us           149
              aten::empty         0.02%       2.570ms         0.02%       2.570ms      17.247us           149
                   detach         0.02%       2.304ms         0.02%       2.304ms      15.464us           149
              aten::fill_         0.01%       1.013ms         0.01%       1.013ms      40.500us            25
              aten::zero_         0.00%     305.888us         0.00%     305.888us      12.236us            25
    cudaDeviceSynchronize         0.00%      16.073us         0.00%      16.073us      16.073us             1
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 14.615s
```

So `aten::uniform_` and `aten::normal_` were taking up, *literally*, 99.95%
of the time during the transformer *initialization*.  That also seemed
bananas for the exact same reason calling `_init_weights()` seemed bananas:
why was so much time and effort being spent initializing distributions when
we were going to immediately overwrite all the weights when we load the
model from the checkpoint?

Now, granted, had I not been using a debug build of PyTorch, those 14.597
seconds spent on weight initialization would be more like unnoticeable
milliseconds at best.  But I didn't know that at the time, so after a bit of
digging, I found out I could subclass my `Embedding` and `Linear` layers
(which were the ones contributing to the uniform and normal distribution
setup time overhead) in such a way that weight initialization would be
skipped entirely.

So, if you look at the [`gpt2.py`](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py#L535)
implementation on Github, you'll see that I'm using a bunch of `NoInit`
classes, as follows:

```python
# ==============================================================
# Classes
# ==============================================================

# N.B. We have simple "no-init" overrides for nn.Embedding and
#      nn.Linear which skip the default initialization routines,
#      significantly reducing the time to load the model by
#      avoiding uniform and random distribution initialization.
#      As we immediately load all the weights from the model
#      checkpoint straight after creating the model, we don't
#      need the default initialization routines.

class NoInitEmbedding(nn.Embedding):
    def reset_parameters(self):
        # Skip default uniform initialization.
        pass


class NoInitLinear(nn.Linear):
    def reset_parameters(self):
        # Skip default Kaiming initialization.
        pass


class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # Key, query, value projections for all heads, but in a batch.
        self.c_attn = NoInitLinear(config.n_embd, 3 * config.n_embd)

        # Output projection.
        self.c_proj = NoInitLinear(config.n_embd, config.n_embd)
        self.c_proj.NANOGPT_SCALE_INIT = 1

        # Regularization.
        self.n_head = config.n_head
        self.n_embd = config.n_embd

    ...

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc = NoInitLinear(config.n_embd, 4 * config.n_embd)
        self.gelu = nn.GELU(approximate='tanh')
        self.c_proj = NoInitLinear(4 * config.n_embd, config.n_embd)
        self.c_proj.NANOGPT_SCALE_INIT = 1

    ...

class GPT:

    ...

    def _init_transformer(self):
        """
        Initialize the transformer.
        """
        config = self.config
        self.transformer = nn.ModuleDict(
            dict(
                wte=NoInitEmbedding(config.vocab_size, config.n_embd),
                wpe=NoInitEmbedding(config.block_size, config.n_embd),
                h=nn.ModuleList(
                    [Block(config) for _ in range(config.n_layer)]
                ),
                ln_f=nn.LayerNorm(config.n_embd),
            )
        )
        self.lm_head = NoInitLinear(
            config.n_embd,
            config.vocab_size,
            bias=False,
        )
        self.transformer.wte.weight = self.lm_head.weight

```


A few weeks after I'd put this code in, I came across this HuggingFace
article on [Big Model Inference](
https://huggingface.co/docs/accelerate/en/usage_guides/big_modeling) where
they discuss the very problem I was hitting, which is problematic on much
larger models even when you're using a release build of PyTorch---not just
pip-squeak GPT2 models with a debug build---and they have a
decorator-oriented solution:

```python
from accelerate import init_empty_weights
with init_empty_weights():
    my_model = ModelClass(...)
```

Granted, I couldn't use `accelerate` because it depends on packages that are
not available for free-threaded builds yet, but I wanted to include the
information here for future reference.

### Can We Load Other Checkpoints?

GPT2 was the last model where OpenAI made the weights publicly available.
So, in theory, I should be able to download their largest GPT2
model---[GPT2 XL](https://huggingface.co/openai-community/gpt2-xl)---figure
out how to extract the weights, and then load them into our janky little
`GPT` class instead of the ones from the locally-trained checkpoint.

Turns out downloading it is easy via `huggingface-cli` (again, not something
that works with free-threading, so you'll need to activate the `py313`
environment and `pip install -U "huggingface_hub[cli]"` per [these](
https://huggingface.co/docs/huggingface_hub/en/guides/cli) instructions):

```bash
% huggingface-cli download openai-community/gpt2-xl
# That should download the model into the following directory.
% cd ~/.cache/huggingface/hub/models--openai-community--gpt2-xl
% tree -h .
tree -h
[4.0K]  .
 [4.0K]  blobs
  [ 124]  057e43033439e068b325f32af95dde9efc9552ae
  [6.0G]  0f8b28eb05a8075f48b61b6f35332978c74fc7763fa9fb4051a1c30511736a6a
  [1018K]  1f1d9aaca301414e7f6c9396df506798ff4eb9a6
  [446K]  226b0752cac7789c48f0cb3ec53eda48b7be36cc
  [ 12K]  34f1cc31333e001c3047539dd604123dafd37346
  [5.8G]  47d4a280f9274f015e8dd1e7e2e1066415fc1f37030ec30d00615973e750e578
  [1.3M]  4b988bccc9dc5adacd403c00b4704976196548f8
  [ 445]  602b71f15d40ed68c5f96330e3f3175a76a32126
  [6.3G]  7b92a55d852494754e2856681833ee0ff05580ee32c1d8d60a1177bbf1f4703a
  [ 689]  932bdee9f4b3878f7f285187564b46f256243aff
  [  26]  be4d21d94f3b4687e5a54d84bf6ab46ed0f8defd
  [ 165]  c791c4d01fc484a57262ee57c878dd5b35863fe7
  [6.0G]  cd2a29e31040ef64d9362cb96801969c9f67b9e0bdbd6e00b9dda57cdbe17435
  [5.8G]  fbc167e52fa30c56fbed46e8b45c25893c4afb165ef666864abc0e70d5c117a4
 [4.0K]  refs
  [  40]  main
 [4.0K]  snapshots
     [4.0K]  15ea56dee5df4983c59b2538573817e1667135e2
         [  52]  config.json -> ../../blobs/932bdee9f4b3878f7f285187564b46f256243aff
         [  76]  flax_model.msgpack -> ../../blobs/fbc167e52fa30c56fbed46e8b45c25893c4afb165ef666864abc0e70d5c117a4
         [  52]  generation_config_for_text_generation.json -> ../../blobs/c791c4d01fc484a57262ee57c878dd5b35863fe7
         [  52]  generation_config.json -> ../../blobs/057e43033439e068b325f32af95dde9efc9552ae
         [  52]  merges.txt -> ../../blobs/226b0752cac7789c48f0cb3ec53eda48b7be36cc
         [  76]  model.safetensors -> ../../blobs/0f8b28eb05a8075f48b61b6f35332978c74fc7763fa9fb4051a1c30511736a6a
         [  76]  pytorch_model.bin -> ../../blobs/cd2a29e31040ef64d9362cb96801969c9f67b9e0bdbd6e00b9dda57cdbe17435
         [  52]  README.md -> ../../blobs/34f1cc31333e001c3047539dd604123dafd37346
         [  76]  rust_model.ot -> ../../blobs/7b92a55d852494754e2856681833ee0ff05580ee32c1d8d60a1177bbf1f4703a
         [  76]  tf_model.h5 -> ../../blobs/47d4a280f9274f015e8dd1e7e2e1066415fc1f37030ec30d00615973e750e578
         [  52]  tokenizer_config.json -> ../../blobs/be4d21d94f3b4687e5a54d84bf6ab46ed0f8defd
         [  52]  tokenizer.json -> ../../blobs/4b988bccc9dc5adacd403c00b4704976196548f8
         [  52]  vocab.json -> ../../blobs/1f1d9aaca301414e7f6c9396df506798ff4eb9a6
```

Well that's pretty neat, if not a tad duplicative, given the 1.5B parameter
model is shipped in five different ~6GB encodings:

```
  [6.0G]  model.safetensors   (0f8b28...)
  [5.8G]  tf_model.h5         (47d4a2...)
  [6.3G]  rust_model.ot       (7b92a5...)
  [6.0G]  pytorch_model.bin   (cd2a29...)
  [5.8G]  flax_model.msgpack  (fbc167...)
```

The `pytorch_model.bin` one is a pickled `dict` of tensors obtained from a
`torch.save()` call---i.e. basically equivalent to the checkpoint we'd been
using for the locally trained GPT2 model we used.  For whatever reason, that
one didn't interest me much, but `model.safetensors` did.

Thankfully, I could `pip install safetensors` in the free-threaded `py313t`
environment, so I wrote some [helper glue](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/util.py#L178)
to give me back a `HuggingFaceModel` with the tensors accessible via a
`safetensors` attribute if I passed it the appropriate model name, e.g.:

```python
@dataclass
class HuggingFaceModel:
    name: str
    config: dict
    safetensors: "safetensors.safe_open"
    tokenizer: dict
    tokenizer_config: dict
    vocab: dict

def get_huggingface_model(model_name: str) -> HuggingFaceModel:
    """
    Returns a Hugging Face model object for the given model name.

    Args:

        model_name (str): Supplies the name of the Hugging Face model.  This
            should be in the format of `namespace/model`, e.g. for GPT2 XL:
            `openai-community/gpt2-xl`.  This will be expanded out to the
            following directory:
                `~/.cache/huggingface/hub/models--openai-community--gpt2-xl`

    Returns:

        HuggingFaceModel: Returns a HuggingFaceModel object containing the
            model name, configuration, and SafeTensors object.
    """
    base = os.path.expanduser('~/.cache/huggingface/hub/models--')
    (namespace, model) = model_name.split('/')
    base_path = f'{base}{namespace}--{model}'
    ref_path = f'{base_path}/refs/main'
    with open(ref_path, 'r') as f:
        ref = f.read().strip()
    snapshots_dir = f'{base_path}/snapshots/{ref}'
    safetensors_path = f'{snapshots_dir}/model.safetensors'
    import safetensors
    timer = ElapsedTimer()
    logging.debug(f'About to load safetensors from {safetensors_path}...')
    with timer:
        st = safetensors.safe_open(
            safetensors_path,
            framework="pt",
            device="cpu",
        )
    msg = (
        f'Loaded safetensors from {safetensors_path} '
        f'in {timer.elapsed:.4f} seconds.'
    )
    logging.info(msg)

    config_path = f'{snapshots_dir}/config.json'
    with open(config_path, 'r') as f:
        config = json.load(f)

    tokenizer_path = f'{snapshots_dir}/tokenizer.json'
    with open(tokenizer_path, 'r') as f:
        tokenizer = json.load(f)

    tokenizer_config_path = f'{snapshots_dir}/tokenizer_config.json'
    with open(tokenizer_config_path, 'r') as f:
        tokenizer_config = json.load(f)

    vocab_path = f'{snapshots_dir}/vocab.json'
    with open(vocab_path, 'r') as f:
        vocab = json.load(f)

    return HuggingFaceModel(
        model_name,
        config,
        st,
        tokenizer,
        tokenizer_config,
        vocab,
    )
```

Now, I remember Andrej had a [`GPT.from_pretrained()`](
https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py#L130)
routine that was geared toward loading the GPT2 models via the
`transformers` Python package along the following lines:

```python
from transformers import GPT2LMHeadModel
hf_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')
```

The technique he used to prime is local `GPT` class from the larger model
loaded from HuggingFace piqued my interest.  I've reproduced the applicable
code below, with some formatting tweaks only.

```python
# create a from-scratch initialized minGPT model
config = GPTConfig(**config_args)
model = GPT(config)
sd = model.state_dict()
sd_keys = sd.keys()
# discard this mask / buffer, not a param
sd_keys = [
    key for key in sd_keys if not key.endswith('.attn.bias')
]

# init a huggingface/transformers model
model_hf = GPT2LMHeadModel.from_pretrained(model_type)
sd_hf = model_hf.state_dict()

# copy while ensuring all of the parameters are aligned
# and match in names and shapes
sd_keys_hf = sd_hf.keys()

# ignore `.attn.masked_bias`; just a buffer
sd_keys_hf = [
    key for key in sd_keys_hf
        if not k.endswith('.attn.masked_bias')
]

# ditto; ignore `.attn.bias`, just the mask (buffer)
sd_keys_hf = [
    key for key in sd_keys_hf
        if not key.endswith('.attn.bias')
]

transposed = [
    'attn.c_attn.weight',
    'attn.c_proj.weight',
    'mlp.c_fc.weight',
    'mlp.c_proj.weight',
]

# basically the openai checkpoints use a "Conv1D" module,
# but we only want to use a vanilla Linear, this means
# that we have to transpose these weights when we import them
assert len(sd_keys_hf) == len(sd_keys), (
    f"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}"
)

for k in sd_keys_hf:
    if any(k.endswith(w) for w in transposed):
        # special treatment for the Conv1D weights
        # we need to transpose
        assert sd_hf[k].shape[::-1] == sd[k].shape
        with torch.no_grad():
            sd[k].copy_(sd_hf[k].t())
    else:
        # vanilla copy over the other parameters
        assert sd_hf[k].shape == sd[k].shape
        with torch.no_grad():
            sd[k].copy_(sd_hf[k])
```

Do all of that fiddling and *et voil*, you've just primed your `GPT` model
with the OpenAI weights!

The reason Andrej's approach piqued my interest---in combination with poking
around at a `safetensors` instance and realizing I could easily extract all
147-or-so tensors by name---was that it may allow us to copy the tensors
from the model read from disk into *our* model in parallel, using multiple
threads.

So I hacked together a variant of Andrej's `GPT.from_pretrained()` that
looked as follows:

```python
class GPT:

    ...

    @classmethod
    def from_pretrained(cls,
                        model_name: str,
                        map_location: Optional[str] = None,
                        manual_seed: Optional[int] = None,
                        torch_profile_activities: Optional[List[type]] = None,
                        ) -> "GPT":
        """
        Load a GPT model from a pretrained model.

        Arguments:

            model_name (str): Supplies the model name to use.  See the
                docstring for `.util.get_huggingface_safetensors()` for
                more information about the format.

            map_location (str): Optionally supplies the device to map the
                loaded tensor parameters to.  If None, "cuda" will be used
                if available, otherwise "cpu".

            manual_seed (int): Optionally supplies the manual seed to use for
                the model.  If None, a random seed will be used.

            torch_profile_activities (list): Optionally supplies a list of
                torch.profiler.ProfilerActivity to profile.

        """
        if manual_seed is None:
            # Use a random seed.
            manual_seed = random.randint(0, 2**32 - 1)

        if map_location is None:
            if torch.cuda.is_available():
                map_location = "cuda"
            else:
                map_location = "cpu"

        timer = ElapsedTimer()
        with timer:
            hf_model = get_huggingface_model(model_name)
        msg = (
            f'Loaded HuggingFace model {model_name} in '
            f'{timer.elapsed:.3f} seconds.'
        )
        logging.info(msg)

        config = GPTConfig(**{
            'block_size': hf_model.config['n_ctx'],
            'vocab_size': hf_model.config['vocab_size'],
            'n_layer': hf_model.config['n_layer'],
            'n_head': hf_model.config['n_head'],
            'n_embd': hf_model.config['n_embd'],
        })
        checkpoint = GPTCheckpoint(**{
            'model': None,
            'step': 0,
            'val_loss': 0.0,
            'config': config,
        })

        with timer:
            model = cls(
                checkpoint=checkpoint,
                device=map_location,
                manual_seed=manual_seed,
                torch_profile_activities=torch_profile_activities,
            )
        logging.info(f'Created GPT model in {timer.elapsed:.3f} seconds.')

        # This logic is based heavily off build-nanogpt's `train_gpt2.py`;
        # specifically: GPT.from_pretrained().

        exclude = ('.attn.bias', '.attn.masked_bias', 'lm_head.weight')
        transpose = (
            'attn.c_attn.weight',
            'attn.c_proj.weight',
            'mlp.c_fc.weight',
            'mlp.c_proj.weight',
        )

        # Identify the HuggingFace keys we're interested in.
        st = hf_model.safetensors

        # Identify our model keys we're interested in.
        sd = model.state_dict()
        sd_keys = [k for k in sd.keys() if not k.endswith(exclude)]
        hf_keys = [k.replace('transformer.', '') for k in sd_keys]

        # Copying tensors in parallel yields decent speedups,
        # at least on my V100s which have five concurrent copy
        # engines.
        def copy_tensor(hf_key, sd_key):                            # <1>
            hf_tensor = st.get_tensor(hf_key)                       # <1>
            if hf_key.endswith(transpose):                          # <1>
                assert hf_tensor.shape[::-1] == sd[sd_key].shape    # <1>
                with torch.no_grad():                               # <1>
                    sd[sd_key].copy_(hf_tensor.t())                 # <1>
            else:                                                   # <1>
                assert hf_tensor.shape == sd[sd_key].shape          # <1>
                with torch.no_grad():                               # <1>
                    sd[sd_key].copy_(hf_tensor)                     # <1>

        keys = zip(hf_keys, sd_keys)
        max_workers = min(os.cpu_count(), len(sd_keys))
        with timer:
            with ThreadPoolExecutor(                                # <2>
                max_workers=max_workers                             # <2>
            ) as executor:                                          # <2>
                futures = {                                         # <2>
                    executor.submit(copy_tensor, hf_key, sd_key):   # <2>
                        (hf_key, sd_key)                            # <2>
                            for (hf_key, sd_key) in keys            # <2>
                }                                                   # <2>
                for future in as_completed(futures):                # <2>
                    future.result()                                 # <2>
        logging.info(
            f'Copied weights with {max_workers} thread(s) '
            f'in {timer.elapsed:.3f} seconds.'
        )

        device = map_location
        with timer:
            model.to(device)
        msg = f'Moved model to {device} in {timer.elapsed:.3f} seconds.'
        logging.info(msg)

        return model
```
1. Define a `copy_tensor` function that is provided with the name of a
   tensor key as it appears in the HuggingFace model (`hf_key`) and as
   it appears in our `state_dict` model (`sd_key`).  The `sd` state dict
   is accessible to all threads, so they can simply copy their tensor
   via `sd[sd_key].copy_(hf_tensor)`.

2. Use a `ThreadPoolExecutor()` to dispatch the `copy_tensor` operations in
   parallel.  In this case I'm using workers equivalent to the number of CPU
   cores, *or* number of tensors, whichever is fewer.

So that all worked pretty well, free-threading can absolutely be used to
speed up things like loading tensors.

Additionally, if you play around with this locally, the `gpt2-xl` model will
be available to select from the drop-down in the UI and also can be used via
the command line (`--model gpt2-xl`) or in the REST `/generate` endpoint as
a query parameter (`/generate/foo...?model=gpt2-xl`).

### Multi-GPU Support

The final big change I introduced was multi-GPU support plus some very
rudimentary round-robin-esque behavior that could be used from the
`generate()` routines to obtain a reference to a model depending on the
incoming user's request.

The `generate()` routine now obtains a model by the following:

```python
model = get_next_model()
```

The implementation for which is [here](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py#L103),
reproduced in part below:

```python
NUM_GPUS = torch.cuda.device_count()
# Add a CPU version at the end.
TOTAL_MODELS = NUM_GPUS + 1
MODELS = [None] * TOTAL_MODELS
MODELS_ROUND_ROBIN = itertools.cycle(range(TOTAL_MODELS))

def get_next_model_random():
    # Randomly select a GPU to use.
    return MODELS[random.randint(0, TOTAL_MODELS - 1)]

def get_next_model_round_robin():
    with MODELS_LOCK:
        index = next(MODELS_ROUND_ROBIN)
    return MODELS[index]

get_next_model = get_next_model_round_robin

def load_models():
    global MODELS
    max_workers = min(TOTAL_MODELS, os.cpu_count())
    timer = ElapsedTimer()
    with timer:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    GPT.from_local_pretrained,
                    model_path=MODEL_CHECKPOINT,
                    map_location=f'cuda:{i}',
                    torch_profile_activities=TORCH_PROFILE_ACTIVITIES,
                ): i for i in range(NUM_GPUS)
            }
            # Add the CPU model.
            futures[executor.submit(
                GPT.from_local_pretrained,
                model_path=MODEL_CHECKPOINT,
                map_location='cpu',
                torch_profile_activities=TORCH_PROFILE_ACTIVITIES,
            )] = NUM_GPUS
            for future in as_completed(futures):
                i = futures[future]
                model = future.result()
                MODELS[i] = model
    msg = (
        f'Loaded model on {NUM_GPUS} GPU(s) and 1 CPU in '
        f'{timer.elapsed:.3f} seconds.'
    )
    logging.info(msg)

PRETRAINED_MODELS = [None] * TOTAL_MODELS
PRETRAINED_MODELS_ROUND_ROBIN = itertools.cycle(range(TOTAL_MODELS))

def get_next_pretrained_model_random():
    # Randomly select a GPU to use.
    return PRETRAINED_MODELS[random.randint(0, TOTAL_MODELS - 1)]

def get_next_pretrained_model_round_robin():
    with PRETRAINED_MODELS_LOCK:
        index = next(PRETRAINED_MODELS_ROUND_ROBIN)
    return PRETRAINED_MODELS[index]

get_next_pretrained_model = get_next_pretrained_model_round_robin

def load_pretrained_models():
    global PRETRAINED_MODELS
    max_workers = min(TOTAL_MODELS, os.cpu_count())
    timer = ElapsedTimer()
    with timer:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    GPT.from_pretrained,
                    model_name='openai-community/gpt2-xl',
                    map_location=f'cuda:{i}',
                    torch_profile_activities=TORCH_PROFILE_ACTIVITIES,
                ): i for i in range(NUM_GPUS)
            }
            # Add the CPU model.
            futures[executor.submit(
                GPT.from_pretrained,
                model_name='openai-community/gpt2-xl',
                map_location='cpu',
                torch_profile_activities=TORCH_PROFILE_ACTIVITIES,
            )] = NUM_GPUS
            for future in as_completed(futures):
                i = futures[future]
                model = future.result()
                PRETRAINED_MODELS[i] = model
    msg = (
        f'Loaded gpt2-xl model on {NUM_GPUS} GPU(s) and 1 CPU in '
        f'{timer.elapsed:.3f} seconds.'
    )
    logging.info(msg)
```

## Model Optimization

Two facilities are available to speed up your model in PyTorch:
[TorchScript](https://pytorch.org/docs/stable/jit.html), and [Torch Dynamo](
https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html), with
the latter being a newer approach than the former.

In our [parallelopedia.gpt2](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)
module, our `Gpt2App` class has a `classmethod` named `init_once()`, which,
as you can probably guess, gets called once by the HTTP server when starting
up.  This is where you stash expensive setup code like loading and compiling
models.

The code looks similar to the following.  We have two globals,
`TORCH_JIT_COMPILE` and `TORCH_DYNAMO_COMPILE` that, when set, will attempt
to optimize the models using the selected method.

```python

class Gpt2App(HttpApp):

    ...

    @classmethod
    def init_once(cls):
        load_models()
        load_pretrained_models()

        global MODELS, PRETRAINED_MODELS

        # This doesn't work because torch.jit doesn't handle our
        # async generator.
        global TRY_JIT_COMPILE
        if TRY_JIT_COMPILE:
            for (i, model) in enumerate(MODELS):
                model.config = dataclasses.asdict(model.config)
                timer = ElapsedTimer()
                with timer:
                    model = torch.jit.script(model)
                    MODELS[i] = model
                logging.info(
                    f'JIT compiled model {i} in {timer.elapsed:.3f} seconds.'
                )

        global TRY_TORCH_COMPILE
        if TRY_TORCH_COMPILE:
            for (i, model) in enumerate(MODELS):
                model.config = dataclasses.asdict(model.config)
                timer = ElapsedTimer()
                with timer:
                    model = torch.compile(model)
                    MODELS[i] = model
                logging.info(
                    f'torch.compiled model {i} in '
                    f'{timer.elapsed:.3f} seconds.'
                )

            for (i, model) in enumerate(PRETRAINED_MODELS):
                model.config = dataclasses.asdict(model.config)
                timer = ElapsedTimer()
                with timer:
                    model = torch.compile(model)
                    PRETRAINED_MODELS[i] = model
                logging.info(
                    f'torch.compiled pretrained model {i} in '
                    f'{timer.elapsed:.3f} seconds.'
                )
```
### TorchScript

TorchScript doesn't work at all for our model---it balks on the `async def
generate_async_for()` routine that is the workhorse of our asynchronous
token-by-token generation.

And that ended my TorchScript experiment :-)

### Torch Dynamo (torch.compile)

Torch Dynamo is a new feature that was introduced by PyTorch 2.0 that hooks
into the Python interpreter and traces model execution and then builds
optimized kernels based on the information observed during runtime tracing.

When it works, it works really well, and you can get significant speedups
both in training and inference with literally a single line:

```python
model = torch.compile(model)
```

The first problem we hit with Dynamo is that it's explicitly not supported
by PyTorch on free-threaded builds:

```python
>>> model = torch.compile(model)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[3], line 1
----> 1 model = torch.compile(model)

File ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/__init__.py:2526, in compile(model, fullgraph, dynamic, backend, mode, options, disable)
   2524     raise RuntimeError("torch.compile is not supported on Python 3.14+")
   2525 elif sysconfig.get_config_var("Py_GIL_DISABLED") == 1:
-> 2526     raise RuntimeError(
   2527         "torch.compile is not supported on Python built with GIL disabled"
   2528     )
   2530 # Decorator mode
   2531 if model is None:

RuntimeError: torch.compile is not supported on Python built with GIL disabled
```

Let's hack that `torch/__init__.py` file as follows and try again.

```diff
--- __init__.py.orig    2025-02-09 13:28:27.892979258 -0800
+++ __init__.py 2025-02-09 13:30:13.879909529 -0800
@@ -2523,9 +2523,7 @@
     if sys.version_info >= (3, 14):
         raise RuntimeError("torch.compile is not supported on Python 3.14+")
     elif sysconfig.get_config_var("Py_GIL_DISABLED") == 1:
-        raise RuntimeError(
-            "torch.compile is not supported on Python built with GIL disabled"
-        )
+        print("torch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported")

     # Decorator mode
     if model is None:
```

```python
>>> model = torch.compile(model)
torch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[3], line 1
----> 1 model = torch.compile(model)

File ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/__init__.py:2563, in compile(model, fullgraph, dynamic, backend, mode, options, disable)
   2560 else:
   2561     backend = _TorchCompileWrapper(backend, mode, options, dynamic)
-> 2563 return torch._dynamo.optimize(
   2564     backend=backend,
   2565     nopython=fullgraph,
   2566     dynamic=dynamic,
   2567     disable=disable,
   2568 )(model)

File ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/_dynamo/eval_frame.py:842, in optimize(*args, **kwargs)
    839         kwargs["nopython"] = ca_kwargs_override["fullgraph"]
    840     return optimize(*args, **kwargs)
--> 842 return _optimize(rebuild_ctx, *args, **kwargs)

File ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/_dynamo/eval_frame.py:881, in _optimize(rebuild_ctx, backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)
    845 def _optimize(
    846     rebuild_ctx: Callable[[], Union[OptimizeContext, _NullDecorator]],
    847     backend="inductor",
   (...)
    853     dynamic=None,
    854 ) -> Union[OptimizeContext, _NullDecorator]:
    855     """
    856     The main entrypoint of TorchDynamo.  Do graph capture and call
    857     backend() to optimize extracted graphs.
   (...)
    879             ...
    880     """
--> 881     check_if_dynamo_supported()
    882     # Note: The hooks object could be global instead of passed around, *however* that would make
    883     # for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.
    884     # There is some prior art around this, w/r/t nesting backend calls are enforced to be the same
    885     # compiler, however, this feels onerous for callback and hooks, and it feels better to give our users an
    886     # easier to understand UX at the cost of a little more plumbing on our end.
    887     hooks = Hooks(guard_export_fn=guard_export_fn, guard_fail_fn=guard_fail_fn)

File ~/mambaforge/envs/py313t/lib/python3.13t/site-packages/torch/_dynamo/eval_frame.py:805, in check_if_dynamo_supported()
    803     raise RuntimeError("Python 3.14+ not yet supported for torch.compile")
    804 elif sysconfig.get_config_var("Py_GIL_DISABLED") == 1:
--> 805     raise RuntimeError(
    806         "torch.compile is not supported on Python built with GIL disabled"
    807     )

RuntimeError: torch.compile is not supported on Python built with GIL disabled
```

Let's hack `torch/_dynamo/eval_frame.py` too:

```diff
--- eval_frame.py.orig  2025-02-09 13:32:18.266470283 -0800
+++ eval_frame.py       2025-02-09 13:32:32.746291774 -0800
@@ -802,9 +802,7 @@
     if sys.version_info >= (3, 14):
         raise RuntimeError("Python 3.14+ not yet supported for torch.compile")
     elif sysconfig.get_config_var("Py_GIL_DISABLED") == 1:
-        raise RuntimeError(
-            "torch.compile is not supported on Python built with GIL disabled"
-        )
+        print("torch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported")


 def is_dynamo_supported():
```

Now let's try again:

```python
>>> model = torch.compile(model)
torch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported
torch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported
>>>
```

So, we can forcibly coerce PyTorch Dynamo to compile our model even if we're
in free-threaded mode.

But does it work?  And is it faster?  Let's investigate.

We can invoke our [`parallelopedia.gpt2`](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)
module directly with various command line arguments to test out generation
performance.  The accompanying `--help` is furnished below for reference:

```bash
% python -m parallelopedia.gpt2 --help
torch.__init__: Ignoring unsupported torch.compile() with no GIL unsupported
torch._dynamo.eval_frame: Ignoring unsupported torch.compile() with no GIL unsupported
usage: gpt2.py [-h] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--model {gpt2,gpt2-xl}] [--device DEVICE]
               [--max-length MAX_LENGTH] [--top-k TOP_K] [--seed SEED] [--prompt PROMPT] [--torch-compile] [--torch-jit]
               [--torch-compile-fullgraph] [--torch-compile-reduce-overhead] [--torch-compile-max-autotune]
               [--generate-slim] [--rounds ROUNDS] [--wrap WRAP] [--note NOTE]

Run the GPT2 module.

options:
  -h, --help            show this help message and exit
  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}
                        Set the logging level.
  --model {gpt2,gpt2-xl}
                        Select the model to use.
  --device DEVICE       Select the device to use.
  --max-length MAX_LENGTH
                        Set the maximum length of the generated text.
  --top-k TOP_K         Set the top-k value for sampling.
  --seed SEED           Set the random seed for generation.
  --prompt PROMPT       Set the prompt for generation.
  --torch-compile       Compile the models using torch.compile().
  --torch-jit           Compile the models using torch.jit.script().
  --torch-compile-fullgraph
                        Compile the models using torch.compile() with fullgraph=True.
  --torch-compile-reduce-overhead
                        Compile the models using torch.compile() with mode="reduce-overhead"
  --torch-compile-max-autotune
                        Compile the models using torch.compile() with mode="max_autotune".
  --generate-slim       Use the generate_slim() method for generation.
  --rounds ROUNDS       Set the number of rounds for generation.
  --wrap WRAP           Set the wrap width for text output.
  --note NOTE           Set a note to include in the JSON output.
```

The full source code for the module's `main()` function follows.  Based on
the command line parameters furnished, we can test various generation
variants, such as no `torch.compile()`, `torch.compile()` and nothing else,
and more advanced permutations such as the following:

```python
kwds = {
    'fullgraph': True,
    'model': 'max-autotune',
}
model = torch.compile(model, **kwds)
```

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Full Source Code for parallelopedia.gpt2.main()"}

```python
def main():
    """
    Main entry point for the parallelopedia.gpt2 module.
    """
    args = parse_arguments()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

    start_time = time.time()
    start_timestamp = datetime.datetime.now().isoformat()

    timer = ElapsedTimer()
    with timer:
        if args.model == 'gpt2-xl':
            model = GPT.from_pretrained(
                model_name='openai-community/gpt2-xl',
                map_location=args.device,
            )
        else:
            model = GPT.from_local_pretrained(
                model_path=MODEL_CHECKPOINT,
                map_location=args.device,
                manual_seed=args.seed,
            )
    logging.info(
        f'Loaded {args.model} on {args.device} '
        f'in {timer.elapsed:.3f} seconds.'
    )

    if args.torch_compile:
        if args.torch_jit:
            msg = 'Cannot specify both --torch-compile and --torch-jit.'
            raise ValueError(msg)
        model.config = dataclasses.asdict(model.config)
        kwds = {}
        if args.torch_compile_fullgraph:
            kwds['fullgraph'] = True
        if args.torch_compile_reduce_overhead:
            if args.torch_compile_max_autotune:
                msg = (
                    'Cannot specify both --torch-compile-reduce-overhead '
                    'and --torch-compile-max-autotune.'
                )
                raise ValueError(msg)
            kwds['mode'] = 'reduce-overhead'
        elif args.torch_compile_max_autotune:
            kwds['mode'] = 'max-autotune'
        with timer:
            model = torch.compile(model, **kwds)
        msg = f'torch.compiled model in {timer.elapsed:.3f} seconds.'
        logging.info(msg)
    elif args.torch_jit:
        model.config = dataclasses.asdict(model.config)
        with timer:
            model = torch.jit.script(model)
        msg = f'JIT compiled model in {timer.elapsed:.3f} seconds.')
        logging.info(msg)

    seed = args.seed
    if seed is None or seed == '':
        seed = random.randint(0, 2**32 - 1)

    if args.generate_slim:
        text_tokens = model.enc.encode(args.prompt)
        prompt_token_length = len(text_tokens)

    rates = []
    for i in range(args.rounds):
        logging.info(f'Round {i + 1} of {args.rounds}.')
        if args.generate_slim:
            with timer:
                x = model.generate_slim(
                    text_tokens,
                    max_length=args.max_length,
                    top_k=args.top_k,
                    seed=seed,
                )
            elapsed = timer.elapsed
            count = x.size(1) - prompt_token_length
            tokens_per_sec = count / elapsed
            rates.append(tokens_per_sec)
            logging.info(
                f'Generated {count} tokens in {elapsed:.2f} seconds '
                f'({tokens_per_sec:.2f} tokens/sec)'
            )
            output = model.enc.decode(x[0].tolist())
        else:
            save_rate = lambda x: rates.append(x)
            output = model.generate(
                args.prompt,
                max_length=args.max_length,
                top_k=args.top_k,
                seed=seed,
                save_rate=save_rate,
            )

        if args.wrap:
            output = textwrap.fill(output, width=args.wrap)
        logging.info(f'Output:\n{output}')

    # The filename is of the form:
    #   `gpt2-rates-<yyyy-mm-dd-hh-ss-mm.sss>-[optional].json`
    now = datetime.datetime.now()
    timestamp = now.strftime('%Y-%m-%d-%H-%M-%S-%f')
    filename = f"gpt2-rates-{timestamp}"
    if args.torch_compile:
        filename += '-torch-compile'
        if args.torch_compile_reduce_overhead:
            filename += '-reduce-overhead'
        elif args.torch_compile_max_autotune:
            filename += '-max-autotune'
        if args.torch_compile_fullgraph:
            filename += '-fullgraph'
    if args.generate_slim:
        filename += '-generate-slim'

    conda_env_name = os.environ.get('CONDA_DEFAULT_ENV', 'Unknown')
    filename += f'-{conda_env_name}'

    filename += '.json'

    if not isinstance(model.config, dict):
        model_config = dataclasses.asdict(model.config)
    else:
        model_config = model.config

    end_time = time.time()
    end_timestamp = datetime.datetime.now().isoformat()

    if args.device.startswith('cuda'):
        ix = args.device.find(':')
        if ix == -1:
            device_index = 0
        else:
            device_index = int(args.device[ix+1:])

        device_name = torch.cuda.get_device_name(device_index)
    else:
        device_name = 'CPU'

    try:
        is_gil_enabled = sys._is_gil_enabled()
    except AttributeError:
        is_gil_enabled = False

    # Prepare a dictionary with the details to save.
    run_details = {
        "rates": rates,
        "model_config": model_config,
        "args": vars(args),
        "start_timestamp": start_timestamp,
        "end_timestamp": end_timestamp,
        "elapsed": f'{end_time - start_time:.3f}',
        "device_name": device_name,
        "conda_env_name": conda_env_name,
        "is_gil_enabled": is_gil_enabled,
        "note": args.note,
    }

    # Write the JSON file.
    with open(filename, "w") as json_file:
        json.dump(run_details, json_file, indent=4)

    logging.info(f"Run details saved to {filename}.")

```
:::

Let's take a look at whether or not `torch.compile()` improves performance
in our `py313t` free-threaded environment, first.

#### Performance Comparison

I wrote a `bash` script [`run-py313t-gpt2-compile-tests.sh`](
https://github.com/tpn/website/blob/main/articles/pytorch-and-python-free-threading/run-py313t-gpt2-compile-tests.sh),
reproduced below, that ran various permutations of generation with different
`torch.compile()` options.

```bash
#!/bin/bash

# Ensure our environment name is `py313t`.
if [ "$CONDA_DEFAULT_ENV" != "py313t" ]; then
  echo "Error: Conda environment is not 'py313t'."
  exit 1
fi

# Ensure PARALLELOPEDIA_ROOT is set.
if [ -z "$PARALLELOPEDIA_ROOT" ]; then
  echo "Error: PARALLELOPEDIA_ROOT is not set."
  exit 1
fi


SEED=42
DEVICE="cuda:3"
ROUNDS=20

OPTIONS=(
  "--torch-compile"
  "--torch-compile --torch-compile-fullgraph"
  "--torch-compile --torch-compile-reduce-overhead"
  "--torch-compile --torch-compile-reduce-overhead --torch-compile-fullgraph"
  "--torch-compile --torch-compile-reduce-overhead --torch-compile-fullgraph"
  "--torch-compile --torch-compile-max-autotune"
  "--torch-compile --torch-compile-max-autotune --torch-compile-fullgraph"
)

echo "GPT.generate() variants"
time python -Xgil=0 -m parallelopedia.gpt2    \
    --seed=$SEED                              \
    --rounds=$ROUNDS                          \
    --device=$DEVICE

for opt in "${OPTIONS[@]}"; do
    # Split opt into separate arguments.
    eval set -- $opt
    time python -Xgil=0 -m parallelopedia.gpt2    \
        --seed=$SEED                              \
        --rounds=$ROUNDS                          \
        --device=$DEVICE                          \
        "$@"
done
```

The full log file for the run is captured in the callout below.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="run-py313t-gpt2-compile-tests.log"}

```bash
{{< include "run-py313t-gpt2-compile-tests.log" >}}
```
:::

After each run, a JSON file is saved capturing details about the run.  These
can all be found in this [`json`](
https://github.com/tpn/website/tree/main/articles/pytorch-and-python-free-threading/json)
directory.

Using [`gpt2-rates-2025-02-09-18-19-29-116321-py313t.json`](
https://github.com/tpn/website/blob/main/articles/pytorch-and-python-free-threading/json/gpt2-rates-2025-02-09-18-19-29-116321-py313t.json)
as an example:

```json
{
    "rates": [
        103.45683188097036,
        133.69021647689885,
        134.64711111769682,
        134.99572668445947,
        135.07252279473053,
        134.93995013027606,
        134.8097791019913,
        134.97874539755563,
        134.96994164914173,
        134.2781961646748,
        133.36089184927351,
        134.91936892711604,
        134.99927221213662,
        134.99574871578727,
        134.84820822748674,
        134.90753647890656,
        134.84542991060954,
        134.76236728440452,
        134.939303436755,
        134.95605543169106
    ],
    "model_config": {
        "block_size": 1024,
        "vocab_size": 50304,
        "n_layer": 12,
        "n_head": 12,
        "n_embd": 768
    },
    "args": {
        "log_level": "INFO",
        "model": "gpt2",
        "device": "cuda:3",
        "max_length": 100,
        "top_k": 50,
        "seed": 42,
        "prompt": "Einstein's Theory of Relativity states that",
        "torch_compile": false,
        "torch_jit": false,
        "torch_compile_fullgraph": false,
        "torch_compile_reduce_overhead": false,
        "torch_compile_max_autotune": false,
        "generate_slim": false,
        "rounds": 20,
        "wrap": 60,
        "note": ""
    },
    "start_timestamp": "2025-02-09T18:19:14.156224",
    "end_timestamp": "2025-02-09T18:19:29.116383",
    "elapsed": "14.960",
    "device_name": "Tesla V100-DGXS-32GB",
    "conda_env_name": "py313t",
    "is_gil_enabled": false,
    "note": ""
}
```

All tests were done for 20 rounds, and the `rates` key contains an array of
floats, representing the tokens/sec generation rate achieved by the call to
`generate()` (or `generate_slim()`, which we'll discuss shortly) after
optionally compiling the model with the requested parameters.

#### Free-Threaded Python (py313t)

I ended up doing multiple runs because... well, as you're about to see from
the data visualization below... it was a pretty noisy test.  Loads of
variance and generally everything was all over the shop.  The total run
times for each different permutation were all about the same, around 14
seconds or so, but definitely no clear winner regarding whether or not
`torch.compile()` or any particular permutation was having a repeatable
speedup.  Interesting.

Line plots and box-plots follow.  The box plots omitted the first *"warmup"*
run (which was always slower and skewed the data unnecessarily).  If you
click on the image you should get a nice *gallery* presentation mode that
allows you to flip between images nicely.

::: {.theme-light}
![Generation Performance - Run 1 (py313t)](py313t-run1-combined-side-by-side-light.svg){.lightbox group="gal1"}
![Generation Performance - Run 2 (py313t)](py313t-run2-combined-side-by-side-light.svg){.lightbox group="gal1"}
![Generation Performance - Run 3 (py313t)](py313t-run3-combined-side-by-side-light.svg){.lightbox group="gal1"}
![Generation Performance - Run 4 (py313t)](py313t-run4-combined-side-by-side-light.svg){.lightbox group="gal1"}
:::

::: {.theme-dark}
![Generation Performance - Run 1 (py313t)](py313t-run1-combined-side-by-side-dark.svg){.lightbox group="gal2"}
![Generation Performance - Run 2 (py313t)](py313t-run2-combined-side-by-side-dark.svg){.lightbox group="gal2"}
![Generation Performance - Run 3 (py313t)](py313t-run3-combined-side-by-side-dark.svg){.lightbox group="gal2"}
![Generation Performance - Run 4 (py313t)](py313t-run4-combined-side-by-side-dark.svg){.lightbox group="gal2"}
:::

#### Normal Python (py313)

Normal Python (not just free-threaded Python with the GIL enabled, but
full-blown old-school normal Python with no knowledge of GIL removal---i.e.
our `py313` environment) was pretty similar:

::: {.theme-light}
![Generation Performance (py313)](py313-run1-combined-side-by-side-light.svg){.lightbox group="gal3"}
:::

::: {.theme-dark}
![Generation Performance (py313)](py313-run1-combined-side-by-side-dark.svg){.lightbox group="gal4"}
:::

#### Explicit @torch.compile Decorator

Here's where it gets interesting.  In a final, last-ditch effort to see if I
could see *any* sort of speedup from `torch.compile()`, I introduced a
slimmer `generate()` routine, aptly named `generate_slim()`, which was
stripped of any superfluous code that would have otherwise impeded Torch
Dynamo's ability to optimize the graph.  That function looked like this:

```python
class GPT:
    ...

    # @torch.compile
    def generate_slim(
        self, text_tokens: torch.Tensor, max_length: int = 1024,
        top_k: int = 50, seed: int = None,
    ) -> str:
        """
        Generate text from the model.  This version differs from
        `generate()` in that it does not use any Python code that
        causes a torch graph break.

        Args:

            text (str): Supplies the prompt.

            max_length (int): Supplies the maximum total length,
                including prompt.

            top_k (int): Supplies the number of tokens to consider
                at each generation step.

            seed (int): Ignored!

        Returns:

            str: The generated text (including the initial prompt).
        """
        # Initialize alias.
        device = self.device
        stop_token = self.stop_token

        # Create the tensor for capturing predicted tokens.
        x = torch.tensor(
            text_tokens,
            dtype=torch.long,
            device=device
        ).unsqueeze(0)

        # Create a random generator for reproducibility.
        # sample_rng = torch.Generator(device=device)
        # if seed is None:
        #     seed = self.manual_seed
        # sample_rng.manual_seed(seed)

        # Generate tokens up to our max length.
        for _ in range(max_length):
            with torch.no_grad():
                # Forward pass, ignoring the returned loss.
                (logits, _) = self(x)

            # Take the logits at the last time-step (shape:
            # (1, vocab_size)).
            logits = logits[:, -1, :]

            # Convert to probabilities.
            probs = F.softmax(logits, dim=-1)

            # Top-k sampling.
            topk_probs, topk_indices = torch.topk(
                probs, k=top_k, dim=-1,
            )

            # Sample the next token.
            next_idx = torch.multinomial(
                topk_probs,
                num_samples=1,
                # generator=sample_rng,
            )
            next_token = torch.gather(topk_indices, -1, next_idx)

            # If the next token is the stop token, we're done.
            # next_token_item = next_token.item()
            # if next_token_item == stop_token:
            #    break

            # Append token to current sequence.
            x = torch.cat((x, next_token), dim=1)

        return x
```

Note that we had to make a number of sizable modifications.  No more
random number generator---that was causing graph breaks.  No more
explicitly checking for the stop token, again, that makes the dynamic
optimizer's job much harder at runtime without extra tracing overhead
for tracking scalars.  So we now generate tokens up to the maximum
specified, ignorant of any stop tokens, and return that.

I wanted to do two runs here, one where we call everything as normal
with all the different `torch.compile()` invocations we'd used in
prior runs, and then a second one where I explicitly mark the
`generate_slim()` routine with a `@torch.compile` decorator.

The latter absolutely does not work in free-threaded Python, it
segfaults after about ten seconds or so, thus, I couldn't test it.

However, on the normal Python version... we finally saw some
interesting results.

First, let's look at our baseline: normal `generate_slim()` with no
`@torch.compile` generator (the bash scripts verify that the decorator
is uncommented and commented as necessary):

::: {.theme-light}
![Generation Performance - generate_slim() - No @torch.compile Decorator (py313)](py313-run2-generate_slim-combined-side-by-side-light.svg){.lightbox}
:::

::: {.theme-dark}
![Generation Performance - generate_slim() - No @torch.compile Decorator (py313)](py313-run2-generate_slim-combined-side-by-side-dark.svg){.lightbox}
:::

Well we finally see one configuration break out from the pack:
apparently `torch.compile(model, {'fullgraph': True})` yielded the
best generation rate we've seen yet, hovering around 160 tokens/sec.
Note that all the total run times are still pretty similar, hovering
around that 14-15s mark.

Now, let's uncomment the `@torch.compile` decorator above `def
generate_slim()` and do another full run:

::: {.theme-light}
![Generation Performance - generate_slim() - With @torch.compile Decorator (py313)](py313-run3-generate_slim-torch-compiled-combined-side-by-side-light.svg){.lightbox}
:::

::: {.theme-dark}
![Generation Performance - generate_slim() - With @torch.compile Decorator (py313)](py313-run3-generate_slim-torch-compiled-combined-side-by-side-dark.svg){.lightbox}
:::

Oh man.  That first compilation took *forever*.  But once compiled,
our tokens/sec generation rate shoots up significantly to the 250+
range instead of the 150+ range!  But at a crazy up-front cost---as
you can see with all of the run times (the values in parenthesis in
the labels and x-axis in first and second plots, respectively) were in
excess of four minutes.

As we're only doing twenty runs of generating ~90-100 characters, that
startup cost is brutal, however, if we were doing model training or
launching a long-running inference service like our HTTP server, the
startup cost would be quickly amortized away as we benefit from about
a 75% speedup.

So, interesting stuff, kind of.  To be fair... PyTorch clearly
indicates free-threaded Python isn't supported, per our hacks earlier,
so, this should be an interesting area of development in future
releases, especially now that we can see how much benefit there is to
doing multi-threaded run-time inference in a single Python process.

# Conclusion

In this article, we've explored the world of free-threaded Python and
PyTorch, and demonstrated that you can now do parallel inference on
PyTorch models, and that it all plays very nicely together when wrapped
up with an `asyncio`-based HTTP server.

Hopefully this encourages more folks to experiment with free-threaded
Python, or perhaps port their existing Python packages to play nicely
when installed in a free-threaded Python environment.  I personally
can't wait until free-threaded Python is the default!  Although that's
probably at least five or so years out at this point.

<!-- vim:set ts=8 sw=2 sts=2 expandtab textwidth=78 -->
