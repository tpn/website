---
title: "DRAFT: From PyParallel to Python Free-Threading"
subtitle: "Optimally Exploiting Multiple Cores with Python"
categories:
  - Python
  - PyParallel
  - Free-threading
  - No-GIL
  - PyTorch
  - asyncio
author: "Trent Nelson"
draft: true
image: "images/python-free-threading.png"
description: |
  This article takes a look at the new no-GIL, free-threading functionality
  introduced in Python 3.13, as well as how it compares and performs against
  PyParallel's attempt at explointing multiple CPU cores over ten years ago.
  Using a demo project named Parallelopedia, we demonstrate the benefits
  afforded by the new functionality, whereby large data structures (tens to
  hundreds of gigabytes) are loaded in main memory and then accessed in
  parallel from Python---a feat not previously possible with the contemporary
  `multiprocessing` approach.  Additionally, we investigate multi-threaded,
  parallel inference (generation) of transformer-based Deep Neural Networks
  via PyTorch, and explorer the limits of what can and can't currently be
  done, as well as a roadmap for future PyTorch support.
---

This article is sponsored by [Meta](https://meta.com) in collaboration with
[Quansight](https://quansight.com) and [OpenTeams](https://openteams.com).

# Introduction

Since it's inception, Python has had a
[Global Interpreter Lock](
https://docs.python.org/3/glossary.html#term-global-interpreter-lock),
or GIL, that prevented more than one thread running Python code at the same
time.

Python 3.13, released in October 2024, is the first version of Python to
introduce support for a "no-GIL" *free-threaded* mode, per
[PEP-703 Making the Global Interpreter Lock Optional in CPython](
https://peps.python.org/pep-0703/), unlocking the ability for multiple Python
threads to run simultaneously.

This allows, for the first time since the language's inception in December
1989, a single Python process to saturate all CPU cores in parallel with
pure Python code (i.e. not farming out to extension modules written in C, C++,
or, more recently, Rust).

I canâ€™t overstate how exciting this is!  For decades, countless attempts have
been made to solve this problem---including my own efforts with
[PyParallel](https://pyparallel.org)---but no-one has prevailed in getting a
solution mainlined into CPython.  Until now!

I will admit I am biased: I am a Python committer (though not directly involved
in this work), I adore the language, and have spent years obsessing over the
dream of a parallel Python.  It's a topic I'm incredibly passionate about, and
this article will unabashedly proselytize the extraordinary engineering efforts
that finally made a free-threaded Python a reality.

Thankfully, it's not just blind, uninformed enthusiasm.  Over the past few
months I've had the opportunity to extensively test-drive free-threaded Python
to prepare for this article.  The verdict?  I think it's phenomenal.  Both the
implementation and performance on real-world problems.  Free-threaded Python
removes a significant limitation, making it possible to tackle a wide range of
problems that were previously out of reach.  I genuinely believe this is the
best thing to ever happen to the Python language over the last three decades,
and it will solidify its position as one of the most dominant programming
languages for the next three decades.  After all, we're not getting fewer
cores.

And this is essentially just the first-pass!  Certain trade-offs had to be made
to deliver the core functionality within a reasonable time frame---like
disabling the non-thread-safe functionality introduced by
[PEP-659 Specializing Adaptive Interpreter](
https://docs.python.org/3/whatsnew/3.11.html#whatsnew311-pep659), which is the
feature responsible for delivering large speedups in the last few prior
releases.  However, this is on track to be re-enabled in free-threaded mode
for Python 3.14, allowing multi-threaded code to also benefit from the same
speedups single-threaded code has enjoyed the past few years.

The goal of this article is to review the technical details of the
implementation, with some comparisons to prior work such as PyParallel.  I'll
then cover the types of problems that are now incredibly well-suited to be
tackled with a free-threaded Python, using a demo project I've created called
[Parallelopedia](https://github.com/tpn/parallelopedia) to provide a basis for
concrete examples of real-world problems.

Finally, I'll demonstrate some examples of how [PyTorch](https://pytorch.org)
can be used today with free-threaded Python, with particular focus on parallel
inference (generation) on a shared model.  Load a model once in a single Python
process, replicate it across GPUs of interest, and perform parallel inference
from multiple threads without needing to mess around with `multiprocessing`.

I'll also cover areas of PyTorch that don't yet work with free-threading, and
the plans for tackling that in the future.

The motivation behind the PyTorch focus is simple: Meta's PyTorch folks,
through their relationship with [Quansight](https://quansight.com), paid for
my time to work on this article.  Meta was also heavily involved in supporting
and funding the free-threaded Python efforts, too; this was also thanks to
their relationship with Quansight.  So special thanks goes out to Meta for all
of their support with both free-threaded Python, and PyTorch overall.

And I'd be remiss if I didn't thank my friend, colleague, and twice-now boss
[Dr. Travis Oliphant](https://en.wikipedia.org/wiki/Travis_Oliphant), CEO of
Quansight and OpenTeams, primary creator of
[NumPy](https://en.wikipedia.org/wiki/NumPy), founding contributor of
[SciPy](https://en.wikipedia.org/wiki/SciPy), and one of the founders of
Continuum.io---now [Anaconda](https://anaconda.com), with whom I worked many
years ago.  After the abrupt layoffs at [Voltron
Data](https://voltrondata.com/) in late September 2024, I reached out to Travis
for work and he snapped me up instantly, putting me to work on the PyTorch
project with [Andrew James](https://github.com/amjames) for a few months before
I started my new role as a Principle Engineer at NVIDIA on the Core CUDA
Compute Libraries team in late January, 2025.

Thank you Meta, Quansight, Travis, and James!  And thanks to all the folks
involved in delivering free-threaded Python, including the Python Steering
Council for taking a calculated risk and accepting
[PEP-703 Making the Global Interpreter Lock Optional in CPython](
https://peps.python.org/pep-0703/) in the first place.

# Background

The [Related Work](https://peps.python.org/pep-0703/#related-work) section of
PEP-703 does a fantastic job of capturing all of the past attempts at improving
Python's ability at simultaneously leveraging multiple CPU cores.

## Greg Stein's Free Threaded Patch (1996)

The first attempt to remove the GIL was by Greg Stein in 1996, with a
[patch](
http://www.python.org/ftp/python/contrib-09-Dec-1999/System/threading.tar.gz)
against Python 1.4.  (1996!  I hadn't even heard of Python back in 1996---I was
only in tenth grade at the time.)

Unfortunately, Greg's approach introduced some pretty severe overhead.  With
the patches applied, *all* Python code was slowed down by about 40%,
regardless of whether it was single-threaded or multi-threaded code.

That severe of a slowdown was prohibitively expensive, and the changes were never
mainlined.

Fast-forward 15 years or so, and David Beazley wrote a detailed post in August
2011 about Greg Stein's work in an article titled: [An Inside Look at the GIL
Removal Patch of Lore](
https://dabeaz.blogspot.com/2011/08/inside-look-at-gil-removal-patch-of.html).
As part of this article, Dave obtains the original Python 1.4 source, applies
Greg Stein's patches, and takes the resulting free-threaded Python for a run.

Dave confirms that, indeed, the slowdown introduced by this approach---where
every increment and decrement reference results in either an interlocked op
or a mutex lock and unlock---is prohibitively expensive.

In writing this article, I reviewed Dave's article again, and attempted to
download Python 1.4 so that I could apply Greg's patches as well.
Unfortunately, the download link Dave cites now 404s, so I googled
*"download Python 1.4 source"*, and, funnily enough, the first hit is a Github repo
titled [tpn/python-1.4](https://github.com/tpn/python-1.4) that, 'lo and
behold, is a repo I put together twelve years ago doing exactly that:

 - [Importing the original Python 1.4
   source](https://github.com/tpn/python-1.4/commit/c6ef13ae4c089eaf8c39942655d65953e6849469)
 - [Applying Greg's
   patches](https://github.com/tpn/python-1.4/commit/5c17e33224e2b57948ab594280a006d7648347e6)

### Python 1.4

#### Objects and VarObjects

Python's main object structure, the `PyObject`, hasn't changed much over the
years.  For non-container objects (i.e. scalars such as integers, floats, etc.)
the structure is simply a reference count and a pointer to a type object:

```c
typedef struct {
    int ob_refcnt;
    PyTypeObject *ob_type;
} PyObject;
```

Container objects--otherwise known as *"variable"* objects--are represented
via the `PyVarObject` structure.  These structures *"inherit"* the base
`PyObject` layout, and then add an additional field for tracking the number
of elements contained by the object: the `ob_size` field.  In Python 1.4, the
structure looked like this:

```c
typedef struct {
    int ob_refcnt;
    PyTypeObject *ob_type;
    int ob_size;
} PyVarObject;
```

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Actual C Header Code"}
The actual C header code for defining these structures is more baroque; the
relevant Python 1.4 [Include/object.h](
https://github.com/tpn/python-1.4/blob/c6ef13ae4c089eaf8c39942655d65953e6849469/Include/object.h#L104)
code looks like this:

```c
#define PyObject_HEAD \
    int ob_refcnt; \
    struct _typeobject *ob_type;
#define PyObject_HEAD_INIT(type) 1, type,

#define PyObject_VAR_HEAD \
    PyObject_HEAD \
    int ob_size; /* Number of items in variable part */
typedef struct _object {
    PyObject_HEAD
} PyObject;

typedef struct {
    PyObject_VAR_HEAD
} varobject;
```
:::

#### Reference Counting

There was no cyclic garbage collection in Python 1.4--that feature wasn't added
until Python 2.0 around the October 2000 timeframe.  Reference counting was the
only means for Python to reclaim the memory used by objects that were no longer
needed, by way of the `PyObject` `ob_refcnt` field.  Incrementing the reference
simply added one to the existing value.  Decrementing the reference subtracted
one and checked to see if the resulting value was zero, and, if it was, the
object would be deallocated via `_Py_Dealloc()`.

The relevant Python 1.4 [Include/object.h](
https://github.com/tpn/python-1.4/blob/c6ef13ae4c089eaf8c39942655d65953e6849469/Include/object.h#L349C1-L354C18)
code used two macros named `Py_INCREF()` and `Py_DECREF()`, which looked like
this:

```c
#define Py_INCREF(op) ((op)->ob_refcnt++)
#define Py_DECREF(op)           \
    if (--(op)->ob_refcnt != 0) \
        ;                       \
    else                        \
        _Py_Dealloc(op)
```

These operations are inherently unsafe in a multi-threaded environment.  If
two or more separate threads are incrementing or decrementing reference counts
to the same object in parallel, the results are effectively undefined, as there
is no guarantee as to the correctness of the underlying value.

The solution for protecting against this was the Global Interpreter Lock, which
ensured that only one thread could be running Python code at any given time.

Thus, if you're embarking on an attempt to avoid this restriction, such that
you can have multiple Python threads executing Python code simultaneously (i.e.
on multi-core systems), the first thing you need to do is figure out how you're
going to handle making reference counting thread-safe.

### Greg's Free-Threaded Approach

Greg's approach tried the most logical solution: make reference counting
a thread-safe operation.  He did this by using interlocked increment and
decrement operations against the `ob_refcnt` field on NT (Windows) systems.
On Linux, a mutex lock was used to protect the reference count.

Using a `WITH_FREE_THREAD` macro to gate his changes, Greg modified the
[reference counting logic](
https://github.com/tpn/python-1.4/commit/5c17e33224e2b57948ab594280a006d7648347e6#diff-87272721a5cf1cd9915d6f503f6a7bbefa2f26c935c7ce83ca78706afd0ad05aR353-R371)
to `Include/object.h` as follows:

```c
#ifdef WITH_FREE_THREAD
#define Py_INCREF(op) Py_SafeIncr(&(op)->ob_refcnt)
#define Py_DECREF(op)                       \
    if (Py_SafeDecr(&(op)->ob_refcnt) != 0) \
        ;                                   \
    else                                    \
        _Py_Dealloc(op)
#else /* !WITH_FREE_THREAD */
#define Py_INCREF(op) ((op)->ob_refcnt++)
#define Py_DECREF(op)           \
    if (--(op)->ob_refcnt != 0) \
        ;                       \
    else                        \
        _Py_Dealloc(op)
#endif /* !WITH_FREE_THREAD */
```

So, `Py_INCREF(op)` became a call to `Py_SafeIncr()`, and `Py_DECREF(op)`
became a call to `Py_SafeDecr()`.  On Windows, these routines were simply
wrappers around the `InterlockedIncrement` and `InterlockedDecrement`
intrinsics, respectively, defined [here](
https://github.com/tpn/python-1.4/commit/5c17e33224e2b57948ab594280a006d7648347e6#diff-9ed9e47e6055e8799816e87f812a456d9d5f37bf32a1973377777184266ff39fR93-R94):

```c
#define Py_SafeIncr(pint)   InterlockedIncrement((long *)(pint))
#define Py_SafeDecr(pint)   InterlockedDecrement((long *)(pint))
```

On non-NT systems, the `Py_SafeIncr()` and `Py_SafeDecr()` routines were
aliased to `_Py_SafeIncr()` and `_Py_SafeDecr()`, respectively, which were
implemented in [Python/pymutex.c](
https://github.com/tpn/python-1.4/commit/5c17e33224e2b57948ab594280a006d7648347e6#diff-3cb31d5133324b025a026d66a2800f91d58736e296776e9f3c24360106eff142R51-R71)
as follows:

```c
/* mutexes for the subsystems */
PyMutex * _Py_RefMutex;
PyMutex * _Py_ListMutex;
PyMutex * _Py_MappingMutex;
PyMutex * _Py_CritMutex;

...

int _Py_SafeIncr(pint)
    int *pint;
{
    int result;
    PyMutex_Lock(_Py_RefMutex);
    result = ++*pint;
    PyMutex_Unlock(_Py_RefMutex);
    return result;
}

int _Py_SafeDecr(pint)
    int *pint;
{
    int result;
    PyMutex_Lock(_Py_RefMutex);
    result = --*pint;
    PyMutex_Unlock(_Py_RefMutex);
    return result;
}
```

Oof!  That's a single global mutex `_Py_RefMutex` protecting all reference
counts for all objects.  (N.B. `PyMutex` was a slim wrapper around
`pthread_mutex_t`, and the lock and unlock operations simply called out to
`pthread_mutex_lock()` and `pthread_mutex_unlock()` respectively.)

Compared to simply incrementing a value in memory by one, obtaining a mutex
lock and then unlocking the mutex is orders of magnitude more expensive, so
it's not surprising that this approach to removing the GIL introduced brutal
slowdowns---not just to new multi-threaded code, but to existing single-threaded
code that wasn't even taking advantage of the fact that there was now no GIL
in place.

Nevertheless, Greg's work tackled a lot of other important concepts like
separating global state into per-thread state (something that is still relied
upon today), as well as injecting pooled mutexes into builtin container types
like `list` and `dict` objects.

However, the performance impact to single-threaded code was deemed too great,
and as such, Greg's work never made it into mainline Python.  I don't believe
the `python-dev@` mailing list archives were public until a few years later,
so we can't see the specific discussions that were had about Greg's work some
nearly 30 years ago.

## PyParallel

### Genesis

#### The python-ideas@ asyncore discussion

In October 2012, there was a discussion on the `python-ideas@` mailing list
titled [asyncore: included batteries don't fit](
https://mail.python.org/pipermail/python-ideas/2012-October/016311.html).
The general discussion centered around providing better asynchronous I/O
primitives in Python 3.4, and eventually led to the `async` and `yield from`
keywords being introduced in Python 3.5, released around two years later in
September 2015.

That thread was also the genesis for [PyParallel](https://pyparallel.org); a
fork of Python 3.3.5 that I created to implement a proof-of-concept version of
the Python interpreter that tackled not only asynchronous I/O but also
multi-core parallelism.

There's a comprehensive [backstory](#pyparallel-backstory) section on
PyParallel at the end of this article that goes into more detail.  For now, I
just want to focus on the implementation details of PyParallel compared to
Greg Stein's earlier work, before discussing the free-threading implementation
we now have today in Python 3.13t.

#### Reviewing Existing Attempts

I remember reading David Beazley's [article](
https://dabeaz.blogspot.com/2011/08/inside-look-at-gil-removal-patch-of.html)
and a couple of YouTube videos he'd also done on the topic of the GIL.  I made
sure to review Greg's approach to see what didn't work in the past.  Clearly,
introducing fine-grained per-object synchronization for reference counts was
prohibitively expensive---the interlocked increment and decrement operations
alone were bad enough; the global `pthread_mutex_t` reference count approach
never stood a chance.

#### The Hunch

So, what else could we try?  My hunch was that there was a way to achieve
parallelism within the Python interpreter when framed through the lens of
asynchronous I/O primitives, such as a TCP/IP socket server.  For example,
why couldn't a simple HTTP server be written in such a way that multiple
threads could respond to incoming client requests on multiple cores
simultaneously?

If you think about the type of server-side computation that is required to
process an incoming HTTP request, it's just pretty vanilla Python code.  Sure,
you may create a lot of temporary Python objects in servicing the request, but
they're only short-lived.  Once you send a response back to the client, you
don't need any of those objects anymore.  And you're generally not mutating
global state, so you don't need any thread synchronization primitives during
request processing.  The underlying operating system's kernel is handling all
of that for you behind the scenes (i.e. you can safely `accept()` on multiple
threads against the same `sockaddr_in` on Linux, or `AcceptEx()` against a
single completion port on Windows).

### The Implementation

So, as it pertained to reference counting and garbage collection---all
thread-sensitive things that need to be handled very carefully in a GIL-less
Python---with PyParallel, I explored the notion of: *what if we just didn't do
any of that in parallel thread contexts*?

That is, instead of trying to remove the GIL, let's keep the GIL, and instead,
introduce a new concept of a *parallel thread*.  Because if we can come up with
an efficient way to determine if we're a *parallel thread* at any given time,
we can instrument all the thread-sensitive calls like `Py_INCREF()` and
`Py_DECREF()` such that:

 - If we're a *parallel thread*, do X, if not, do Y.
    - X: a thread-safe alternative
    - Y: what we normally do (i.e. when we're holding the GIL)

And as the thread-sensitive calls we need to instrument are ubiquitous---you
really can't do much in Python before needing to `Py_INCREF()` or `Py_DECREF()`
something---so the viability of this approach would be entirely contingent upon
finding an efficient way to detect if we're a parallel thread.

#### Detecting Parallel Threads

I introduced a macro named `Py_PXCTX()` for PyParallel that was responsible for
detecting if we were in one of these *parallel threads* (or *parallel contexts*
as I referred to them in the source code).

As PyParallel was a Windows-only proof-of-concept, I was able to leverage an
x64 intrinsic named `readgsdword` to interrogate the current thread's TEB
([Thread Environment
Block](https://learn.microsoft.com/en-us/windows/win32/api/winternl/ns-winternl-teb))
to obtain the current thread ID (which lives at offset `0x48` on x64 Windows).
By comparing this to the known thread ID of the *main thread* (i.e. the thread
holding the GIL), we could determine if we were a parallel thread with a couple
of instructions.

A simplified 
The [implementation](
https://github.com/pyparallel/pyparallel/blob/branches/3.3-px/Include/pyintrinsics.h#L13)
looked like this:

```c
#define _Py_get_current_thread_id() (__readgsdword(0x48))

static inline
int
_PyParallel_IsParallelContext(void)
{
    return (Py_MainThreadId != _Py_get_current_thread_id());
}
#define Py_PXCTX() (_PyParallel_IsParallelContext())
```

#### Instrumenting Thread-Sensitive Calls

With the ability to quickly determine if we were a parallel thread, we could
instrument all of the thread-sensitive calls to follow the aforementioned
paradigm of:

> If we're a *parallel thread*, do X, if not, do Y.

For reference counting, the *do Y* was literally: do nothing.  E.g., instead
of the standard Python reference counting mechanics that basically looked
like this:

```c
#define Py_INCREF(op) ((op)->ob_refcnt++)
#define Py_DECREF(op) \
    (--(op)->ob_refcnt == 0 ? _Py_Dealloc(op) : ;)
```

The PyParallel approach would gate the reference counting logic with the
`Py_PXCTX()` macro, along these lines:

```c
#define Py_INCREF(op) (!Py_PXCTX() && ((op)->ob_refcnt++))
#define Py_DECREF(op) \
    (!Py_PXCTX() && (--(op)->ob_refcnt == 0 ? _Py_Dealloc(op) : ;))
```

This is a purposefully-simplified example---the actual `Py_INCREF(op)` and
`Py_DECREF(op)` calls in PyParallel eventually grew debug guards that allowed
me to catch erroneous cases of, for example, the main thread attempting to
mutate the reference count of an object created by a parallel thread.

#### Changes to PyObject

And in order to ascertain whether any given object was created by the Python
*main thread*, or one of these new *parallel thread contexts*, I had to augment
the `PyObject` structure to capture some additional state about each object.
Note that the Python Free-Threading implementation, which we'll discuss after
this, also does the same thing (adds new `PyObject` fields).

I made three changes to `PyObject`:

1. A field I could fill with a sentinel value that unambiguously indicated
   [from whence it came](https://apps.npr.org/arrested-development/joke-39.html)
   (i.e. a main thread or parallel thread): `void *is_px`.
1. A flags field I could store pertinent per-object state in:
   `size_t px_flags`.
1. A pointer to the object's original type object: `PyTypeObject *orig_type`.
   In some situations, we would override an object's type with a
   parallel-specific shim type that could intercept certain operations.

Technically, the `is_px` field was redundant once I introduced the `px_flags`
field, as we could track whether or not an object was a parallel object by way
of a single flag bit, so I could have shaved off 8 bytes by removing it.

I kept it in because a) I never got around to removing it, and b) it was helpful
when debugging, as the `is_px` field was initialized to one of two
`0xDEADBEEF`-style sentinel values, shown below, which made it very easy to
eyeball in the debugger when traipsing through memory or crash dumps.

```c
#define _Py_DEADBEEF 0xdead1234beef5678 /* is_px when main thread */
#define _Px_DEADBEEF 0x9876beef5432dead /* is_px when parallel thread */
```
A side-by-side comparison of the 64-bit Python 3.3 `PyObject` structure and
the equivalent PyParallel `PyObject` structure is shown below:

<div class="side-by-side-struct-layout">
<div class="side-by-side-struct-column">

##### Python PyObject

<div class="struct-layout">
<div class="struct-cell py-field" style="grid-column: span 8;">
<pre>Py_ssize_t ob_refcnt
(8 bytes)</pre>
</div>
<div class="struct-cell py-field" style="grid-column: span 8;">
<pre>PyTypeObject &#42;ob_type
(8 bytes)</pre>
</div>
</div>

<div style="text-align: left; margin-top: 1em;">
Total structure size: 16 bytes.
</div>

</div>
<div class="side-by-side-struct-column">

##### PyParallel PyObject

<div class="struct-layout"><div class="struct-cell px-field" style="grid-column: span 8;">
<pre>void &#42;is_px
(8 bytes)</pre>
</div>
<div class="struct-cell px-field" style="grid-column: span 8;">
<pre>size_t px_flags
(8 bytes)</pre>
</div>
<div class="struct-cell px-field" style="grid-column: span 8;">
<pre>PyTypeObject &#42;orig_type
(8 bytes)</pre>
</div>
<div class="struct-cell py-field" style="grid-column: span 8;">
<pre>Py_ssize_t ob_refcnt
(8 bytes)</pre>
</div>
<div class="struct-cell py-field" style="grid-column: span 8;">
<pre>PyTypeObject &#42;ob_type
(8 bytes)</pre>
</div></div>

<div style="text-align: left; margin-top: 1em;">
Total structure size: 40 bytes (24 bytes larger than standard Python).
</div>

</div>
</div>

#### Memory Allocation

So, with regards to reference counting, my hunch was simply: who cares?
Again, I was keen on targetting the very narrow domain of an asynchronous I/O
based TCP/IP socket server, like a HTTP server, where none of the objects that
are created as part of servicing the request need to persist once we've sent
bytes back on the wire to the client.

So who cares about reference counts?  We know when the parallel context starts
and ends: it starts when we call the user's `data_received()` routine, and it
ends when that routine returns bytes which we then send back to the client.

Each parallel thread can have a thread-local, simple block allocator, that is
responsible for furnishing all memory allocation requests thanks to us having
instrumented calls like `PyObject_New` and `PyMem_Malloc`.  Block allocators
are about the simplest memory allocator you can have: you have a pre-allocated
block of memory (say, 4KB, or one page), a request comes in for 120 bytes,
you capture the address of the current block pointer, increment it by 120
bytes, then return the captured address.

How do you free memory?  Easy!  You don't!  Again, socket-server callbacks are
pretty short-lived; you're not allocating gobs of memory that has an ambiguous
lifetime.  You're doing lots of little allocations with very clear lifetimes:
the allocations will cease to exist once the callback returns.

How do you handle garbage collection?  Easy!  You don't!  As you're not
mutating global state, any cyclic references you construct during the parallel
thread callback will be magically taken care of when the callback completes,
by virtue of the fact you're simply resetting the block allocator back to the
start of the first block.  By not relying on reference counts to hit zero to
trigger deallocation, you don't need to worry about reference cycles.



### The Performance

The performance of PyParallel was really good.  There was negligible overhead
introduced by instrumenting all of the thread-sensitive macros, which was a big
win.  Remember, Greg Stein's approach made **all** Python code run about 40%
slower, single-threaded or multi-threaded.  PyParallel's approach of detecting
if we're in a parallel thread by way of the `Py_PXCTX()` macro, plus fast
thread-safe alternates of the thread-sensitive calls, proved that there were
viable ways to achieve parallelism in Python without also incurring prohibitive
overhead.

### The Limitations

For all of the performance benefits of PyParallel, it was, in essence, a
proof-of-concept project I predominantly hacked together over the course of
about three months during a very cold Michigan Winter in 2012-2013.

By design, I never intended to support *"free-threaded"* Python, i.e. you
would never be able to create Python `threading.Thread` objects yourself,
instead, you'd have to use the machinery provided to you by PyParallel,
such as `async.server()`, `async.submit_work()`, etc.  (This was before
`async` became a keyword in Python.)

...

# Python Free-Threading

Now, let's talk about the Python Free-Threading implementation that has been
introduced in Python 3.13t.

## Implementation

### Changes to PyObject

::: {.panel-tabset}

## Python vs Free-Threading

:::: {.panel-tabset}

### Python PyObject

<div class="struct-layout">
<div class="struct-cell py-field" style="grid-column: span 8;">
<pre>Py_ssize_t ob_refcnt
(8 bytes)</pre>
</div>
<div class="struct-cell py-field" style="grid-column: span 8;">
<pre>PyTypeObject &#42;ob_type
(8 bytes)</pre>
</div>
</div>

<div style="text-align: left; margin-top: 1em;">
Total structure size: 16 bytes.
</div>

### Free-Threaded PyObject

<div class="struct-layout">
<div class="struct-cell ft-field" style="grid-column: span 8;">
<pre>uintptr_t ob_tid
(8 bytes)</pre>
</div>
<div class="struct-cell ft-field" style="grid-column: span 2;">
<pre>uint16_t
ob_flags
(2 bytes)</pre>
</div>
<div class="struct-cell ft-field" style="grid-column: span 1;">
<pre>PyMutex
ob_mutex
(1 byte)</pre>
</div>
<div class="struct-cell ft-field" style="grid-column: span 1;">
<pre>uint8_t
ob_gc_bits
(1 byte)</pre>
</div>
<div class="struct-cell ft-field" style="grid-column: span 4;">
<pre>uint32_t
ob_ref_local
(4 bytes)</pre>
</div>
<div class="struct-cell ft-field" style="grid-column: span 8;">
<pre>Py_ssize_t ob_ref_shared
(8 bytes)</pre>
</div>
<div class="struct-cell py-field" style="grid-column: span 8;">
<pre>PyTypeObject &#42;ob_type
(8 bytes)</pre>
</div></div>

<div style="text-align: left; margin-top: 1em;">
Total structure size: 32 bytes (16 bytes larger than standard Python).
</div>

::::

## PyParallel vs Free-Threading

:::: {.panel-tabset}

### PyParallel PyObject

TBD

### Free-Threaded PyObject

TBD

::::

:::

# Appendix

## PyParallel Backstory {#pyparallel-backstory}

I'm not sure how interesting this section will be to readers, which is why I've
stashed it at the end of the article.  The older I get---especially now that I
have a daughter---the more cathartic it is reminiscing about the early parts of
my career some nearly-two-decades-ago.

This section captures the backstory behind PyParallel.  PyParallel was born out
of a) my interest in asynchronous I/O and multithreaded programming, and b) my
experience with Snakebite, an "open source network" I built with the goal to
provide Python committer's access to all the different types of platforms
Python ran on---especially all the UNIX derivatives from the late 90s to 00s
for which I seemed to be genetically predisposed to overly romanticize.

And I came up with the idea for Snakebite as a Python committer, where I found
myself often frustrated at trying to debug buildbot breaks on platforms for
which I had no access.  And becoming a Python committer was an artifact
of my involvement with the Python language, particularly with regards to
providing buildbots and help with Windows 64-bit Python builds in the early
days where AMD64 was still a novelty.

And I was led to Python by a random article by Eric S. Raymond, coupled with
growing resentment toward Perl, which I found myself having to use when working
with ClearQuest.

That's the reverse chronological history in a nutshell.  The next few sections
go into more detail chronologically.

### Becoming a Python Committer

From around 2000 to 2004, I was writing mostly C, C++, and Perl.  I had founded
a company with a business partner, Jamie Echlin, called
[OnResolve](https://web.archive.org/web/20080704070902/http://www.onresolve.com/home/),
and provided consultancy for an IBM---formerly Rational---change management
product called [ClearQuest](https://www.ibm.com/products/rational-clearquest),
as well as a number of software products that extended the functionality of
ClearQuest.  ClearQuest could be customized in two languages: Visual Basic, and
Perl.  The most prominent software product Jamie and I created was OnMessage,
which was a combination of C++ and Perl, and provided enhanced e-mail
notification facilities for ClearQuest.  Jamie deserves much of the credit for
what eventually became known as
[OnMessage](https://web.archive.org/web/20080622060654/http://www.onresolve.com/products/onmessage/)
---despite having a quirky Perl coding style I found difficult to work with, he
absolutely laid the foundation for the product and the subsequent success we had
with it.

ClearQuest, and its version-control counterpart, ClearCase, were ubiquitous in
all of the industries that had the most money in the early to mid 2000s.
Particularly finance and oil.  No-one liked using either of them, they were
expensive, required huge teams to maintain and administer, and with the advent
of tools like Git and Github (albeit many years later), they are now relegated
to the annals of history.

I was relatively productive with Perl, but I didn't particularly like it.  And
I absolutely loathed having to deal with other people's Perl code.  Around late
2004 and 2005, Python arrived on my radar.  I remember reading Eric S.
Raymond's [Why Python](https://www.linuxjournal.com/article/3882)
article about Python, and it resonated strongly with me.  Coming from
Perl, Python just felt simply magical.  It was intuitive, easy to read, not
cryptic, easy to work with other people's code, powerful--particularly as a glue
language.

I adored it.  I tackled all new projects with Python.  I wrote a Python wrapper
around ClearQuest named [cqpython](https://github.com/tpn/cqpython), which I
used to carry out one of my most favorite projects to date: merging two
ClearQuest databases into a single instance by auto-generating optimized SQL
to conduct the merge at the database level.  (This was harder than it sounds,
as all ClearQuest databases would start off with the same unique ID offsets,
so two otherwise unrelated databases would have almost identical keys for
different entities, which needed to be resolved efficiently at merge time.)

By 2007-2008, I was bored of ClearQuest---it wasn't a fun product to work with,
nor did it look like it would be a good career move to continue specializing in
it; the prices were extortionate, a lot of companies were trying to move off it,
and better, cheaper alternatives were popping up.

However, it sure had been lucrative.  I was able to enjoy extended periods of
not needing to work, and I spent that time on things I was finding really fun,
like contributing to Python.  I set up a bunch of buildbots for Python, and
was particularly interested in getting the 64-bit Windows Python builds "green"
on the buildbots I had set up.  I think I even paid for the Verisign code
signing certificate used for the Python.org official Windows binaries back then
(this was still the Python 2.x days).

This was back in the day where Martin von LÃ¶wis was still an active contributor
for Python, and if I recall correctly, the release manager for the Python 2.x
series.  Martin also maintained the Windows Python builds, a role that was
eventually taken over by Steve Dower, who is still doing it to this day.

In 2008, I attended PyCon in Chicago.  It was my first-ever PyCon.  I had a
fantastic time, and particularly loved the core Python sprints that happened
after the conference.  For those that are unaware, this is the one time every
year where many of the active Python committers, including Guido, get together
in a big conference room and just hack on Python collectively for a few days.

It was a bit of a surreal experience, sitting in a room hacking on Python
alongside the very founder of Python himself---Guido---plus many other
luminaries that actively contributed to the language over the years.  I don't
know who took this photo, nor why it appears to have dimensions tailored
towards ants, but you can see myself and Michael Foord in the foreground table
(dark maroon t-shirt on the left and orange t-shirt on the right, respectively),
Guido in the bright green shirt at the top right table, sitting with Neal
Norwitz and Barry Warsaw.  And that looks like Brett Canon in thinking pose in
the dark t-shirt and Martin von LÃ¶wis sitting next to him scratching his head
at whatever Brett was doing at the middle table :-)

![PyCon 2008 Core Sprint](images/pycon2008-chicago-core-sprints.jpg)

I remember inundating Martin with patch bombs fixing various buildbot-related
things at the start of that sprint, to which Martin eventually dealt with by
simply offering me commit privileges.  I enthusiastically accepted!  Honestly,
that was a career highlight.  This was in the days before everything was Github
and Pull Requests---becoming a Python committer meant you would literally get
svn+ssh access to svn.python.org, plus ssh access to the python.org boxes
themselves if you wanted it/needed it (i.e. for buildbot master configuration).
This was certainly more compelling than the ClearQuest consultancy I'd done in
the past, that's for sure!

As much as I love Git and Github, getting your pull request accepted for an
open source project just doesn't have the same feel as getting offered commit
privileges like back in the old days.


# Colophon {#colophon}

As much as I enjoy reading technical articles, I often find myself equally
interested in the tools the author used in the production of the article.  In
this final section, I attempt to capture details regarding the tools used to
author this article, in no particular order.

## Quarto & Markdown

This article was written in Markdown using [Quarto](https://quarto.org).  The
source code is
[here](https://github.com/tpn/website/edit/main/articles/from-pyparallel-to-python-free-threading/index.qmd);
there's also an *Edit this page* link on the top-right of the page, under the
contents (assuming you're viewing the site on a wide-enough device).

For the syntax color scheme, I copied the
[`dracula.theme`](https://github.com/quarto-dev/quarto-cli/blob/80bf6b5848bf644de3a4d59e26b5eb067375986a/src/resources/pandoc/highlight-styles/dracula.theme)
from the Quarto repository into
[`tpn.theme`](https://github.com/tpn/website/blob/main/tpn.theme) and then
just hacked on it until I was mostly content with the results.

My whole [trent.me](https://trent.me) website uses Quarto, and is hosted
publicly on Github at my [website](https://github.com/tpn/website) repo.
I also leverage Github's hosting facilities via their "Pages" functionality,
the configuration of which can be seen below.

::: {.callout-tip collapse="true" appearance="simple" icon="false" title="Github Pages Configuration"}
![Github Pages Configuration for tpn/website](images/github-tpn-website-pages.png){.lightbox}
:::

"Development" on the website (i.e. writing articles like this one) is done
on a variety of boxes, typically all Linux/WSL2.  Quarto has a useful preview
feature, so my local workflow is usually just something as simple as:

```bash
% cd ~/src/website
% quarto preview
```

I edited this `index.qmd` page predominantly in vim, although I'd sometimes use
VS Code and the Quarto extension with integrated previewing.  I generally turn
Copilot off in vim when writing pure text, it's too distracting, and isn't
geared toward writing new prose versus coding tasks, which is obviously its
bread and butter.

## AI Tooling

In general, I for one welcome our new AI overlords.  I have heavily integrated
Copilot and ChatGPT into my workflow now as a developer for the past two years
or so, and I thoroughly enjoy leveraging both tools.

### LM Studio

[LM Studio](https://lmstudio.ai/) has also left a lasting impression and I've
enjoyed experimenting with it a lot recently too.  This has been aided by
having a local box at home with 256 GB RAM, 40 cores, four Tesla V100 32GB
GPUs, and a 14TB RAID0 stripe---plenty of room to download all of these huge
models coming out.

A nice thing about LM Studio is that with a few clicks you can expose a local,
OpenAI-compatible REST interface.  LM Studio is powered by
[llama.cpp](https://github.com/ggerganov/llama.cpp).

### Aider

I came across [Aider](https://aider.chat) recently.  Aider is, to quote their
website, directly, **AI pair programming in your terminal**.  It's pretty neat,
and I played around with having it drive a little bit of the Python development
in the [parallelopedia](https://github.com/tpn/parallelopedia) repo, and a lot
of the development in the
[parallelopedia-ui](https://github.com/tpn/parallelopedia-ui) repo, which is
the React, Bootstrap, JavaScript/JSX web UI seen in this article.

i'm not a web developer, I don't know how to write web apps, I don't care about
code quality of web apps I do write (as I'm not writing anything other than
demos or small little tools), so, whatever gets the job done is fine.  However,
I am, by trade, a software engineer, so a lot of the core skill set still
**commutes** when working in foreign domains.  *Especially* these days when you
have AI an alt-tab away, or, in the case of Aider, available in a terminal to
carry out your development requests.

The biggest issue I had with Aider was honestly just biting the bullet and
just trying it.  **Begin, the rest is easy** as they say.  It's definitely
not perfect--I had to jump in and fix things a few times every session I had
with it, but we're talking maybe five interventions within a 3-4 hour coding
session, give or take.  It was particularly good at generating scaffolding that
I could hack further with a single sentence.  And it was really good at writing
an entire Python unit test module for me based on a single-sentence.

To get started with Aider:

```bash
% python -m pip install aider-install
% aider-install
# Restart shell, or `export PATH=$HOME/.local/bin:$PATH`
% which aider
/home/trent/.local/bin/aider
# I've got my OpenAI keys in a GPG-encrypted file ~/.zsh/openai_key.asc.
% which init_openai_key
init_openai_key () {
    if [ -f ~/.zsh/openai_key.asc ]
    then
        export OPENAI_API_KEY=$(
            gpg -d --batch --quiet ~/.zsh/openai_key.asc
        )
    fi
}
% init_openai_key
% aider --model gpt-4o --vim
```
It was trivial to point it at a local model hosted by LM Studio, too:

```bash
% aider --model lm_studio/qwen2.5-coder-32b-instruct@fp16 --vim
```

By default, Aider keeps a Markdown transcript of your sessions in a file named
`.aider.chat.history.md` in the root of the repository you're working with.  I
have included the transcript from my
[parallelopedia-ui](https://github.com/tpn/parallelopedia-ui) repo below.  I
haven't edited it, so it contains all my mistakes and errors and whatnot.

::: {.callout-tip collapse="true" appearance="simple" icon="false" title="Aider Chat History"}
{{< include _aider.chat.history.md >}}
:::

## PyTorch, Deep Neural Networks, LLMs, and Andrej Karpathy's YouTube Series

# Comments

<!-- vim:set ts=8 sw=2 sts=2 expandtab textwidth=79 -->
