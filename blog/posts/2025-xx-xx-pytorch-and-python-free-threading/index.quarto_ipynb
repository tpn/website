{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"DRAFT: PyTorch and Python Free-Threading\"\n",
        "categories:\n",
        "  - PyTorch\n",
        "  - Python\n",
        "  - Free-Threading\n",
        "  - No-GIL\n",
        "  - LLM\n",
        "  - GPT2\n",
        "author: \"Trent Nelson\"\n",
        "draft: true\n",
        "image: \"images/pytorch-and-python-free-threading.png\"\n",
        "description: |\n",
        "  This blog post explores multi-threaded PyTorch parallel inference via an\n",
        "  `asyncio`-based HTTP server and the new *No-GIL* free-threaded Python,\n",
        "  using a (predominantly-AI-written) simple React Bootstrap web interface.\n",
        "  We capture what is currently possible, what type of issues are typically\n",
        "  encountered, and what the roadmap may look like for the future.\n",
        "jupyter:\n",
        "  kernelspec:\n",
        "    display_name: \"Python 3.13t (Free-Threaded)\"\n",
        "    language: python\n",
        "    name: py313t\n",
        "---\n",
        "\n",
        "\n",
        "This blog post is sponsored in part by [Meta](https://meta.com) in\n",
        "collaboration with [Quansight](https://quansight.com) and\n",
        "[OpenTeams](https://openteams.com).\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Python 3.13, released in October 2024, is the first version of Python to\n",
        "introduce support for a \"no-GIL\" *free-threaded* mode, per\n",
        "[PEP-703 Making the Global Interpreter Lock Optional in CPython](\n",
        "https://peps.python.org/pep-0703/), unlocking the ability for multiple Python\n",
        "threads to run simultaneously.\n",
        "\n",
        "This allows, for the first time since the language's inception in December\n",
        "1989, a single Python process to saturate all CPU cores in parallel with\n",
        "pure Python code (i.e. not farming out to extension modules written in C, C++,\n",
        "or, more recently, Rust).\n",
        "\n",
        "A handful of the [motivations](https://peps.python.org/pep-0703/#motivation)\n",
        "captured in that PEP opine on how the GIL impedes Python AI workflows,\n",
        "particularly as it relates to GPU programming.\n",
        "\n",
        "This blog post explores what can be done with [PyTorch](https://pytorch.org)\n",
        "now with the new free-threaded version of Python, specifically focusing on\n",
        "run-time inference on transformer-based generative models.  Using a simple\n",
        "React Bootstrap web interface for the front-end, a pure-Python\n",
        "`asyncio`-based multi-threaded HTTP server is used to facilitate\n",
        "multi-threaded, simultaneous parallel model inference.\n",
        "\n",
        "# Getting Started\n",
        "\n",
        "All of this work was done on Linux (Ubuntu 22.04) with Python 3.13t, PyTorch\n",
        "2.6, and CUDA 12.6.  I would have liked to get Windows support working too,\n",
        "but unfortunately, the multi-threaded `asyncio`-based HTTP server I wrote\n",
        "for Linux doesn't appear to leverage multiple threads on Windows.  (From a\n",
        "cursory inspection, it appears to be an issue either with event loops not\n",
        "getting created properly on non-main-thread threads, or a more insidious\n",
        "issue with how the `IocpProactor()` code uses I/O completion ports.)\n",
        "\n",
        "## Environments\n",
        "\n",
        "In general, I endeavored to minimize the number of external library\n",
        "dependencies as best I could.  The primary reason for this is that a lot of\n",
        "the modern Python AI stack isn't yet compatible with Python free-threaded\n",
        "builds, unfortunately.  This is improving at a rapid pace, though.\n",
        "\n",
        "Anything written in Rust is particularly problematic, as\n",
        "[PyO3](https://github.com/PyO3/pyo3) (Rust bindings for Python) only recently\n",
        "supported free-threaded Python 3.13+ as of 0.23.3, which was released early\n",
        "December, 2025, and a lot of projects haven't yet updated to it (like TikToken\n",
        "and Pydantic).  Other common projects like `lxml` aren't compatible yet either,\n",
        "which means you can't simply do `pip install llama-stack` and start playing\n",
        "around.  Likewise for `transformers` and other parts of the HuggingFace stack.\n",
        "\n",
        "I will reference two `conda` environments in this post: a Python 3.13\n",
        "free-threaded one named `py313t`, and a normal, not-free-threaded Python\n",
        "3.13 one named `py313`.\n",
        "\n",
        "The primary motivation behind the second `py313` environment is it allows\n",
        "us to install Jupyter Lab, which, at the time of writing, still isn't\n",
        "compatible with a Python free-threaded installation.  However, we can still\n",
        "register a free-threaded Python kernel with Jupyter, which is all we really\n",
        "care about when running the code in this post in a free-threaded environment.\n",
        "\n",
        "Details on creating the `conda` environments follow.\n",
        "\n",
        "### Free-Threaded 3.13 Env (py313t)\n",
        "\n",
        "I use `conda` to create the Python 3.13 free-threaded environment plus\n",
        "initial dependencies, activate it, then install the remaining dependencies\n",
        "via pip, as follows:\n",
        "\n",
        "```bash\n",
        "conda create -n py313t python=3.13 python-freethreading \\\n",
        "    nodejs pip tqdm flake8 rust \\\n",
        "        -c conda-forge\n",
        "conda activate py313t\n",
        "pip install numpy setuptools_rust regex requests datrie\n",
        "```\n",
        "\n",
        "`nodejs` is required for the UI component we'll introduce later.  `regex`,\n",
        "`rust`, and `setuptools_rust` are needed for `tiktoken`, described next.\n",
        "Finally, `numpy` is for `torch`, which we install later, too.\n",
        "\n",
        "#### TikToken\n",
        "\n",
        "[TikToken](https://github.com/openai/tiktoken) is a fast BPE tokenizer from\n",
        "OpenAI that is used extensively in the emerging Python LLM landscape.  At the\n",
        "time of writing, the latest TikToken release was [0.8.0](\n",
        "https://github.com/openai/tiktoken/releases/tag/0.8.0), which was built\n",
        "against PyO3 0.22.2, which isn't compatible with free-threaded Python.\n",
        "\n",
        "Thankfully, it was trivial to get a local installation of `tiktoken` working\n",
        "by cloning the Github repo, bumping the PyO3 version in `Cargo.toml`, then\n",
        "rebuilding and installing.\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "This is a perfect example of the type of fiddling around I wanted to avoid\n",
        "by not depending on any external packages other than the bare necessities,\n",
        "such as PyTorch.  I made an exception for `tiktoken` because a) it's arguably\n",
        "an equally-important part of the LLM stack as `torch`, and b) it thankfully\n",
        "wasn't *too* difficult getting a compatible version of `tiktoken` installed\n",
        "locally.\n",
        "\n",
        ":::\n",
        "\n",
        "Clone the tiktoken git repo and cd into it as follows:\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/openai/tiktoken\n",
        "cd tiktoken\n",
        "```\n",
        "\n",
        "Edit the `Cargo.toml` file and change the `pyo3` dependency version to at\n",
        "least 0.23.3^[I used 0.23.3, as that was the latest version available at the\n",
        "time, however, 0.23.4 has since been released, so you could try that too]:\n",
        "\n",
        "```diff\n",
        "diff --git a/Cargo.toml b/Cargo.toml\n",
        "index 2eed0c1..6be5f63 100644\n",
        "--- a/Cargo.toml\n",
        "+++ b/Cargo.toml\n",
        "@@ -9,7 +9,7 @@ name = \"_tiktoken\"\n",
        " crate-type = [\"cdylib\"]\n",
        " \n",
        " [dependencies]\n",
        "-pyo3 = { version = \"0.22.2\", default-features = false, features = [\"extension-module\", \"macros\"] }\n",
        "+pyo3 = { version = \"0.23.3\", default-features = false, features = [\"extension-module\", \"macros\"] }\n",
        " \n",
        " # tiktoken dependencies\n",
        " fancy-regex = \"0.13.0\"\n",
        "```\n",
        "\n",
        "With this patch applied, and the `py313t` conda environment active (with\n",
        "`rust` and `setuptools_rust` installed):\n",
        "\n",
        "```bash\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "```\n",
        "\n",
        "::: {.callout-note collapse=\"true\" appearance=\"simple\" icon=\"false\" title=\"Show Build & Install Output\"}\n",
        "\n",
        "\n",
        "```bash\n",
        "(py313t) {pytorch} [12.6] [trent@dgx/ttypts/3(~s/tiktoken)%] python setup.py build\n",
        "running build\n",
        "running build_py\n",
        "copying tiktoken/_educational.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/__init__.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/registry.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/model.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/load.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/core.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken_ext/openai_public.py -> build/lib.linux-x86_64-cpython-313t/tiktoken_ext\n",
        "running egg_info\n",
        "writing tiktoken.egg-info/PKG-INFO\n",
        "writing dependency_links to tiktoken.egg-info/dependency_links.txt\n",
        "writing requirements to tiktoken.egg-info/requires.txt\n",
        "writing top-level names to tiktoken.egg-info/top_level.txt\n",
        "reading manifest file 'tiktoken.egg-info/SOURCES.txt'\n",
        "reading manifest template 'MANIFEST.in'\n",
        "warning: no files found matching 'Makefile'\n",
        "adding license file 'LICENSE'\n",
        "writing manifest file 'tiktoken.egg-info/SOURCES.txt'\n",
        "copying tiktoken/py.typed -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "running build_ext\n",
        "running build_rust\n",
        "cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --\n",
        "   Compiling target-lexicon v0.12.16\n",
        "   Compiling once_cell v1.20.2\n",
        "   Compiling proc-macro2 v1.0.92\n",
        "   Compiling unicode-ident v1.0.14\n",
        "   Compiling memchr v2.7.4\n",
        "   Compiling regex-syntax v0.8.5\n",
        "   Compiling libc v0.2.169\n",
        "   Compiling autocfg v1.4.0\n",
        "   Compiling heck v0.5.0\n",
        "   Compiling bit-vec v0.6.3\n",
        "   Compiling unindent v0.2.3\n",
        "   Compiling indoc v2.0.5\n",
        "   Compiling cfg-if v1.0.0\n",
        "   Compiling rustc-hash v1.1.0\n",
        "     Running `rustc --crate-name build_script_build --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/target-lexicon-0.12.16/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arch_zkasm\", \"default\", \"serde\", \"serde_support\", \"std\"))' -C metadata=826df8be4fa9ef21 -C extra-filename=-826df8be4fa9ef21 --out-dir /home/trent/src/tiktoken/target/release/build/target-lexicon-826df8be4fa9ef21 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name once_cell --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/once_cell-1.20.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"race\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"atomic-polyfill\", \"critical-section\", \"default\", \"parking_lot\", \"portable-atomic\", \"race\", \"std\", \"unstable\"))' -C metadata=6870d82906744d65 -C extra-filename=-6870d82906744d65 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.92/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=4463b02a2f05d75c -C extra-filename=-4463b02a2f05d75c --out-dir /home/trent/src/tiktoken/target/release/build/proc-macro2-4463b02a2f05d75c -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name unicode_ident --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unicode-ident-1.0.14/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=17278dafa5f26de9 -C extra-filename=-17278dafa5f26de9 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name memchr --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memchr-2.7.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"compiler_builtins\", \"core\", \"default\", \"libc\", \"logging\", \"rustc-dep-of-std\", \"std\", \"use_std\"))' -C metadata=25fa9792dd9399b0 -C extra-filename=-25fa9792dd9399b0 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name regex_syntax --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-syntax-0.8.5/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arbitrary\", \"default\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\"))' -C metadata=66f570e05dbe3825 -C extra-filename=-66f570e05dbe3825 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.169/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=42fd5088387abf7a -C extra-filename=-42fd5088387abf7a --out-dir /home/trent/src/tiktoken/target/release/build/libc-42fd5088387abf7a -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name autocfg --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/autocfg-1.4.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b8e4c5d316ce5bfb -C extra-filename=-b8e4c5d316ce5bfb --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name bit_vec --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bit-vec-0.6.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"serde\", \"serde_no_std\", \"serde_std\", \"std\"))' -C metadata=de2a0d1e2ef2490a -C extra-filename=-de2a0d1e2ef2490a --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name heck --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/heck-0.5.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=5e22b1dffa7f4255 -C extra-filename=-5e22b1dffa7f4255 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name once_cell --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/once_cell-1.20.2/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"race\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"atomic-polyfill\", \"critical-section\", \"default\", \"parking_lot\", \"portable-atomic\", \"race\", \"std\", \"unstable\"))' -C metadata=05003007543cfa87 -C extra-filename=-05003007543cfa87 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name unindent --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/unindent-0.2.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=d53e2a0385a47a80 -C extra-filename=-d53e2a0385a47a80 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name indoc --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/indoc-2.0.5/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=b4e94d9ecbd21a39 -C extra-filename=-b4e94d9ecbd21a39 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro --cap-lints allow`\n",
        "     Running `rustc --crate-name cfg_if --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/cfg-if-1.0.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"compiler_builtins\", \"core\", \"rustc-dep-of-std\"))' -C metadata=e4ded2c19830fbdd -C extra-filename=-e4ded2c19830fbdd --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "     Running `rustc --crate-name rustc_hash --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rustc-hash-1.1.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=b006f4d81e95dfaf -C extra-filename=-b006f4d81e95dfaf --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow`\n",
        "   Compiling bit-set v0.5.3\n",
        "     Running `rustc --crate-name bit_set --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bit-set-0.5.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"std\"))' -C metadata=40d90f83eb5bab57 -C extra-filename=-40d90f83eb5bab57 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern bit_vec=/home/trent/src/tiktoken/target/release/deps/libbit_vec-de2a0d1e2ef2490a.rmeta --cap-lints allow`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/libc-42fd5088387abf7a/build-script-build`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/proc-macro2-4463b02a2f05d75c/build-script-build`\n",
        "   Compiling memoffset v0.9.1\n",
        "     Running `rustc --crate-name build_script_build --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memoffset-0.9.1/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"unstable_const\", \"unstable_offset_of\"))' -C metadata=679ebbc3261d6845 -C extra-filename=-679ebbc3261d6845 --out-dir /home/trent/src/tiktoken/target/release/build/memoffset-679ebbc3261d6845 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern autocfg=/home/trent/src/tiktoken/target/release/deps/libautocfg-b8e4c5d316ce5bfb.rlib --cap-lints allow`\n",
        "     Running `rustc --crate-name libc --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/libc-0.2.169/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"align\", \"const-extern-fn\", \"default\", \"extra_traits\", \"rustc-dep-of-std\", \"rustc-std-workspace-core\", \"std\", \"use_std\"))' -C metadata=6b0fbe5bd5ba9d30 -C extra-filename=-6b0fbe5bd5ba9d30 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow --cfg freebsd11 --cfg libc_const_extern_fn --check-cfg 'cfg(emscripten_new_stat_abi)' --check-cfg 'cfg(espidf_time32)' --check-cfg 'cfg(freebsd10)' --check-cfg 'cfg(freebsd11)' --check-cfg 'cfg(freebsd12)' --check-cfg 'cfg(freebsd13)' --check-cfg 'cfg(freebsd14)' --check-cfg 'cfg(freebsd15)' --check-cfg 'cfg(libc_const_extern_fn)' --check-cfg 'cfg(libc_deny_warnings)' --check-cfg 'cfg(libc_thread_local)' --check-cfg 'cfg(libc_ctest)' --check-cfg 'cfg(target_os,values(\"switch\",\"aix\",\"ohos\",\"hurd\",\"rtems\",\"visionos\",\"nuttx\"))' --check-cfg 'cfg(target_env,values(\"illumos\",\"wasi\",\"aix\",\"ohos\"))' --check-cfg 'cfg(target_arch,values(\"loongarch64\",\"mips32r6\",\"mips64r6\",\"csky\"))'`\n",
        "     Running `rustc --crate-name proc_macro2 --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/proc-macro2-1.0.92/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"nightly\", \"proc-macro\", \"span-locations\"))' -C metadata=4c69c42d9df03375 -C extra-filename=-4c69c42d9df03375 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern unicode_ident=/home/trent/src/tiktoken/target/release/deps/libunicode_ident-17278dafa5f26de9.rmeta --cap-lints allow --cfg wrap_proc_macro --check-cfg 'cfg(fuzzing)' --check-cfg 'cfg(no_is_available)' --check-cfg 'cfg(no_literal_byte_character)' --check-cfg 'cfg(no_literal_c_string)' --check-cfg 'cfg(no_source_text)' --check-cfg 'cfg(proc_macro_span)' --check-cfg 'cfg(procmacro2_backtrace)' --check-cfg 'cfg(procmacro2_nightly_testing)' --check-cfg 'cfg(procmacro2_semver_exempt)' --check-cfg 'cfg(randomize_layout)' --check-cfg 'cfg(span_locations)' --check-cfg 'cfg(super_unstable)' --check-cfg 'cfg(wrap_proc_macro)'`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/target-lexicon-826df8be4fa9ef21/build-script-build`\n",
        "     Running `rustc --crate-name target_lexicon --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/target-lexicon-0.12.16/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"arch_zkasm\", \"default\", \"serde\", \"serde_support\", \"std\"))' -C metadata=a879275207ec599a -C extra-filename=-a879275207ec599a --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow --cfg 'feature=\"rust_1_40\"'`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/memoffset-679ebbc3261d6845/build-script-build`\n",
        "     Running `rustc --crate-name memoffset --edition=2015 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/memoffset-0.9.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"unstable_const\", \"unstable_offset_of\"))' -C metadata=65fe1a8a113b3005 -C extra-filename=-65fe1a8a113b3005 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --cap-lints allow --cfg tuple_ty --cfg allow_clippy --cfg maybe_uninit --cfg doctests --cfg raw_ref_macros --cfg stable_const --cfg stable_offset_of`\n",
        "   Compiling aho-corasick v1.1.3\n",
        "     Running `rustc --crate-name aho_corasick --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aho-corasick-1.1.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"std\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"perf-literal\", \"std\"))' -C metadata=6668c9f838fb89b3 -C extra-filename=-6668c9f838fb89b3 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --cap-lints allow`\n",
        "   Compiling pyo3-build-config v0.23.3\n",
        "     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-build-config-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"resolve-config\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"python3-dll-a\", \"resolve-config\"))' -C metadata=d8ab86bb094cf645 -C extra-filename=-d8ab86bb094cf645 --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-build-config-d8ab86bb094cf645 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern target_lexicon=/home/trent/src/tiktoken/target/release/deps/libtarget_lexicon-a879275207ec599a.rlib --cap-lints allow`\n",
        "   Compiling quote v1.0.38\n",
        "     Running `rustc --crate-name quote --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/quote-1.0.38/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"proc-macro\"))' -C metadata=169332b6fe3d21d6 -C extra-filename=-169332b6fe3d21d6 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rmeta --cap-lints allow`\n",
        "   Compiling syn v2.0.95\n",
        "     Running `rustc --crate-name syn --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/syn-2.0.95/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"clone-impls\"' --cfg 'feature=\"default\"' --cfg 'feature=\"derive\"' --cfg 'feature=\"extra-traits\"' --cfg 'feature=\"full\"' --cfg 'feature=\"parsing\"' --cfg 'feature=\"printing\"' --cfg 'feature=\"proc-macro\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"clone-impls\", \"default\", \"derive\", \"extra-traits\", \"fold\", \"full\", \"parsing\", \"printing\", \"proc-macro\", \"test\", \"visit\", \"visit-mut\"))' -C metadata=29d4a0ddbd98f61f -C extra-filename=-29d4a0ddbd98f61f --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rmeta --extern quote=/home/trent/src/tiktoken/target/release/deps/libquote-169332b6fe3d21d6.rmeta --extern unicode_ident=/home/trent/src/tiktoken/target/release/deps/libunicode_ident-17278dafa5f26de9.rmeta --cap-lints allow`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/pyo3-build-config-d8ab86bb094cf645/build-script-build`\n",
        "     Running `rustc --crate-name pyo3_build_config --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-build-config-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"resolve-config\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"python3-dll-a\", \"resolve-config\"))' -C metadata=b884267014f5753b -C extra-filename=-b884267014f5753b --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern once_cell=/home/trent/src/tiktoken/target/release/deps/libonce_cell-6870d82906744d65.rmeta --extern target_lexicon=/home/trent/src/tiktoken/target/release/deps/libtarget_lexicon-a879275207ec599a.rmeta --cap-lints allow`\n",
        "   Compiling pyo3-ffi v0.23.3\n",
        "   Compiling pyo3-macros-backend v0.23.3\n",
        "   Compiling pyo3 v0.23.3\n",
        "     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-ffi-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"generate-import-lib\"))' -C metadata=5c5f8f108a22b6ae -C extra-filename=-5c5f8f108a22b6ae --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-ffi-5c5f8f108a22b6ae -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rlib --cap-lints allow`\n",
        "     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-macros-backend-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"experimental-async\"))' -C metadata=bb4a4b3911c85e51 -C extra-filename=-bb4a4b3911c85e51 --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-macros-backend-bb4a4b3911c85e51 -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rlib --cap-lints allow`\n",
        "     Running `rustc --crate-name build_script_build --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.23.3/build.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type bin --emit=dep-info,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"pyo3-macros\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"anyhow\", \"auto-initialize\", \"chrono\", \"chrono-tz\", \"default\", \"either\", \"experimental-async\", \"experimental-inspect\", \"extension-module\", \"eyre\", \"full\", \"generate-import-lib\", \"hashbrown\", \"indexmap\", \"indoc\", \"inventory\", \"macros\", \"multiple-pymethods\", \"nightly\", \"num-bigint\", \"num-complex\", \"num-rational\", \"py-clone\", \"pyo3-macros\", \"rust_decimal\", \"serde\", \"smallvec\", \"unindent\"))' -C metadata=65f0dbfaaf67ed5a -C extra-filename=-65f0dbfaaf67ed5a --out-dir /home/trent/src/tiktoken/target/release/build/pyo3-65f0dbfaaf67ed5a -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rlib --cap-lints allow`\n",
        "   Compiling regex-automata v0.4.9\n",
        "     Running `rustc --crate-name regex_automata --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-automata-0.4.9/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"dfa\"' --cfg 'feature=\"dfa-build\"' --cfg 'feature=\"dfa-onepass\"' --cfg 'feature=\"dfa-search\"' --cfg 'feature=\"hybrid\"' --cfg 'feature=\"meta\"' --cfg 'feature=\"nfa\"' --cfg 'feature=\"nfa-backtrack\"' --cfg 'feature=\"nfa-pikevm\"' --cfg 'feature=\"nfa-thompson\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-literal-multisubstring\"' --cfg 'feature=\"perf-literal-substring\"' --cfg 'feature=\"std\"' --cfg 'feature=\"syntax\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --cfg 'feature=\"unicode-word-boundary\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"dfa\", \"dfa-build\", \"dfa-onepass\", \"dfa-search\", \"hybrid\", \"internal-instrument\", \"internal-instrument-pikevm\", \"logging\", \"meta\", \"nfa\", \"nfa-backtrack\", \"nfa-pikevm\", \"nfa-thompson\", \"perf\", \"perf-inline\", \"perf-literal\", \"perf-literal-multisubstring\", \"perf-literal-substring\", \"std\", \"syntax\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unicode-word-boundary\"))' -C metadata=01135b6ed6d4413e -C extra-filename=-01135b6ed6d4413e --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern aho_corasick=/home/trent/src/tiktoken/target/release/deps/libaho_corasick-6668c9f838fb89b3.rmeta --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --extern regex_syntax=/home/trent/src/tiktoken/target/release/deps/libregex_syntax-66f570e05dbe3825.rmeta --cap-lints allow`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/pyo3-macros-backend-bb4a4b3911c85e51/build-script-build`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/pyo3-ffi-5c5f8f108a22b6ae/build-script-build`\n",
        "     Running `/home/trent/src/tiktoken/target/release/build/pyo3-65f0dbfaaf67ed5a/build-script-build`\n",
        "     Running `rustc --crate-name pyo3_ffi --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-ffi-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' --cfg 'feature=\"default\"' --cfg 'feature=\"extension-module\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"default\", \"extension-module\", \"generate-import-lib\"))' -C metadata=9ad2a4f2678f35bd -C extra-filename=-9ad2a4f2678f35bd --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern libc=/home/trent/src/tiktoken/target/release/deps/liblibc-6b0fbe5bd5ba9d30.rmeta --cap-lints allow --cfg Py_3_7 --cfg Py_3_8 --cfg Py_3_9 --cfg Py_3_10 --cfg Py_3_11 --cfg Py_3_12 --cfg Py_3_13 --cfg Py_GIL_DISABLED --cfg rustc_has_once_lock --cfg invalid_from_utf8_lint --cfg c_str_lit --cfg diagnostic_namespace --check-cfg 'cfg(Py_LIMITED_API)' --check-cfg 'cfg(Py_GIL_DISABLED)' --check-cfg 'cfg(PyPy)' --check-cfg 'cfg(GraalPy)' --check-cfg 'cfg(py_sys_config, values(\"Py_DEBUG\", \"Py_REF_DEBUG\", \"Py_TRACE_REFS\", \"COUNT_ALLOCS\"))' --check-cfg 'cfg(invalid_from_utf8_lint)' --check-cfg 'cfg(pyo3_disable_reference_pool)' --check-cfg 'cfg(pyo3_leak_on_drop_without_reference_pool)' --check-cfg 'cfg(diagnostic_namespace)' --check-cfg 'cfg(c_str_lit)' --check-cfg 'cfg(rustc_has_once_lock)' --check-cfg 'cfg(Py_3_7)' --check-cfg 'cfg(Py_3_8)' --check-cfg 'cfg(Py_3_9)' --check-cfg 'cfg(Py_3_10)' --check-cfg 'cfg(Py_3_11)' --check-cfg 'cfg(Py_3_12)' --check-cfg 'cfg(Py_3_13)'`\n",
        "     Running `rustc --crate-name pyo3_macros_backend --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-macros-backend-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"experimental-async\"))' -C metadata=c7424188824c71f8 -C extra-filename=-c7424188824c71f8 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern heck=/home/trent/src/tiktoken/target/release/deps/libheck-5e22b1dffa7f4255.rmeta --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rmeta --extern pyo3_build_config=/home/trent/src/tiktoken/target/release/deps/libpyo3_build_config-b884267014f5753b.rmeta --extern quote=/home/trent/src/tiktoken/target/release/deps/libquote-169332b6fe3d21d6.rmeta --extern syn=/home/trent/src/tiktoken/target/release/deps/libsyn-29d4a0ddbd98f61f.rmeta --cap-lints allow --cfg rustc_has_once_lock --cfg invalid_from_utf8_lint --cfg c_str_lit --cfg diagnostic_namespace --check-cfg 'cfg(Py_LIMITED_API)' --check-cfg 'cfg(Py_GIL_DISABLED)' --check-cfg 'cfg(PyPy)' --check-cfg 'cfg(GraalPy)' --check-cfg 'cfg(py_sys_config, values(\"Py_DEBUG\", \"Py_REF_DEBUG\", \"Py_TRACE_REFS\", \"COUNT_ALLOCS\"))' --check-cfg 'cfg(invalid_from_utf8_lint)' --check-cfg 'cfg(pyo3_disable_reference_pool)' --check-cfg 'cfg(pyo3_leak_on_drop_without_reference_pool)' --check-cfg 'cfg(diagnostic_namespace)' --check-cfg 'cfg(c_str_lit)' --check-cfg 'cfg(rustc_has_once_lock)' --check-cfg 'cfg(Py_3_7)' --check-cfg 'cfg(Py_3_8)' --check-cfg 'cfg(Py_3_9)' --check-cfg 'cfg(Py_3_10)' --check-cfg 'cfg(Py_3_11)' --check-cfg 'cfg(Py_3_12)' --check-cfg 'cfg(Py_3_13)'`\n",
        "   Compiling fancy-regex v0.13.0\n",
        "   Compiling regex v1.11.1\n",
        "   Compiling bstr v1.11.3\n",
        "     Running `rustc --crate-name regex --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/regex-1.11.1/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"perf-backtrack\"' --cfg 'feature=\"perf-cache\"' --cfg 'feature=\"perf-dfa\"' --cfg 'feature=\"perf-inline\"' --cfg 'feature=\"perf-literal\"' --cfg 'feature=\"perf-onepass\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --cfg 'feature=\"unicode-age\"' --cfg 'feature=\"unicode-bool\"' --cfg 'feature=\"unicode-case\"' --cfg 'feature=\"unicode-gencat\"' --cfg 'feature=\"unicode-perl\"' --cfg 'feature=\"unicode-script\"' --cfg 'feature=\"unicode-segment\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"logging\", \"pattern\", \"perf\", \"perf-backtrack\", \"perf-cache\", \"perf-dfa\", \"perf-dfa-full\", \"perf-inline\", \"perf-literal\", \"perf-onepass\", \"std\", \"unicode\", \"unicode-age\", \"unicode-bool\", \"unicode-case\", \"unicode-gencat\", \"unicode-perl\", \"unicode-script\", \"unicode-segment\", \"unstable\", \"use_std\"))' -C metadata=9ab83d1dbb2872e1 -C extra-filename=-9ab83d1dbb2872e1 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern aho_corasick=/home/trent/src/tiktoken/target/release/deps/libaho_corasick-6668c9f838fb89b3.rmeta --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --extern regex_automata=/home/trent/src/tiktoken/target/release/deps/libregex_automata-01135b6ed6d4413e.rmeta --extern regex_syntax=/home/trent/src/tiktoken/target/release/deps/libregex_syntax-66f570e05dbe3825.rmeta --cap-lints allow`\n",
        "     Running `rustc --crate-name fancy_regex --edition=2018 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/fancy-regex-0.13.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"perf\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"default\", \"perf\", \"std\", \"track_caller\", \"unicode\"))' -C metadata=2bd42caf041ad10c -C extra-filename=-2bd42caf041ad10c --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern bit_set=/home/trent/src/tiktoken/target/release/deps/libbit_set-40d90f83eb5bab57.rmeta --extern regex_automata=/home/trent/src/tiktoken/target/release/deps/libregex_automata-01135b6ed6d4413e.rmeta --extern regex_syntax=/home/trent/src/tiktoken/target/release/deps/libregex_syntax-66f570e05dbe3825.rmeta --cap-lints allow`\n",
        "     Running `rustc --crate-name bstr --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bstr-1.11.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' --cfg 'feature=\"unicode\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"alloc\", \"default\", \"serde\", \"std\", \"unicode\"))' -C metadata=cc96e78f4e9fd97f -C extra-filename=-cc96e78f4e9fd97f --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern memchr=/home/trent/src/tiktoken/target/release/deps/libmemchr-25fa9792dd9399b0.rmeta --extern regex_automata=/home/trent/src/tiktoken/target/release/deps/libregex_automata-01135b6ed6d4413e.rmeta --cap-lints allow`\n",
        "   Compiling pyo3-macros v0.23.3\n",
        "     Running `rustc --crate-name pyo3_macros --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-macros-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type proc-macro --emit=dep-info,link -C prefer-dynamic -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' -C debug-assertions=off --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"experimental-async\", \"multiple-pymethods\"))' -C metadata=97a13ca0f34dda72 -C extra-filename=-97a13ca0f34dda72 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern proc_macro2=/home/trent/src/tiktoken/target/release/deps/libproc_macro2-4c69c42d9df03375.rlib --extern pyo3_macros_backend=/home/trent/src/tiktoken/target/release/deps/libpyo3_macros_backend-c7424188824c71f8.rlib --extern quote=/home/trent/src/tiktoken/target/release/deps/libquote-169332b6fe3d21d6.rlib --extern syn=/home/trent/src/tiktoken/target/release/deps/libsyn-29d4a0ddbd98f61f.rlib --extern proc_macro --cap-lints allow`\n",
        "     Running `rustc --crate-name pyo3 --edition=2021 /home/trent/.cargo/registry/src/index.crates.io-6f17d22bba15001f/pyo3-0.23.3/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -C embed-bitcode=no --warn=rust_2018_idioms '--warn=clippy::useless_transmute' '--warn=clippy::used_underscore_binding' --warn=unused_lifetimes '--warn=clippy::unnecessary_wraps' '--warn=clippy::todo' --warn=rust_2021_prelude_collisions '--warn=clippy::manual_ok_or' '--warn=clippy::manual_assert' '--warn=clippy::let_unit_value' --warn=invalid_doc_attributes '--warn=clippy::flat_map_option' '--warn=clippy::filter_map_next' '--warn=clippy::explicit_iter_loop' '--warn=clippy::explicit_into_iter_loop' --warn=elided_lifetimes_in_paths '--warn=clippy::dbg_macro' '--warn=clippy::checked_conversions' '--warn=rustdoc::broken_intra_doc_links' '--warn=rustdoc::bare_urls' --cfg 'feature=\"extension-module\"' --cfg 'feature=\"indoc\"' --cfg 'feature=\"macros\"' --cfg 'feature=\"pyo3-macros\"' --cfg 'feature=\"unindent\"' --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values(\"abi3\", \"abi3-py310\", \"abi3-py311\", \"abi3-py312\", \"abi3-py37\", \"abi3-py38\", \"abi3-py39\", \"anyhow\", \"auto-initialize\", \"chrono\", \"chrono-tz\", \"default\", \"either\", \"experimental-async\", \"experimental-inspect\", \"extension-module\", \"eyre\", \"full\", \"generate-import-lib\", \"hashbrown\", \"indexmap\", \"indoc\", \"inventory\", \"macros\", \"multiple-pymethods\", \"nightly\", \"num-bigint\", \"num-complex\", \"num-rational\", \"py-clone\", \"pyo3-macros\", \"rust_decimal\", \"serde\", \"smallvec\", \"unindent\"))' -C metadata=99d0b007138eadf4 -C extra-filename=-99d0b007138eadf4 --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern cfg_if=/home/trent/src/tiktoken/target/release/deps/libcfg_if-e4ded2c19830fbdd.rmeta --extern indoc=/home/trent/src/tiktoken/target/release/deps/libindoc-b4e94d9ecbd21a39.so --extern libc=/home/trent/src/tiktoken/target/release/deps/liblibc-6b0fbe5bd5ba9d30.rmeta --extern memoffset=/home/trent/src/tiktoken/target/release/deps/libmemoffset-65fe1a8a113b3005.rmeta --extern once_cell=/home/trent/src/tiktoken/target/release/deps/libonce_cell-05003007543cfa87.rmeta --extern pyo3_ffi=/home/trent/src/tiktoken/target/release/deps/libpyo3_ffi-9ad2a4f2678f35bd.rmeta --extern pyo3_macros=/home/trent/src/tiktoken/target/release/deps/libpyo3_macros-97a13ca0f34dda72.so --extern unindent=/home/trent/src/tiktoken/target/release/deps/libunindent-d53e2a0385a47a80.rmeta --cap-lints allow --cfg Py_3_7 --cfg Py_3_8 --cfg Py_3_9 --cfg Py_3_10 --cfg Py_3_11 --cfg Py_3_12 --cfg Py_3_13 --cfg Py_GIL_DISABLED --cfg rustc_has_once_lock --cfg invalid_from_utf8_lint --cfg c_str_lit --cfg diagnostic_namespace --check-cfg 'cfg(Py_LIMITED_API)' --check-cfg 'cfg(Py_GIL_DISABLED)' --check-cfg 'cfg(PyPy)' --check-cfg 'cfg(GraalPy)' --check-cfg 'cfg(py_sys_config, values(\"Py_DEBUG\", \"Py_REF_DEBUG\", \"Py_TRACE_REFS\", \"COUNT_ALLOCS\"))' --check-cfg 'cfg(invalid_from_utf8_lint)' --check-cfg 'cfg(pyo3_disable_reference_pool)' --check-cfg 'cfg(pyo3_leak_on_drop_without_reference_pool)' --check-cfg 'cfg(diagnostic_namespace)' --check-cfg 'cfg(c_str_lit)' --check-cfg 'cfg(rustc_has_once_lock)' --check-cfg 'cfg(Py_3_7)' --check-cfg 'cfg(Py_3_8)' --check-cfg 'cfg(Py_3_9)' --check-cfg 'cfg(Py_3_10)' --check-cfg 'cfg(Py_3_11)' --check-cfg 'cfg(Py_3_12)' --check-cfg 'cfg(Py_3_13)'`\n",
        "       Dirty tiktoken v0.8.0 (/home/trent/src/tiktoken): the toolchain changed\n",
        "   Compiling tiktoken v0.8.0 (/home/trent/src/tiktoken)\n",
        "     Running `rustc --crate-name _tiktoken --edition=2021 src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts,future-incompat --diagnostic-width=254 --crate-type cdylib --emit=dep-info,link -C opt-level=3 -C embed-bitcode=no --check-cfg 'cfg(docsrs)' --check-cfg 'cfg(feature, values())' -C metadata=2d15c1d1b98ec97b --out-dir /home/trent/src/tiktoken/target/release/deps -C linker=/home/trent/mambaforge/envs/py313t/bin/x86_64-conda-linux-gnu-cc -C strip=debuginfo -L dependency=/home/trent/src/tiktoken/target/release/deps --extern bstr=/home/trent/src/tiktoken/target/release/deps/libbstr-cc96e78f4e9fd97f.rlib --extern fancy_regex=/home/trent/src/tiktoken/target/release/deps/libfancy_regex-2bd42caf041ad10c.rlib --extern pyo3=/home/trent/src/tiktoken/target/release/deps/libpyo3-99d0b007138eadf4.rlib --extern regex=/home/trent/src/tiktoken/target/release/deps/libregex-9ab83d1dbb2872e1.rlib --extern rustc_hash=/home/trent/src/tiktoken/target/release/deps/librustc_hash-b006f4d81e95dfaf.rlib`\n",
        "warning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n",
        "   --> src/lib.rs:508:16\n",
        "    |\n",
        "508 |         buffer.into_py(py)\n",
        "    |                ^^^^^^^\n",
        "    |\n",
        "    = note: `#[warn(deprecated)]` on by default\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n",
        "   --> src/lib.rs:555:38\n",
        "    |\n",
        "555 |         let py_completions = PyList::new_bound(\n",
        "    |                                      ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n",
        "   --> src/lib.rs:559:36\n",
        "    |\n",
        "559 |                 .map(|seq| PyList::new_bound(py, &seq[..])),\n",
        "    |                                    ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n",
        "   --> src/lib.rs:561:34\n",
        "    |\n",
        "561 |         (tokens, py_completions).into_py(py)\n",
        "    |                                  ^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:589:38\n",
        "    |\n",
        "589 |             Ok(bytes) => Ok(PyBytes::new_bound(py, &bytes).into()),\n",
        "    |                                      ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:596:32\n",
        "    |\n",
        "596 |             return Ok(PyBytes::new_bound(py, bytes).into());\n",
        "    |                                ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:599:32\n",
        "    |\n",
        "599 |             return Ok(PyBytes::new_bound(py, bytes).into());\n",
        "    |                                ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:611:31\n",
        "    |\n",
        "611 |             .map(|x| PyBytes::new_bound(py, x).into())\n",
        "    |                               ^^^^^^^^^\n",
        "\n",
        "warning: `tiktoken` (lib) generated 8 warnings\n",
        "    Finished `release` profile [optimized] target(s) in 13.17s\n",
        "Copying rust artifact from target/release/lib_tiktoken.so to build/lib.linux-x86_64-cpython-313t/tiktoken/_tiktoken.cpython-313t-x86_64-linux-gnu.so\n",
        "(py313t) {pytorch} [12.6] [trent@dgx/ttypts/3(~s/tiktoken)%] python setup.py install\n",
        "running install\n",
        "/home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/setuptools/_distutils/cmd.py:79: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
        "!!\n",
        "\n",
        "        ********************************************************************************\n",
        "        Please avoid running ``setup.py`` directly.\n",
        "        Instead, use pypa/build, pypa/installer or other\n",
        "        standards-based tools.\n",
        "\n",
        "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
        "        ********************************************************************************\n",
        "\n",
        "!!\n",
        "  self.initialize_options()\n",
        "running build\n",
        "running build_py\n",
        "copying tiktoken/_educational.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/__init__.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/registry.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/model.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/load.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken/core.py -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "copying tiktoken_ext/openai_public.py -> build/lib.linux-x86_64-cpython-313t/tiktoken_ext\n",
        "running egg_info\n",
        "writing tiktoken.egg-info/PKG-INFO\n",
        "writing dependency_links to tiktoken.egg-info/dependency_links.txt\n",
        "writing requirements to tiktoken.egg-info/requires.txt\n",
        "writing top-level names to tiktoken.egg-info/top_level.txt\n",
        "reading manifest file 'tiktoken.egg-info/SOURCES.txt'\n",
        "reading manifest template 'MANIFEST.in'\n",
        "warning: no files found matching 'Makefile'\n",
        "adding license file 'LICENSE'\n",
        "writing manifest file 'tiktoken.egg-info/SOURCES.txt'\n",
        "copying tiktoken/py.typed -> build/lib.linux-x86_64-cpython-313t/tiktoken\n",
        "running build_ext\n",
        "running build_rust\n",
        "cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib --\n",
        "       Fresh target-lexicon v0.12.16\n",
        "       Fresh unicode-ident v1.0.14\n",
        "       Fresh memchr v2.7.4\n",
        "       Fresh regex-syntax v0.8.5\n",
        "       Fresh autocfg v1.4.0\n",
        "       Fresh aho-corasick v1.1.3\n",
        "       Fresh heck v0.5.0\n",
        "       Fresh bit-vec v0.6.3\n",
        "       Fresh indoc v2.0.5\n",
        "       Fresh cfg-if v1.0.0\n",
        "       Fresh once_cell v1.20.2\n",
        "       Fresh unindent v0.2.3\n",
        "       Fresh rustc-hash v1.1.0\n",
        "       Fresh proc-macro2 v1.0.92\n",
        "       Fresh regex-automata v0.4.9\n",
        "       Fresh libc v0.2.169\n",
        "       Fresh bit-set v0.5.3\n",
        "       Fresh pyo3-build-config v0.23.3\n",
        "       Fresh quote v1.0.38\n",
        "       Fresh memoffset v0.9.1\n",
        "       Fresh bstr v1.11.3\n",
        "       Fresh fancy-regex v0.13.0\n",
        "       Fresh regex v1.11.1\n",
        "       Fresh syn v2.0.95\n",
        "       Fresh pyo3-macros-backend v0.23.3\n",
        "       Fresh pyo3-ffi v0.23.3\n",
        "       Fresh pyo3-macros v0.23.3\n",
        "       Fresh pyo3 v0.23.3\n",
        "       Fresh tiktoken v0.8.0 (/home/trent/src/tiktoken)\n",
        "warning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n",
        "   --> src/lib.rs:508:16\n",
        "    |\n",
        "508 |         buffer.into_py(py)\n",
        "    |                ^^^^^^^\n",
        "    |\n",
        "    = note: `#[warn(deprecated)]` on by default\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n",
        "   --> src/lib.rs:555:38\n",
        "    |\n",
        "555 |         let py_completions = PyList::new_bound(\n",
        "    |                                      ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyList::new_bound`: renamed to `PyList::new`\n",
        "   --> src/lib.rs:559:36\n",
        "    |\n",
        "559 |                 .map(|seq| PyList::new_bound(py, &seq[..])),\n",
        "    |                                    ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated method `pyo3::IntoPy::into_py`: `IntoPy` is going to be replaced by `IntoPyObject`. See the migration guide (https://pyo3.rs/v0.23.0/migration) for more information.\n",
        "   --> src/lib.rs:561:34\n",
        "    |\n",
        "561 |         (tokens, py_completions).into_py(py)\n",
        "    |                                  ^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:589:38\n",
        "    |\n",
        "589 |             Ok(bytes) => Ok(PyBytes::new_bound(py, &bytes).into()),\n",
        "    |                                      ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:596:32\n",
        "    |\n",
        "596 |             return Ok(PyBytes::new_bound(py, bytes).into());\n",
        "    |                                ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:599:32\n",
        "    |\n",
        "599 |             return Ok(PyBytes::new_bound(py, bytes).into());\n",
        "    |                                ^^^^^^^^^\n",
        "\n",
        "warning: use of deprecated associated function `pyo3::types::PyBytes::new_bound`: renamed to `PyBytes::new`\n",
        "   --> src/lib.rs:611:31\n",
        "    |\n",
        "611 |             .map(|x| PyBytes::new_bound(py, x).into())\n",
        "    |                               ^^^^^^^^^\n",
        "\n",
        "warning: `tiktoken` (lib) generated 8 warnings\n",
        "    Finished `release` profile [optimized] target(s) in 0.03s\n",
        "Copying rust artifact from target/release/lib_tiktoken.so to build/lib.linux-x86_64-cpython-313t/tiktoken/_tiktoken.cpython-313t-x86_64-linux-gnu.so\n",
        "running install_lib\n",
        "creating /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken_ext\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken_ext/openai_public.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken_ext\n",
        "creating /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/_educational.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/__init__.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/py.typed -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/registry.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/model.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/load.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/_tiktoken.cpython-313t-x86_64-linux-gnu.so -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "copying build/lib.linux-x86_64-cpython-313t/tiktoken/core.py -> /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken\n",
        "byte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken_ext/openai_public.py to openai_public.cpython-313.pyc\n",
        "byte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/_educational.py to _educational.cpython-313.pyc\n",
        "byte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/__init__.py to __init__.cpython-313.pyc\n",
        "byte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/registry.py to registry.cpython-313.pyc\n",
        "byte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/model.py to model.cpython-313.pyc\n",
        "byte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/load.py to load.cpython-313.pyc\n",
        "byte-compiling /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken/core.py to core.cpython-313.pyc\n",
        "running install_egg_info\n",
        "Copying tiktoken.egg-info to /home/trent/mambaforge/envs/py313t/lib/python3.13t/site-packages/tiktoken-0.8.0-py3.13.egg-info\n",
        "running install_scripts\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "After this, you should be able to import the `tiktoken` module in Python:\n",
        "\n",
        "```bash\n",
        "% cd ..\n",
        "% python -Xgil=0\n",
        "Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux\n",
        "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
        ">>> import tiktoken\n",
        ">>>\n",
        "```\n",
        "\n",
        "#### Torch\n",
        "\n",
        "Install PyTorch 2.6 via `pip` with the conda `py313t` environment active:\n",
        "\n",
        "```bash\n",
        "pip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126\n",
        "```\n",
        "\n",
        "If you have trouble installing PyTorch, consult their [Getting Started](\n",
        "https://pytorch.org/get-started/locally/) guide.\n",
        "\n",
        "You can verify torch installed correctly as follows:\n",
        "\n",
        "```bash\n",
        "% python -Xgil=0\n",
        "Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux\n",
        "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
        ">>> import torch\n",
        ">>> torch.cuda.is_available()\n",
        "True\n",
        "```\n",
        "\n",
        "#### IPython Kernel\n",
        "\n",
        "Installing IPython Kernel allows us to use our free-threaded Python\n",
        "installation via the Jupyter Lab instance we install in the `py313`\n",
        "environment.\n",
        "\n",
        "```bash\n",
        "conda activate py313t\n",
        "pip install ipykernel\n",
        "```\n",
        "\n",
        "Once `ipykernel` is installed, run the following:\n",
        "\n",
        "```bash\n",
        "% python3.13t -m ipykernel --install --name py313t --user\n",
        "Installed kernelspec py313t in /home/trent/.local/share/jupyter/kernels/py313t\n",
        "```\n",
        "\n",
        "This will install a kernel configuration in\n",
        "`~/.local/jupyter/share/kernels/py313t`, which we then need to tweak by adding\n",
        "the `-Xgil=0` startup flag:\n",
        "\n",
        "```bash\n",
        "% cd ~/.local/jupyter/share/kernels/py313t\n",
        "% cp kernel.json kernel.json.orig\n",
        "% vi kernel.json\n",
        "# Edit kernel.json to make it look like the diff below.\n",
        "```\n",
        "\n",
        "```diff\n",
        "--- kernel.json.orig    2025-02-04 15:02:21.814112004 -0800\n",
        "+++ kernel.json 2025-02-04 15:02:36.553806199 -0800\n",
        "@@ -1,6 +1,7 @@\n",
        " {\n",
        "  \"argv\": [\n",
        "   \"/home/trent/mambaforge/envs/py313t/bin/python3.13t\",\n",
        "+  \"-Xgil=0\",\n",
        "   \"-Xfrozen_modules=off\",\n",
        "   \"-m\",\n",
        "   \"ipykernel_launcher\",\n",
        "@@ -12,4 +13,4 @@\n",
        "  \"metadata\": {\n",
        "   \"debugger\": true\n",
        "  }\n",
        "```\n",
        "\n",
        "#### Datrie and Cython\n",
        "\n",
        "[datrie](https://github.com/pytries/datrie) is a Python library that provides\n",
        "a *trie* (or *digital search tree*) data structure by way of the [libdatrie](\n",
        "https://linux.thai.net/~thep/datrie/datrie.html) C library.  The Python\n",
        "`datrie` library isn't strictly necessary to run `parallelopedia.gpt2`, but\n",
        "other components rely on it, so it's handy to get installed now, if possible.\n",
        "\n",
        "It relies upon Cython, and thus, for now, you need to install a free-threaded\n",
        "compatible version of Cython first, as follows:\n",
        "\n",
        "```bash\n",
        "conda activate py313t\n",
        "pip install git+https://github.com/cython/cython\n",
        "```\n",
        "Then, clone the `datrie` repo and install as follows:\n",
        "\n",
        "```bash\n",
        "conda activate py313t\n",
        "git clone https://github.com/pytries/datrie --recursive\n",
        "cd datrie\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "```\n",
        "\n",
        "If everything goes well, you should see something like this when you launch\n",
        "Python and import `datrie`:\n",
        "\n",
        "```bash\n",
        "% python\n",
        "Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux\n",
        "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
        ">>> import datrie\n",
        ">>>\n",
        "```\n",
        "\n",
        "### Normal 3.13 Env (py313)\n",
        "\n",
        "The second `py313` environment is almost identical to `py313t`, except it is\n",
        "not a `python-freethreading` installation, and, additionally, we install\n",
        "Jupyter Lab.  We can install `tiktoken` directly via `pip`, so we don't need\n",
        "the supporting Rust cruft.  Likewise for `datrie`, we don't need to first\n",
        "install Cython and then build `datrie` from git.\n",
        "\n",
        "```bash\n",
        "conda create -n py313 python=3.13 \\\n",
        "    nodejs pip tqdm flake8 jupyterlab \\\n",
        "        -c conda-forge\n",
        "conda activate py313\n",
        "pip install numpy datrie tiktoken\n",
        "pip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126\n",
        "```\n",
        "\n",
        "## Parallelopedia\n",
        "\n",
        "All of the code in this article is available in the [Parallelopedia](\n",
        "https://github.com/tpn/parallelopedia) repository on Github.  The code we'll\n",
        "be focusing on in this post lives in the [parallelopedia.gpt2](\n",
        "https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)\n",
        "module.\n",
        "\n",
        "Clone the repository as follows:\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/tpn/parallelopedia\n",
        "```\n",
        "\n",
        "The code and command examples in this post will assume you've added the\n",
        "`src` directory to your `PYTHONPATH`, and the `bin` directory to your `PATH`:\n",
        "\n",
        "```bash\n",
        "cd parallelopedia\n",
        "export PYTHONPATH=$(pwd)/src:$PYTHONPATH\n",
        "export PATH=$(pwd)/bin:$PATH\n",
        "\n",
        "```\n",
        "\n",
        "You can perform a quick sanity check that things are working as follows:\n",
        "\n",
        "```bash\n",
        "% python -Xgil=0 -m parallelopedia.http.server --help\n",
        "usage: server.py [-h] [--ip IP] [--port PORT] [--debug] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--threads THREADS] [--protocol-class PROTOCOL_CLASS] [--app-classes APP_CLASSES [APP_CLASSES ...]] [--listen-backlog LISTEN_BACKLOG]\n",
        "\n",
        "Run the HTTP server.\n",
        "\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "  --ip IP               IP address to bind the server to.\n",
        "  --port PORT           Port number to bind the server to.\n",
        "  --debug               Enable debug mode for asyncio.\n",
        "  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}\n",
        "                        Set the logging level.\n",
        "  --threads THREADS     Number of threads to use.\n",
        "  --protocol-class PROTOCOL_CLASS\n",
        "                        The protocol class to use for the server.\n",
        "  --app-classes APP_CLASSES [APP_CLASSES ...]\n",
        "                        Space-separated list of HTTP application classes.\n",
        "  --listen-backlog LISTEN_BACKLOG\n",
        "                        The listen backlog for the server.\n",
        "\n",
        "```\n",
        "\n",
        "## Parallelopedia Web Interface\n",
        "\n",
        "The React Bootstrap web interface lives in the [Parallelopedia-UI](\n",
        "https://github.com/tpn/parallelopedia-ui) repository.\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "Full disclaimer: I'm not a web developer.  I don't know JavaScript, React,\n",
        "or Bootstrap, or anything else in the modern web stack.  So, like any\n",
        "modern developer in 2025, when faced with needing to whip up something in an\n",
        "area I am not proficient, I farm it all out to AI---either ChatGPT, local\n",
        "LLMs via [LM Studio](https://lmstudio.ai/), or, more recently, [Aider](\n",
        "https://aider.chat).\n",
        "\n",
        "TL;DR: the web interface code probably sucks.\n",
        "\n",
        ":::\n",
        "\n",
        "Clone the repository as follows:\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/tpn/parallelopedia-ui\n",
        "```\n",
        "\n",
        "Both the `py313t` and `py313` environments included `nodejs`, so with either\n",
        "of them active, you should be able to change into that directory and run\n",
        "`npm run start` to start up the web interface:\n",
        "\n",
        "```bash\n",
        "conda activate py313\n",
        "cd parallelopedia-ui\n",
        "npm run start\n",
        "```\n",
        "\n",
        "That should launch a browser automatically to `http://localhost:3000/`, which\n",
        "should have a `GPT2` tab that, when selected, looks something like this:\n",
        "\n",
        "::: {.lightbox .theme-light}\n",
        "![Parallelopedia UI Example](images/parallelopedia-ui-gpt2-light.png)\n",
        ":::\n",
        "\n",
        "::: {.lightbox .theme-dark}\n",
        "![Parallelopedia UI Example](images/parallelopedia-ui-gpt2-dark.png)\n",
        ":::\n",
        "\n",
        "# Learning about LLMs\n",
        "\n",
        "I started at [OpenTeams](https://openteams.com) around the middle of November,\n",
        "2024 (last year), and joined the PyTorch team.  At the time I honestly knew\n",
        "nothing about PyTorch.  Nor deep neural networks.  Nor LLMs---other than\n",
        "knowing I thoroughly enjoyed using them, having been an avid ChatGPT user\n",
        "for a while now.\n",
        "\n",
        "Thanks to [Andrej Karpathy](https://karpathy.ai/)'s phenomenal YouTube series\n",
        "on deep neural networks and LLMs titled [Neural Networks: From Zero to Hero](\n",
        "https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ),\n",
        "featuring 19 hours, 21 minutes and two seconds of content across ten videos,\n",
        "over the course of about 3-4 weeks, I went from zero to... *hero* is too\n",
        "strong of a word---I'd say I at least made it to *not-completely-clueless*\n",
        "level with regards to my understandings of how LLMs work.  I was also soaking\n",
        "up as much PyTorch exposure as I could throughout this period, which is made\n",
        "easier given Andrej leverages PyTorch in later videos.\n",
        "\n",
        "The content is simply fantastic.  Andrej's is a great teacher, and he does a\n",
        "wonderful job of laying the foundation of modern neural networks, particularly\n",
        "with regard to the *bread-and-butter* concepts like auto-differentiation and\n",
        "back-propagation that fuel today's LLM AI models.\n",
        "\n",
        "None of the work presented in this post would have been possible had I not\n",
        "invested the time in Andrej's series.  If you're reading this Andrej, thanks,\n",
        "and keep up the brilliant work!\n",
        "\n",
        "## Training GPT-2 (124M) Locally\n",
        "\n",
        "Equipped with my new knowledge about LLMs, PyTorch, and, thanks to Andrej's\n",
        "final video in the series titled [Let's reproduce GPT-2 (124M)](\n",
        "https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)\n",
        "and the accompanying [build-nanogpt](\n",
        "https://github.com/karpathy/build-nanogpt) Github repo, I was able to train\n",
        "a local GPT-2 model via PyTorch, from scratch, using the [edu_fineweb10B](\n",
        "https://huggingface.co/rhysjones/gpt2-124M-edu-fineweb-10B) dataset.\n",
        "\n",
        "I only had to make one change in order to run locally:\n",
        "[Use 8 for micro batch size](https://github.com/tpn/build-nanogpt/commit/0069c4a35d1a362c1fd50f9f5bce00a170e15904).\n",
        "With that change in place, I was able to train GPT-2 from scratch as follows:\n",
        "\n",
        "```bash\n",
        "conda activate py313\n",
        "git clone gh:tpn/build-nanogpt\n",
        "cd build-nanogpt\n",
        "# Download the fineweb dataset.\n",
        "python fineweb.py\n",
        "# Train!\n",
        "torchrun --standalone --nproc_per_node=4 train_gpt2.py\n",
        "```\n",
        "\n",
        "This was run on an NVIDIA DGX workstation from 2017, which has an Intel\n",
        "Xeon(R) CPU E5-2698 v4 @ 2.20GHz (20 cores, 40 threads), and four Tesla\n",
        "V100-DGXS-32GB GPUs.\n",
        "\n",
        "Training in parallel across all four GPUs yielded around 36,000 tokens/sec,\n",
        "with an average time of about 14.5 seconds per loop iteration.  Training\n",
        "took about 3 days and 5 hours for 19,072 steps.  All four GPUs were pegged\n",
        "close to their 300W power limit for those three days.\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "Amusingly, well after the fact, I decided to see what kind of training\n",
        "performance I'd get on my Windows 11 gaming box, which has an AMD Ryzen 9\n",
        "7950X3D (16 cores, 32 threads) and NVIDIA RTX 4090.  Training via\n",
        "`python train_gpt2.py` (`torchrun` wasn't needed as I wasn't using multiple\n",
        "GPUs) yielded about 45,000 tokens/sec, which is a nice bump, but what was most\n",
        "impressive was the reduction to the loop iteration duration, which averaged\n",
        "out to about 180ms.\n",
        "\n",
        "So, I could have completed the same training process in about an hour or so,\n",
        "at a vastly reduced impact on my electricity bill that month :-)\n",
        "\n",
        ":::\n",
        "\n",
        "Once training completes, a `log/model_19072.pt` file is produced, which is\n",
        "the checkpoint of the model at that final step, obtained via a call to\n",
        "`torch.save()`.  The model has 124M parameters---which is tiny by modern\n",
        "standards---and is just under 500MB on disk.\n",
        "\n",
        "You can download that very model I trained via the HuggingFace dataset I set\n",
        "up here: [model_19072.pt](\n",
        "https://huggingface.co/datasets/trentnelson/parallelopedia-data-gpt2/blob/main/model_19072.pt).\n",
        "Once downloaded, place the file in `~/src/parallelopedia/data` (assuming you\n",
        "cloned the repo to `~/src/parallelopedia`; change as necessary), and that will\n",
        "be all that's required for the next steps.\n",
        "\n",
        "# Parallel PyTorch Inference with Free-Threaded Python\n",
        "\n",
        "Now we finally get to the fun part.  The primary objective I intended to\n",
        "tackle with this work was to investigate whether or not PyTorch model\n",
        "inference could be done simultaneously, by multiple threads, in a single\n",
        "free-threaded Python process, ideally with just a single GPU at minimum.\n",
        "\n",
        "I didn't focus on exploring free-threading and *training* PyTorch models.\n",
        "Mainly because it's a lot more complex, plus there's already a huge body of\n",
        "existing work in PyTorch that handles distributed training across multiple\n",
        "GPUs via `multiprocessing`.  On the flip side, having multiple threads do\n",
        "simultaneous parallel inference on a single model in a single Python process\n",
        "is novel territory: it hasn't been possible prior to now, because of the\n",
        "Python GIL.\n",
        "\n",
        "::: {.hidden}\n",
        "Additionally, parallel multi-threaded inference is particularly interesting\n",
        "because you should arguably be able to have an HTTP server servicing *\"chat\"*\n",
        "requests (i.e. like an OpenAI compatible REST API) in parallel with only one\n",
        "GPU.  (Although no reason multiple GPUs can't also be leveraged, as we'll\n",
        "find out.)\n",
        ":::\n",
        "\n",
        "## A Simple PyTorch GPT-2 Implementation\n",
        "\n",
        "Let's introduce the first version of the Python code we're going to use.\n",
        "Again, all of this has been made possible thanks to Andrej Karpathy's work\n",
        "with his [YouTube series](\n",
        "https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)\n",
        "and [build-nanogpt](\n",
        "https://github.com/karpathy/build-nanogpt) repo, so any and all code you see\n",
        "in this post can typically be traced back to something equivalent that appears\n",
        "in [train_gpt2.py](\n",
        "https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py).  None\n",
        "of this code would have made any sense to me a month or two ago---but I can\n",
        "promise you that if you devote sufficient time to watching and understanding\n",
        "the entire series, you'll grok every single line!\n",
        "\n",
        "You can follow along in a Jupyter Lab notebook if you activate the `py313`\n",
        "environment and launch `jupyter lab`.  If you correctly registered your\n",
        "`py313t` kernel per the instructions earlier, you should see an option when\n",
        "creating a new notebook to use the `py313t` Python kernel, which will be the\n",
        "free-threaded version.\n",
        "\n",
        "The code below roughly corresponds to my first version of the code in the\n",
        "commit [3ed4fe6: Add gpt2.py](\n",
        "https://github.com/tpn/parallelopedia/blob/3ed4fe60a767a12b31fca183fed00fef43c65827/src/parallelopedia/gpt2.py),\n",
        "with some formatting and style tweaks to ensure the code is viewable on mobile\n",
        "devices without requiring horizontal scrolling.\n",
        "\n",
        "There are a number of deficiencies in this code, which we'll address in\n",
        "subsequent versions.  For now, it's a good starting point to get a feel for\n",
        "how we can use PyTorch to load a GPT-2 model checkpoint, tokenize some input\n",
        "text, and generate some output text.\n",
        "\n",
        "\n",
        "```{.python .numberLines}\n",
        "# ===================================================================\n",
        "# Imports\n",
        "# ===================================================================\n",
        "import dataclasses\n",
        "import logging\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# ===================================================================\n",
        "# Globals\n",
        "# ===================================================================\n",
        "\n",
        "# The following assumes you've created a notebook in the\n",
        "# root of the parallelopedia repo, and you've downloaded\n",
        "# the model_19072.pt file from HuggingFace per earlier\n",
        "# instructions and placed it in the 'data' directory.\n",
        "MODEL_CHECKPOINT = 'data/model_19072.pt'\n",
        "\n",
        "LOG_LEVEL = 'DEBUG'\n",
        "\n",
        "# ===================================================================\n",
        "# Logging\n",
        "# ===================================================================\n",
        "logging.basicConfig(\n",
        "    level=getattr(logging, LOG_LEVEL),\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        ")\n",
        "\n",
        "# ===================================================================\n",
        "# Setup\n",
        "# ===================================================================\n",
        "\n",
        "# Use bfloat16 for matmul precision where possible.\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# ===================================================================\n",
        "# GPT2 PyTorch Model Components\n",
        "# ===================================================================\n",
        "\n",
        "# Now define the classes making up our GPT2 implementation.\n",
        "# These map directly to the components introduced by the\n",
        "# now-seminal 2017 \"Attention Is All You Need\" paper.\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Causal self-attention for the GPT2 model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # Key, query, value projections for all heads, but in a batch.\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "\n",
        "        # Output projection.\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "        # Regularization.\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Batch size, sequence length, embedding dimensionality.\n",
        "        B, T, C = (x.size())\n",
        "\n",
        "        # Calculate query, key, values for all heads in\n",
        "        # batch and move head forward to be the batch dim.\n",
        "        #\n",
        "        # N.B. nh is \"number of heads\", hs is \"head size\",\n",
        "        #      and C (number of channels) is nh * hs.\n",
        "        #      E.g. in GPT-2 (124M), n_head=12, hs=64, so\n",
        "        #      nh*hs=C=768 channels in the Transformer.\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        head_dim = C // self.n_head\n",
        "\n",
        "        # (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "        # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "        # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "        # Flash attention.\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        # Re-assemble all head outputs side by side.\n",
        "        y = (y.transpose(1, 2).contiguous().view(B, T, C))\n",
        "\n",
        "        # Output projection.\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer perceptron for the GPT2 model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block for the GPT2 model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "# ===================================================================\n",
        "# GPT2 Supporting Classes\n",
        "# ===================================================================\n",
        "\n",
        "# N.B. These differ slightly from Andrej's classes in\n",
        "#      `train_gpt2.py`.  `GPTCheckpoint` is a helper\n",
        "#      class I wrote that has no analog in the former.\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    \"\"\"\n",
        "    Configuration class for GPT model.\n",
        "\n",
        "    Attributes:\n",
        "\n",
        "        block_size (int): Maximum sequence length.\n",
        "\n",
        "        vocab_size (int): Number of tokens.  GPT2 from\n",
        "            huggingface has a vocab size of 50257, which\n",
        "            includes 50,000 BPE merges, 256 byte tokens,\n",
        "            and 1 <|endoftext|> token.  However, Andrej\n",
        "            Karpathy's `build-nanogpt/train_gpt2.py`\n",
        "            uses a vocab size of 50304.  I vaguely recall\n",
        "            the explanation for this discrepancy as a local\n",
        "            optimization to yield better alignment sizes,\n",
        "            but I'm not 100% certain.\n",
        "\n",
        "            The local GPT2 training that we did on\n",
        "            edu_fineweb10b used 50304, so we will use\n",
        "            that here.\n",
        "\n",
        "        n_layer (int): Number of layers.\n",
        "\n",
        "        n_head (int): Number of attention heads.\n",
        "\n",
        "        n_embd (int): Embedding dimension.\n",
        "    \"\"\"\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTCheckpoint:\n",
        "    \"\"\"\n",
        "    Checkpoint class for GPT model.\n",
        "\n",
        "    Mandatory Attributes:\n",
        "\n",
        "        model (dict): The model state_dict.\n",
        "\n",
        "        step (int): The step number.\n",
        "\n",
        "        val_loss (float): The validation loss.\n",
        "\n",
        "        config (GPTConfig): The configuration.\n",
        "    \"\"\"\n",
        "    model: dict\n",
        "    step: int\n",
        "    val_loss: float\n",
        "    config: GPTConfig\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls,\n",
        "             checkpoint_path: str,\n",
        "             device: str) -> \"GPTCheckpoint\":\n",
        "        \"\"\"\n",
        "        Load a checkpoint from a file.\n",
        "\n",
        "        Args:\n",
        "\n",
        "            checkpoint_path (str): Supplies the path to the\n",
        "                checkpoint file.\n",
        "\n",
        "            device (str): Supplies the device to use for\n",
        "                the model.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "            GPTCheckpoint: A new GPTCheckpoint instance.\n",
        "\n",
        "        \"\"\"\n",
        "        data = torch.load(\n",
        "            checkpoint_path,\n",
        "            map_location=device,\n",
        "        )\n",
        "        checkpoint = cls(\n",
        "            model=data[\"model\"],\n",
        "            step=data[\"step\"],\n",
        "            val_loss=data[\"val_loss\"],\n",
        "            config=GPTConfig(**data[\"config\"]),\n",
        "        )\n",
        "        return checkpoint\n",
        "\n",
        "    def save(self, checkpoint_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the checkpoint to a file.\n",
        "\n",
        "        Args:\n",
        "\n",
        "            checkpoint_path (str): Supplies the path to the\n",
        "                checkpoint file.\n",
        "        \"\"\"\n",
        "        # N.B. We save config as a raw dictionary to avoid\n",
        "        #      pickling issues with object namespaces when\n",
        "        #      reloading.\n",
        "        data = {\n",
        "            \"model\": self.model,\n",
        "            \"step\": self.step,\n",
        "            \"val_loss\": self.val_loss,\n",
        "            \"config\": dataclasses.asdict(self.config),\n",
        "        }\n",
        "        torch.save(data, checkpoint_path)\n",
        "\n",
        "# ===================================================================\n",
        "# GPT2 Model Implementation\n",
        "# ===================================================================\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "                h=nn.ModuleList(\n",
        "                    [Block(config) for _ in range(config.n_layer)]\n",
        "                ),\n",
        "                ln_f=nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(\n",
        "            config.n_embd, config.vocab_size, bias=False\n",
        "        )\n",
        "\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the GPT model.\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): Supplies the input tensor of shape\n",
        "                (B, T).\n",
        "            targets (torch.Tensor): Optionally supplies the target\n",
        "                tensor of shape (B, T) for computing the loss.\n",
        "\n",
        "        \"\"\"\n",
        "        (B, T) = idx.size()\n",
        "\n",
        "        # Forward the token and position embeddings.\n",
        "\n",
        "        # Shape (T)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "\n",
        "        # Position embeddings of shape (T, n_embd).\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "\n",
        "        # Token embeddings of shape (B, T, n_embd).\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Forward the blocks of the transformer.\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        # Forward the final layernorm and the classifier.\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        # (B, T, vocab_size)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
        "            )\n",
        "\n",
        "        return (logits, loss)\n",
        "\n",
        "    @classmethod\n",
        "    def from_local_pretrained(\n",
        "        cls, model_path: str, map_location: str = \"cpu\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Load a model from a local checkpoint.\n",
        "\n",
        "        N.B. This is a new method based off GPT.from_pretrained\n",
        "             in Andrej Karpathy's train_gpt2.py.\n",
        "\n",
        "        Args:\n",
        "            cls (type): Supplies the class type.\n",
        "\n",
        "            model_path (str): Supplies the path to the model\n",
        "                checkpoint.\n",
        "\n",
        "            map_location (str): Supplies the device to which\n",
        "                the model will be mapped.\n",
        "        \"\"\"\n",
        "        with torch.serialization.safe_globals([GPTConfig]):\n",
        "            checkpoint = torch.load(\n",
        "                model_path,\n",
        "                map_location=map_location,\n",
        "            )\n",
        "\n",
        "        config = checkpoint[\"config\"]\n",
        "        model = cls(config)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        model.eval()\n",
        "\n",
        "        msg = (\n",
        "            f\"Loaded model from step {checkpoint['step']}, \"\n",
        "            f\"val_loss {checkpoint['val_loss']}\"\n",
        "        )\n",
        "        logging.info(msg)\n",
        "        return model\n",
        "\n",
        "    def generate(\n",
        "        self, text: str, max_length: int = 1024, top_k: int = 50,\n",
        "        seed: int = None,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate text from the model.\n",
        "\n",
        "        N.B. This is a new method based off the generation code\n",
        "             present in Andrej Karpathy's train_gpt2.py.\n",
        "\n",
        "        Args:\n",
        "\n",
        "            text (str): Supplies the prompt.\n",
        "\n",
        "            max_length (int): Supplies the maximum total length,\n",
        "                including prompt.\n",
        "\n",
        "            top_k (int): Supplies the number of tokens to consider\n",
        "                at each generation step.\n",
        "\n",
        "            seed (int): Optionally supplies the manual seed to use\n",
        "                for the generator.  If None, the model's manual\n",
        "                seed will be used.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "            str: The generated text (including the initial prompt).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        # Obtain our GPT2 tokenizer, and resolve the stop token.\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        stop_string = '<|endoftext|>'\n",
        "        stop_token = enc.n_vocab - 1\n",
        "        actual = enc.decode([stop_token])\n",
        "        assert actual == stop_string, (\n",
        "            f\"expected {stop_string}, got {actual}\"\n",
        "        )\n",
        "\n",
        "        # Encode the prompt.\n",
        "        tokens = enc.encode(text)\n",
        "        x = torch.tensor(\n",
        "            tokens, dtype=torch.long, device=device\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        # Create a random generator for reproducibility.\n",
        "        if seed is None:\n",
        "            seed = self.manual_seed\n",
        "        sample_rng = torch.Generator(device=device)\n",
        "        sample_rng.manual_seed(seed)\n",
        "\n",
        "        # Generate tokens up to our max length, or until we hit the\n",
        "        # stop token.\n",
        "        start = time.perf_counter()\n",
        "        count = 0\n",
        "        while x.size(1) < max_length:\n",
        "            count += 1\n",
        "            with torch.no_grad():\n",
        "                # Forward pass, ignoring the returned loss.\n",
        "                (logits, _) = self(x)\n",
        "\n",
        "            # Take the logits at the last time-step (shape:\n",
        "            # (1, vocab_size)).\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Convert to probabilities.\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling.\n",
        "            topk_probs, topk_indices = torch.topk(\n",
        "                probs, k=top_k, dim=-1\n",
        "            )\n",
        "\n",
        "            # Sample the next token.\n",
        "            next_idx = torch.multinomial(\n",
        "                topk_probs, num_samples=1, generator=sample_rng\n",
        "            )\n",
        "            next_token = torch.gather(topk_indices, -1, next_idx)\n",
        "\n",
        "            # If the next token is the stop token, we're done.\n",
        "            if next_token.item() == stop_token:\n",
        "                break\n",
        "\n",
        "            # Otherwise, append the token to the current sequence\n",
        "            # and continue generation.\n",
        "            x = torch.cat((x, next_token), dim=1)\n",
        "\n",
        "        end = time.perf_counter()\n",
        "        elapsed = end - start\n",
        "        tokens_per_sec = float(count) / elapsed\n",
        "\n",
        "        msg = (\n",
        "            f'Generated {count} tokens in {elapsed:.2f} seconds '\n",
        "            f'({tokens_per_sec:.2f} tokens/sec)'\n",
        "        )\n",
        "        logging.debug(msg)\n",
        "\n",
        "        # Decode the output tokens and return the generated text,\n",
        "        # including the initial prompt.\n",
        "        output_tokens = x[0].tolist()\n",
        "        return enc.decode(output_tokens)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "```{.python .numberLines}\n",
        "from _gpt2_v1 import *\n",
        "# Load the model checkpoint.\n",
        "model = GPT.from_local_pretrained(MODEL_CHECKPOINT)\n",
        "model.to('cuda')\n",
        "prompt = \"Albert Einstein's Theory of Relativity stated that\"\n",
        "print(model.generate(prompt))\n",
        "```\n",
        "\n",
        "<!-- vim:set ts=8 sw=2 sts=2 expandtab textwidth=78 -->"
      ],
      "id": "4a56f65c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.13t (Free-Threaded)",
      "language": "python",
      "name": "py313t"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}