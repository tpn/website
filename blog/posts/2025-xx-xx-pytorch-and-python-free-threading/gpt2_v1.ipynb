{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6790c6a-b45d-4e9b-87bb-ff03fa6ca06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: gpt2-v1-setup\n",
    "#| echo: true\n",
    "#| code-fold: true\n",
    "\n",
    "# gpt2_v1.ipynb\n",
    "\n",
    "# ===================================================================\n",
    "# Imports\n",
    "# ===================================================================\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# ===================================================================\n",
    "# Helper Timer Class\n",
    "# ===================================================================\n",
    "class ElapsedTimer:\n",
    "    \"\"\"\n",
    "    Context manager and reusable timer to measure elapsed time.\n",
    "\n",
    "    Example:\n",
    "        timer = elapsed_timer()\n",
    "        with timer:\n",
    "            do_something()\n",
    "        print(f'Elapsed: {timer.elapsed:.3f}')\n",
    "\n",
    "        # Re-enterable:\n",
    "        with timer:\n",
    "            do_something_else()\n",
    "        print(f'Elapsed: {timer.elapsed:.3f}')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        self._elapsed = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._elapsed = time.perf_counter() - self.start\n",
    "\n",
    "    @property\n",
    "    def elapsed(self):\n",
    "        \"\"\"\n",
    "        Return the elapsed time for the most recent context.\n",
    "        \"\"\"\n",
    "        if self._elapsed is None:\n",
    "            raise ValueError(\"Timer has not been used in a context yet.\")\n",
    "        return self._elapsed\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Globals\n",
    "# ===================================================================\n",
    "\n",
    "LOG_LEVEL = 'DEBUG'\n",
    "PARALLELOPEDIA_ROOT = os.environ['PARALLELOPEDIA_ROOT']\n",
    "PARALLELOPEDIA_DATA_DIR = join(PARALLELOPEDIA_ROOT, 'data')\n",
    "MODEL_CHECKPOINT = join(\n",
    "    PARALLELOPEDIA_DATA_DIR,\n",
    "    'model_19072.pt',\n",
    ")\n",
    "MODEL_DOWNLOAD_URL = (\n",
    "    \"https://huggingface.co/datasets/trentnelson/\"\n",
    "    \"parallelopedia-data-gpt2/resolve/main/model_19072.pt\"\n",
    ")\n",
    "\n",
    "# Download the model from huggingface if necessary.\n",
    "os.makedirs(PARALLELOPEDIA_DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not exists(MODEL_CHECKPOINT):\n",
    "    print(f'Downloading {MODEL_DOWNLOAD_URL} via wget '\n",
    "          'this might take a while...')\n",
    "    args = [\n",
    "        \"wget\",\n",
    "        \"--quiet\",\n",
    "        MODEL_DOWNLOAD_URL,\n",
    "        \"-P\",\n",
    "        PARALLELOPEDIA_DATA_DIR,\n",
    "    ]\n",
    "    timer = ElapsedTimer()\n",
    "    with timer:\n",
    "        subprocess.run(args, check=True)\n",
    "    print(f'Downloaded model in {timer.elapsed:.3f} seconds.')\n",
    "    \n",
    "assert exists(MODEL_CHECKPOINT), \"Missing checkpoint.\"\n",
    "\n",
    "# ===================================================================\n",
    "# Logging\n",
    "# ===================================================================\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, LOG_LEVEL),\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# Setup\n",
    "# ===================================================================\n",
    "\n",
    "# Use bfloat16 for matmul precision where possible.\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# ===================================================================\n",
    "# GPT2 PyTorch Model Components\n",
    "# ===================================================================\n",
    "\n",
    "# Now define the classes making up our GPT2 implementation.\n",
    "# These map directly to the components introduced by the\n",
    "# now-seminal 2017 \"Attention Is All You Need\" paper.\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention for the GPT2 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Key, query, value projections for all heads, but in a batch.\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "\n",
    "        # Output projection.\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "        # Regularization.\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Batch size, sequence length, embedding dimensionality.\n",
    "        B, T, C = (x.size())\n",
    "\n",
    "        # Calculate query, key, values for all heads in\n",
    "        # batch and move head forward to be the batch dim.\n",
    "        #\n",
    "        # N.B. nh is \"number of heads\", hs is \"head size\",\n",
    "        #      and C (number of channels) is nh * hs.\n",
    "        #      E.g. in GPT-2 (124M), n_head=12, hs=64, so\n",
    "        #      nh*hs=C=768 channels in the Transformer.\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\n",
    "        head_dim = C // self.n_head\n",
    "\n",
    "        # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "\n",
    "        # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "\n",
    "        # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Flash attention.\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # Re-assemble all head outputs side by side.\n",
    "        y = (y.transpose(1, 2).contiguous().view(B, T, C))\n",
    "\n",
    "        # Output projection.\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for the GPT2 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block for the GPT2 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# ===================================================================\n",
    "# GPT2 Supporting Classes\n",
    "# ===================================================================\n",
    "\n",
    "# N.B. These differ slightly from Andrej's classes in\n",
    "#      `train_gpt2.py`.  `GPTCheckpoint` is a helper\n",
    "#      class I wrote that has no analog in the former.\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for GPT model.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        block_size (int): Maximum sequence length.\n",
    "\n",
    "        vocab_size (int): Number of tokens.  GPT2 from\n",
    "            huggingface has a vocab size of 50257, which\n",
    "            includes 50,000 BPE merges, 256 byte tokens,\n",
    "            and 1 <|endoftext|> token.  However, Andrej\n",
    "            Karpathy's `build-nanogpt/train_gpt2.py`\n",
    "            uses a vocab size of 50304.  I vaguely recall\n",
    "            the explanation for this discrepancy as a local\n",
    "            optimization to yield better alignment sizes,\n",
    "            but I'm not 100% certain.\n",
    "\n",
    "            The local GPT2 training that we did on\n",
    "            edu_fineweb10b used 50304, so we will use\n",
    "            that here.\n",
    "\n",
    "        n_layer (int): Number of layers.\n",
    "\n",
    "        n_head (int): Number of attention heads.\n",
    "\n",
    "        n_embd (int): Embedding dimension.\n",
    "    \"\"\"\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# GPT2 Model Implementation\n",
    "# ===================================================================\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.manual_seed = 42\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList(\n",
    "                    [Block(config) for _ in range(config.n_layer)]\n",
    "                ),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.n_embd, config.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the GPT model.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Supplies the input tensor of shape\n",
    "                (B, T).\n",
    "            targets (torch.Tensor): Optionally supplies the target\n",
    "                tensor of shape (B, T) for computing the loss.\n",
    "\n",
    "        \"\"\"\n",
    "        (B, T) = idx.size()\n",
    "\n",
    "        # Forward the token and position embeddings.\n",
    "\n",
    "        # Shape (T)\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "\n",
    "        # Position embeddings of shape (T, n_embd).\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "\n",
    "        # Token embeddings of shape (B, T, n_embd).\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Forward the blocks of the transformer.\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # Forward the final layernorm and the classifier.\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # (B, T, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
    "            )\n",
    "\n",
    "        return (logits, loss)\n",
    "\n",
    "    @classmethod\n",
    "    def from_local_pretrained(\n",
    "        cls, model_path: str, map_location: str = \"cuda\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load a model from a local checkpoint.\n",
    "\n",
    "        N.B. This is a new method based off GPT.from_pretrained\n",
    "             in Andrej Karpathy's train_gpt2.py.\n",
    "\n",
    "        Args:\n",
    "            cls (type): Supplies the class type.\n",
    "\n",
    "            model_path (str): Supplies the path to the model\n",
    "                checkpoint.\n",
    "\n",
    "            map_location (str): Supplies the device to which\n",
    "                the model will be mapped.\n",
    "        \"\"\"\n",
    "        with torch.serialization.safe_globals([GPTConfig]):\n",
    "            checkpoint = torch.load(\n",
    "                model_path,\n",
    "                map_location=map_location,\n",
    "            )\n",
    "\n",
    "        config = checkpoint[\"config\"]\n",
    "        config = GPTConfig(**checkpoint[\"config\"])\n",
    "        model = cls(config, device=map_location)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        model.eval()\n",
    "\n",
    "        msg = (\n",
    "            f\"Loaded model from step {checkpoint['step']}, \"\n",
    "            f\"val_loss {checkpoint['val_loss']}\"\n",
    "        )\n",
    "        logging.info(msg)\n",
    "        return model\n",
    "\n",
    "    def generate(\n",
    "        self, text: str, max_length: int = 1024, top_k: int = 50,\n",
    "        seed: int = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate text from the model.\n",
    "\n",
    "        N.B. This is a new method based off the generation code\n",
    "             present in Andrej Karpathy's train_gpt2.py.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            text (str): Supplies the prompt.\n",
    "\n",
    "            max_length (int): Supplies the maximum total length,\n",
    "                including prompt.\n",
    "\n",
    "            top_k (int): Supplies the number of tokens to consider\n",
    "                at each generation step.\n",
    "\n",
    "            seed (int): Optionally supplies the manual seed to use\n",
    "                for the generator.  If None, the model's manual\n",
    "                seed will be used.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            str: The generated text (including the initial prompt).\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = self.device\n",
    "        # Obtain our GPT2 tokenizer, and resolve the stop token.\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        stop_string = '<|endoftext|>'\n",
    "        stop_token = enc.n_vocab - 1\n",
    "        actual = enc.decode([stop_token])\n",
    "        assert actual == stop_string, (\n",
    "            f\"expected {stop_string}, got {actual}\"\n",
    "        )\n",
    "\n",
    "        # Encode the prompt.\n",
    "        tokens = enc.encode(text)\n",
    "        x = torch.tensor(\n",
    "            tokens, dtype=torch.long, device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        # Create a random generator for reproducibility.\n",
    "        if seed is None:\n",
    "            seed = self.manual_seed\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(seed)\n",
    "\n",
    "        # Generate tokens up to our max length, or until we hit the\n",
    "        # stop token.\n",
    "        start = time.perf_counter()\n",
    "        count = 0\n",
    "        while x.size(1) < max_length:\n",
    "            count += 1\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, ignoring the returned loss.\n",
    "                (logits, _) = self(x)\n",
    "\n",
    "            # Take the logits at the last time-step (shape:\n",
    "            # (1, vocab_size)).\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Convert to probabilities.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Top-k sampling.\n",
    "            topk_probs, topk_indices = torch.topk(\n",
    "                probs, k=top_k, dim=-1\n",
    "            )\n",
    "\n",
    "            # Sample the next token.\n",
    "            next_idx = torch.multinomial(\n",
    "                topk_probs, num_samples=1, generator=sample_rng\n",
    "            )\n",
    "            next_token = torch.gather(topk_indices, -1, next_idx)\n",
    "\n",
    "            # If the next token is the stop token, we're done.\n",
    "            if next_token.item() == stop_token:\n",
    "                break\n",
    "\n",
    "            # Otherwise, append the token to the current sequence\n",
    "            # and continue generation.\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        elapsed = end - start\n",
    "        tokens_per_sec = float(count) / elapsed\n",
    "\n",
    "        msg = (\n",
    "            f'Generated {count} tokens in {elapsed:.2f} seconds '\n",
    "            f'({tokens_per_sec:.2f} tokens/sec)'\n",
    "        )\n",
    "        logging.debug(msg)\n",
    "\n",
    "        # Decode the output tokens and return the generated text,\n",
    "        # including the initial prompt.\n",
    "        output_tokens = x[0].tolist()\n",
    "        return enc.decode(output_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa15731-cef4-4cf8-a8cd-7fa90f015ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 13:56:01,730 - INFO - Loaded model from step 19072, val_loss 3.0519702434539795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| label: gpt2-v1-load-model\n",
    "#| echo: true\n",
    "model = GPT.from_local_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    map_location='cuda',\n",
    ")\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb4d6d8-ca4e-483b-b80c-23c2a00c3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 13:56:03,054 - DEBUG - Generated 79 tokens in 0.80 seconds (98.77 tokens/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein's Theory of Relativity stated\n",
      "that the speed of light was approximately 10\n",
      "000 of parsecs, whereas quantum physicists\n",
      "have suggested that, as we move further into\n",
      "the universe, the universe might grow older.\n",
      "The new experiment, conducted by researchers\n",
      "at the University of New Jersey, New York,\n",
      "and the University of California, Berkeley\n",
      "shows that photons travelling at the speed of\n",
      "light will be around 30 to 65 kilometres per\n",
      "second.\n"
     ]
    }
   ],
   "source": [
    "#| label: gpt2-v1-generate-1\n",
    "#| echo: true\n",
    "prompt = \"Albert Einstein's Theory of Relativity stated that\"\n",
    "result = model.generate(prompt, seed=42)\n",
    "from textwrap import wrap\n",
    "print('\\n'.join(wrap(result, width=45)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d262863-de28-4ca7-b44b-1c87f9593a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 13:56:19,873 - DEBUG - Generated 1015 tokens in 16.81 seconds (60.37 tokens/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein's Theory of Relativity stated\n",
      "that the speed of light is the same as it is\n",
      "in two places, which means that a given speed\n",
      "can either be described by two different\n",
      "speed equations directly or they may be both\n",
      "equations. It is then assumed that the speed\n",
      "of light is the speed of the universe or the\n",
      "universe's existence relative to Earth. In\n",
      "relativity, a measure of the speed of light\n",
      "is the absolute speed of the light. As long\n",
      "as the speed of light is less than its speed\n",
      "in two different places, the absolute speed\n",
      "can be calculated. For example, the absolute\n",
      "speed is 1/2990000000 (2,299,792,458) km/hr\n",
      "with an absolute speed about 10 times as fast\n",
      "as it is in two different places. Now we can\n",
      "use the following equation to describe the\n",
      "speed of light: E = C/C2 The speed of light,\n",
      "as a function of C, is a constant. By\n",
      "Einstein's definition of relativity, the\n",
      "speed of light is a constant. This is because\n",
      "light travels at its maximum speed along the\n",
      "direction (if it's travelling above the speed\n",
      "of light, the point where light must be\n",
      "observed is called \"aperture\" of the speed of\n",
      "light). The speed of light is about half as\n",
      "fast as the speed of light because the speed\n",
      "of light has a smaller varying velocity for\n",
      "each direction of radiation. The speed of\n",
      "light, as a function of C, is a constant. The\n",
      "speed of a wave is the constant measured\n",
      "along the direction of the wave relative to\n",
      "its location in space. E = C/C2 where E is\n",
      "the speed of light along the direction of the\n",
      "wave. Because the speed of the wave is the\n",
      "speed of the particle in the wave, and c the\n",
      "speed of the particle, E's is also given by\n",
      "the speed of light. For example, a light\n",
      "particle is moving from its place of greatest\n",
      "velocity to its location of greatest\n",
      "velocity. E.g. C = F/d, C = d/d For most\n",
      "materials and most other objects, the speed\n",
      "of light is the same for all wavelengths. The\n",
      "speed of light is, on the other hand, the\n",
      "speed of the energy form of a photon. E.g. c\n",
      "= C/d, C = e/d For most particles, light\n",
      "travels over one degree of separation and\n",
      "this is how photons interact with other\n",
      "particles. We can compare a particle's\n",
      "velocity to an object's velocity. The speed\n",
      "of light is measured by the distance between\n",
      "the particle's nose and the surface of the\n",
      "object. For example, a photon of light emits\n",
      "the energy of a single photon. If a photon of\n",
      "another type is fired at the same speed as\n",
      "the first, it will get out of the light, but\n",
      "a photon of the other type will not get back\n",
      "to the ground. The fractional energy will be\n",
      "reduced. The distance between two photons of\n",
      "the same type will be reduced to the square\n",
      "of their energies. E.g. C = C/C2, C = -D/d.,\n",
      "D = 9/6 A photon of color does not have\n",
      "sufficient energy to be emitted by that color\n",
      "and is therefore subject to The speed of\n",
      "light is the change in velocity over time.\n",
      "This is a constant, but sometimes it is\n",
      "possible to express it like this: E = c2/e In\n",
      "relativity, the length of the distance is the\n",
      "length of time the length of wave is divided\n",
      "by the speed of light. E.g. a beam of light\n",
      "travelling at about 9.2 miles per second must\n",
      "travel at around 7.3 miles per second to get\n",
      "E.g. a beam moving at 3.2 miles per second\n",
      "must travel at around 8 miles per second to\n",
      "get E.g. a beam moving at 1.8 miles per\n",
      "second must travel at 9.0 miles per second to\n",
      "get E.g. an object going at 2.3 miles per\n",
      "second must travel at 1.8 miles per second to\n",
      "get E.g. a beam moving at 2.3 miles per\n",
      "second must travel at 3.4 miles per second to\n",
      "get E.g.. a beam traveling at 3.4 miles per\n",
      "second to get E.g.. a beam moving at 2.3\n",
      "miles per second must travel at 3.8 miles per\n",
      "second to get E.g.. a beam traveling at 3.8\n",
      "miles per second to get E.g.. a beam moving\n",
      "at about 4.4 miles per second must travel at\n",
      "about 3.9 miles per second to get E.g.. a\n",
      "beam moving at 5.5 miles per second to get a\n",
      "beam moving at 5.9 miles per S.G.D.. is the\n",
      "same thing as a mass. The distance is a unit\n",
      "in terms of the speed of light. Determining\n",
      "the speed of light is an additional measure\n",
      "of the energy. For most things\n"
     ]
    }
   ],
   "source": [
    "#| label: gpt2-v1-generate-2\n",
    "#| echo: true\n",
    "result = model.generate(prompt, seed=20190903)\n",
    "print('\\n'.join(wrap(result, width=45)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313t",
   "language": "python",
   "name": "py313t"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
