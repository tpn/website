---
title: "DRAFT: PyTorch and Python Free-Threading"
subtitle: |
    Unlocking multi-threaded parallel inference on PyTorch models
categories:
  - PyTorch
  - Python
  - Free-Threading
  - No-GIL
  - LLM
  - GPT2
author: "Trent Nelson"
draft: true
image: "images/pytorch-and-python-free-threading.png"
description: |
  This post examines multi-threaded parallel inference on PyTorch models
  using the new *No-GIL*, free-threaded version of Python.  Using a simple
  124M parameter GPT2 model that we train from scratch, we explore the novel
  new territory unlocked by free-threaded Python: parallel PyTorch model
  inference, where multiple threads, unimpeded by the Python GIL, attempt to
  generate text from a transformer-based model in parallel.
jupyter:
  kernelspec:
    display_name: "Python 3.13t (Free-Threaded)"
    language: python
    name: py313t
execute:
  echo: true
  eval: true
  output: true
  error: true
lightbox: true
---

This blog post is sponsored in part by [Meta](https://meta.com) in
collaboration with [Quansight](https://quansight.com) and
[OpenTeams](https://openteams.com).

# Introduction

Python 3.13, released in October 2024, is the first version of Python to
introduce support for a "no-GIL" *free-threaded* mode, per
[PEP-703 Making the Global Interpreter Lock Optional in CPython](
https://peps.python.org/pep-0703/), unlocking the ability for multiple Python
threads to run simultaneously.

This allows, for the first time since the language's inception in December
1989, a single Python process to saturate all CPU cores in parallel with
pure Python code (i.e. not farming out to extension modules written in C, C++,
or, more recently, Rust).

A handful of the [motivations](https://peps.python.org/pep-0703/#motivation)
captured in that PEP opine on how the GIL impedes Python AI workflows,
particularly as it relates to GPU programming.

This blog post explores what can be done with [PyTorch](https://pytorch.org)
now with the new free-threaded version of Python, specifically focusing on
run-time inference on transformer-based generative models.

::: {.callout-note}

I didn't focus on how *training* PyTorch models might look in the new
free-threaded Python world for a couple of reasons.

Primarily, training is
a lot more complex if you're involving multiple nodes---as gradients need
to be carefully synchronized at critical points, for example---and is well
outside the scope of a simple blog post.

Additionally, there's already a huge body of existing work tackling
multi-node training in PyTorch by way of the [Distributed Data Parallel](
https://pytorch.org/tutorials/intermediate/ddp_tutorial.html),
`multiprocessing`-based facilities exposed by [`torch.distributed`](
https://pytorch.org/tutorials/intermediate/dist_tuto.html).

Whereas, on the flip side, no one has really explored what parallel
inference might look like in a single-threaded Python because the GIL has
prevented that from even being an option until now.

:::

# Getting Started

All of this work was done on Linux (Ubuntu 22.04) with Python 3.13t, PyTorch
2.6, and CUDA 12.6.

Full source code is provided to everything captured in this post.  It is
worth noting that in a few cases, I am rolling my own solutions for things
that have existing solutions in the broader Python ecosystem.  For example,
in the tail end of the post, I leverage a multi-threaded `asyncio`-based
HTTP server I wrote instead of using existing solutions like
[FastAPI](https://fastapi.tiangolo.com/).

The reason for this is that, as free-threaded Python is still in its
infancy, a lot of packages do not work with it yet, especially those that
rely on Cython, C or C++ code, or Rust.

In fact, Rust dependencies are particularly problematic due to the
proliferation of Python projects leveraging [PyO3](
https://github.com/PyO3/pyo3) (Rust bindings for Python), especially
prominent projects such as [TikToken](https://github.com/openai/tiktoken)
and [Pydantic](https://docs.pydantic.dev/latest/), upon which a lot of the
Python AI ecosystem is built.  PyO3 only recently grew support for
free-threaded Python in their 0.23.3 release, which came out in December,
2025, and many dependent projects are yet to update to it.

Thus, this post and its supporting code should not be considered the state
of the art for production deployments---its primary goal is exploratory in
nature, and minimizing the number of moving pieces in the stack helps
achieve this goal.

## Environments

It is fiddly getting the environments set up in support of this post.
Again, this is due to the infancy of free-threaded Python.  So I apologize
in advance for how long this environment setup section is.

I reference two `conda` environments in this post: a Python 3.13
free-threaded one named `py313t`, and a normal, not-free-threaded Python
3.13 one named `py313`.

The primary motivation behind the second `py313` environment is it allows
us to install Jupyter Lab, which, at the time of writing, still isn't
compatible with a Python free-threaded installation.  However, we can still
register a free-threaded Python kernel with Jupyter, which is all we really
care about when running the code in this post in a free-threaded environment.

Details on creating the `conda` environments follow.

### Free-Threaded 3.13 Env (py313t)

I use `conda` to create the Python 3.13 free-threaded environment plus
initial dependencies, activate it, then install the remaining dependencies
via pip, as follows:

```bash
conda create -n py313t python=3.13 python-freethreading \
    nodejs pip tqdm flake8 rust requests \
        -c conda-forge
conda activate py313t
pip install numpy setuptools_rust regex safetensors
```

`nodejs` is required for the UI component we'll introduce later.  `regex`,
`rust`, and `setuptools_rust` are needed for `tiktoken`, described next.
Finally, `numpy` is for `torch`, which we install later, too.

#### TikToken

[TikToken](https://github.com/openai/tiktoken) is a fast BPE tokenizer from
OpenAI that is used extensively in the emerging Python LLM landscape.  At the
time of writing, the latest TikToken release was [0.8.0](
https://github.com/openai/tiktoken/releases/tag/0.8.0), which was built
against PyO3 0.22.2, which isn't compatible with free-threaded Python.

Thankfully, it was trivial to get a local installation of `tiktoken` working
by cloning the Github repo, bumping the PyO3 version in `Cargo.toml`, then
rebuilding and installing.

::: {.callout-note}

This is a perfect example of the type of fiddling around I wanted to avoid
by not depending on any external packages other than the bare necessities,
such as PyTorch.  I made an exception for `tiktoken` because a) it's arguably
an equally-important part of the LLM stack as `torch`, and b) it thankfully
wasn't *too* difficult getting a compatible version of `tiktoken` installed
locally.

:::

Clone the tiktoken git repo and cd into it as follows:

```bash
git clone https://github.com/openai/tiktoken
cd tiktoken
```

Edit the `Cargo.toml` file and change the `pyo3` dependency version to at
least 0.23.3^[I used 0.23.3, as that was the latest version available at the
time, however, 0.23.4 has since been released, so you could try that too.]:

```diff
diff --git a/Cargo.toml b/Cargo.toml
index 2eed0c1..6be5f63 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -9,7 +9,7 @@ name = "_tiktoken"
 crate-type = ["cdylib"]
 
 [dependencies]
-pyo3 = { version = "0.22.2", default-features = false, features = ["extension-module", "macros"] }
+pyo3 = { version = "0.23.3", default-features = false, features = ["extension-module", "macros"] }
 
 # tiktoken dependencies
 fancy-regex = "0.13.0"
```

With this patch applied, and the `py313t` conda environment active (with
`rust` and `setuptools_rust` installed):

```bash
python setup.py build
python setup.py install
```

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Show Build & Install Output"}

{{< include tiktoken-setup.py-build-and-install-output.md >}}

:::

After this, you should be able to import the `tiktoken` module in Python:

```bash
% cd ..
% python -Xgil=0
Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import tiktoken
>>>
```

#### Torch

Install PyTorch 2.6 via `pip` with the conda `py313t` environment active:

```bash
% conda activate py313t
% pip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126
```

If you have trouble installing PyTorch, consult their [Getting Started](
https://pytorch.org/get-started/locally/) guide.

You can verify torch installed correctly as follows:

```bash
% python -Xgil=0
Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> torch.cuda.is_available()
True
```

#### IPython Kernel

Installing IPython Kernel allows us to use our free-threaded Python
installation via the Jupyter Lab instance we install in the `py313`
environment.

```bash
conda activate py313t
pip install ipykernel
```

Once `ipykernel` is installed, run the following:

```bash
% python3.13t -m ipykernel --install --name py313t --user
Installed kernelspec py313t in /home/trent/.local/share/jupyter/kernels/py313t
```

This will install a kernel configuration file, `kernel.json`, which we need
to tweak by adding the `-Xgil=0` startup flag to the Python interpreter:

```bash
% cd ~/.local/jupyter/share/kernels/py313t
% cp kernel.json kernel.json.orig
% vi kernel.json
# Edit kernel.json to make it look like the diff below.
% diff -u kernel.json.orig kernel.json
```

```diff
--- kernel.json.orig    2025-02-04 15:02:21.814112004 -0800
+++ kernel.json 2025-02-04 15:02:36.553806199 -0800
@@ -1,6 +1,7 @@
 {
  "argv": [
   "/home/trent/mambaforge/envs/py313t/bin/python3.13t",
+  "-Xgil=0",
   "-Xfrozen_modules=off",
   "-m",
   "ipykernel_launcher",
@@ -12,4 +13,4 @@
  "metadata": {
   "debugger": true
  }
```

#### Datrie and Cython

[datrie](https://github.com/pytries/datrie) is a Python library that provides
a *trie* (or *digital search tree*) data structure by way of the [libdatrie](
https://linux.thai.net/~thep/datrie/datrie.html) C library.  The Python
`datrie` library isn't strictly necessary to run `parallelopedia.gpt2`, but
other components rely on it, so it's handy to get installed now, if possible.

It relies upon Cython, and thus, for now, you need to install a free-threaded
compatible version of Cython first, as follows:

```bash
conda activate py313t
pip install git+https://github.com/cython/cython
```
Then, clone the `datrie` repo and install as follows:

```bash
conda activate py313t
git clone https://github.com/pytries/datrie --recursive
cd datrie
python setup.py build
python setup.py install
```

If everything goes well, you should see something like this when you launch
Python and import `datrie`:

```bash
% python
Python 3.13.1 experimental free-threading build | packaged by conda-forge | (main, Jan 13 2025, 09:59:40) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import datrie
>>>
```

### Normal 3.13 Env (py313)

The second `py313` environment is almost identical to `py313t`, except it is
not a `python-freethreading` installation, and, additionally, we install
Jupyter Lab.  We can install `tiktoken` directly via `pip`, so we don't need
the supporting Rust cruft.  Likewise for `datrie`, we don't need to first
install Cython and then build `datrie` from git.

```bash
conda create -n py313 python=3.13 \
    nodejs pip tqdm flake8 jupyterlab requests \
        -c conda-forge
conda activate py313
pip install numpy datrie tiktoken safetensors
pip install torch==2.6 --index-url https://download.pytorch.org/whl/cu126
```

## Parallelopedia

All of the code in this article is available in the [Parallelopedia](
https://github.com/tpn/parallelopedia) repository on Github.  The code we'll
be focusing on in this post lives in the [parallelopedia.gpt2](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/gpt2.py)
module.

There is also a web user interface component named [Parallelopedia-UI](
https://github.com/tpn/parallelopedia-ui), which we will use later in the
post.

Clone the repositories as follows:

```bash
% git clone https://github.com/tpn/parallelopedia
% git clone https://github.com/tpn/parallelopedia-ui
```

::: {.callout-important .env-vars collapse="false" title="Important Environment Variables"}

The code and command examples in this post will assume you've added the
`src` directory to your `PYTHONPATH`, the `bin` directory to your `PATH`,
and set the `PARALLELOPEDIA_ROOT` environment variable to the root of the
repository.  You can do this as follows:

```bash
cd parallelopedia
export PYTHONPATH=$(pwd)/src:$PYTHONPATH
export PATH=$(pwd)/bin:$PATH
export PARALLELOPEDIA_ROOT=$(pwd)
cd ..
cd parallelopedia-ui
export PARALLELOPEDIA_UI=$(pwd)
```

It is recommended that you add these to your shell.  For me, using zsh, I
use the following:

```zsh
export PARALLELOPEDIA_ROOT=~s1/parallelopedia
export PARALLELOPEDIA_UI=~s1/parallelopedia-ui
export PYTHONPATH=$PARALLELOPEDIA_ROOT/src:$PYTHONPATH
export PATH=$PARALLELOPEDIA_ROOT/bin:$PATH
```

:::

You can perform a quick sanity check that things are working as follows:

```bash
% python -Xgil=0 -m parallelopedia.http.server --help
usage: server.py [-h] [--ip IP] [--port PORT] [--debug] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--threads THREADS] [--protocol-class PROTOCOL_CLASS] [--app-classes APP_CLASSES [APP_CLASSES ...]] [--listen-backlog LISTEN_BACKLOG]

Run the HTTP server.

options:
  -h, --help            show this help message and exit
  --ip IP               IP address to bind the server to.
  --port PORT           Port number to bind the server to.
  --debug               Enable debug mode for asyncio.
  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}
                        Set the logging level.
  --threads THREADS     Number of threads to use.
  --protocol-class PROTOCOL_CLASS
                        The protocol class to use for the server.
  --app-classes APP_CLASSES [APP_CLASSES ...]
                        Space-separated list of HTTP application classes.
  --listen-backlog LISTEN_BACKLOG
                        The listen backlog for the server.

```

# PyTorch and LLM Crash Course

My involvement with PyTorch and Large Language Models (LLMs) started around
late November last year, 2024.  Going in, I knew nothing about PyTorch, nor
deep neural networks, nor LLMs---other than having enjoyed using LLMs
thoroughly the past couple of years.  I had never trained an AI model of
any kind.  I did have a bit of NumPy and data science exposure up my sleeve,
plus general familiarity with Python.

Thanks to [Andrej Karpathy](https://karpathy.ai/)'s phenomenal YouTube series
on deep neural networks and LLMs titled [Neural Networks: From Zero to Hero](
https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ),
over the course of about 3 weeks or so I went from zero to... well I
wouldn't necessarily say *hero*---perhaps zero to *not-completley-clueless*
is more apropos.

Andrej's content is a fantastic resource to learn everything you need to
know to understand how modern LLMs work from the ground-up.  It's not a
short series---there are 19 hours, 21 minutes and two seconds of content
across ten videos---and you'll probably spend double that if you *really*
want to properly absorb the content.

None of the work presented in this post would have been possible had I not
invested the time in Andrej's series.  If you're reading this Andrej, thanks,
and keep up the brilliant work!

## Training GPT-2 (124M) Locally

Equipped with my new knowledge about LLMs, PyTorch, and, thanks to Andrej's
final video in the series titled [Let's reproduce GPT-2 (124M)](
https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)
and the accompanying [build-nanogpt](
https://github.com/karpathy/build-nanogpt) Github repo, I was able to train
a local GPT-2 model via PyTorch, from scratch, using the [edu_fineweb10B](
https://huggingface.co/rhysjones/gpt2-124M-edu-fineweb-10B) dataset.

I only had to make one change in order to run locally:
[Use 8 for micro batch size instead of 64](
https://github.com/tpn/build-nanogpt/commit/0069c4a35d1a362c1fd50f9f5bce00a170e15904).
With that change in place, I was able to train GPT-2 from scratch as follows:

```bash
% conda activate py313
% git clone gh:tpn/build-nanogpt
% cd build-nanogpt
# Download the fineweb dataset.
% python fineweb.py
# Train!
% torchrun --standalone --nproc_per_node=4 train_gpt2.py
```

This was run on an NVIDIA DGX workstation from 2017, which has an Intel
Xeon(R) CPU E5-2698 v4 @ 2.20GHz (20 cores, 40 threads), and four Tesla
V100-DGXS-32GB GPUs.

Training in parallel across all four GPUs yielded around 36,000 tokens/sec,
with an average time of about 14.5 seconds per loop iteration.  Training
took about 3 days and 5 hours for 19,072 steps.  All four GPUs were pegged
close to their 300W power limit for those three days.

::: {.callout-note}

Amusingly, well after the fact, I decided to see what kind of training
performance I'd get on my Windows 11 gaming box (via WSL2 and Ubuntu 22.04),
which has an AMD Ryzen 9 7950X3D (16 cores, 32 threads) and NVIDIA RTX 4090.
Training via `python train_gpt2.py` (`torchrun` wasn't needed as I wasn't
using multiple GPUs) yielded about 45,000 tokens/sec, which is a nice bump,
but what was most impressive was the reduction to the loop iteration
duration, which averaged out to about 180ms!

So, I could have completed the same training process in about an hour or so,
at a vastly reduced impact on my electricity bill that month :-)

:::

Once training completes, a `log/model_19072.pt` file is produced, which is
the checkpoint of the model at that final step, obtained via a call to
`torch.save()`.  The model has 124M parameters---which is tiny by modern
standards---and is just under 500MB on disk.

You can download that very model I trained via the HuggingFace dataset I set
up here: [model_19072.pt](
https://huggingface.co/datasets/trentnelson/parallelopedia-data-gpt2/blob/main/model_19072.pt).
Once downloaded, place the file in `$PARALLELOPEDIA_ROOT/data`;
alternatively, if you run the Jupyter Notebook below, it'll automatically
download the model from HuggingFace on first run.

# PyTorch GPT-2 Implementation

Let's introduce the first version of the Python code we're going to use.
Again, all of this has been made possible thanks to Andrej Karpathy's work
with his [YouTube series](
https://www.youtube.com/watch?v=l8pRSuU81PU&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=10&t=1286s&pp=iAQB)
and [build-nanogpt](
https://github.com/karpathy/build-nanogpt) repo, so any and all code you see
in this post can typically be traced back to something equivalent that appears
in [train_gpt2.py](
https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py).  None
of this code would have made any sense to me a month or two ago---but I can
promise you that if you devote sufficient time to watching and understanding
the entire series, you'll have a comprehensive understanding of all of the
pieces present in this post.

You can follow along in a Jupyter Lab notebook if you activate the `py313`
environment and launch `jupyter lab`.  If you correctly registered your
`py313t` kernel per the instructions earlier, you should see an option when
creating a new notebook to use the `py313t` Python kernel, which will be the
free-threaded version.  On the right of this page you should see all of the
notebooks referenced.

## First Version

The code below roughly corresponds to my first version of the code in the
commit [3ed4fe6: Add gpt2.py](
https://github.com/tpn/parallelopedia/blob/3ed4fe60a767a12b31fca183fed00fef43c65827/src/parallelopedia/gpt2.py),
with some formatting and style tweaks to ensure the code is viewable on mobile
devices without requiring horizontal scrolling.

There are a number of deficiencies in this code, which we'll address in
subsequent versions.  For now, it's a good starting point to get a feel for
how we can use PyTorch to load a GPT-2 model checkpoint, tokenize some input
text, and generate some output text.

{{< embed gpt2_v1.ipynb#gpt2-v1-setup >}}

## Loading the Model

With the code above executed in a preceding Jupyter Notebook cell, we can
load the model as follows:

{{< embed gpt2_v1.ipynb#gpt2-v1-load-model >}}

## Generating Text

Once we've got a model instance, text can be generated by simply calling
the model's `generate()` function with a prompt, and, optionally, some
additional parameters like seed and max length.  This is also referred to
as inference---the two terms are interchangeable, and mean the same thing:
the act of providing some input tokens---your encoded prompt---to your
trained model, and having it generate tokens in response.

Note that this isn't a *chat* or *instruction* model, nor has it been
fine-tuned to remotely resemble something actually usable.  So you can't
ask it questions or have it write code for you.

What you can do, though, is provide it with a half written sentence, and
then laugh at the ridiculous content it generates in response.  Although
note that its syntax is pretty good---the model has clearly learned enough
during training about how the English language is structured, which words
make sense when placed together, that sort of thing.  It just has no clue
about underlying semantics.

::: {.desktop-only}
{{< embed gpt2_v1.ipynb#gpt2-v1-generate-1-desktop >}}
:::

::: {.tablet-only}
{{< embed gpt2_v1.ipynb#gpt2-v1-generate-1-tablet >}}
:::

::: {.phone-only}
{{< embed gpt2_v1.ipynb#gpt2-v1-generate-1-phone >}}
:::

Now, it's worth noting at this point that a 124 million parameter GPT2 model,
trained from scratch for 19,072 iterations on the `edu_fineweb10b` data set,
with a final loss score of 3.052, is, quite frankly, hot garbage :-)

Do not be expecting output from this model to rival anything close to what
you'd expect from a contemporary LLM.  In fact, we can't even rely on it to
even remotely generate content that is factual in nature.  It spews hot
probabilisitic garbage that mostly conforms to the structure of the English
language.

But at least it's *our* hot garbage that we trained from nothing, and it's
all we need to start playing around with generating text in parallel.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Hot Garbage Example"}

Provide a slightly different seed to the prompt above and you'll precisely
see some hot garbage generation in action.  In the following example, with
an identical prompt as the one prior, it just streams nonsense until hitting
its max token limit, nary a stop token in sight.

::: {.desktop-only}
{{< embed gpt2_v1.ipynb#gpt2-v1-generate-2-desktop >}}
:::

::: {.tablet-only}
{{< embed gpt2_v1.ipynb#gpt2-v1-generate-2-tablet >}}
:::

::: {.phone-only}
{{< embed gpt2_v1.ipynb#gpt2-v1-generate-2-phone >}}
:::

lolwat.

:::

## Tweaked Version

TODO.

# Parallel PyTorch Inference

Now we finally get to the fun part: let's investigate whether or not PyTorch
model inference can be done simultaneously, in parallel, by multiple
threads, running on multiple cores at the same time, in a single
free-threaded Python process.  And ideally, we should only need one GPU,
with all of these threads *sharing* it as fairly as possible.  Although if
we have multiple GPUs, we should be able to distribute the incoming work
evenly across those too, if we want.

This is novel, uncharted territory we're able to now explore thanks to
the free-threaded version of Python.

::: {.callout-note title="Simultaneous vs Parallel vs Concurrent"}

The phrasing I've used above---*"simultaneously, in parallel"*---is a bit
redundant.  They both imply the same thing.  When I use either word in this
post, I'm explicitly referring to the new ability unlocked by free-threaded
Python, where multiple threads can be running Python code on different CPU
cores at the same time---i.e. *simultaneously*.  And thus, you're performing
work in *parallel*.

When I use the term *concurrent* or *concurrency*, I'm using it in the
traditional sense within the context of Python: making progress on multiple
things at a time.  This term is well suited to describe things like
web servers, where a single Python process, with a single thread, running on
a single CPU core, can service multiple clients at any given time (by way
of [non-blocking socket I/O and event multiplexing](
https://speakerdeck.com/trent/pyparallel-how-we-removed-the-gil-and-exploited-all-cores?slide=27)),
but it's not servicing any of those clients *simultaneously* on different
cores, because that would require multiple threads running in *parallel*.

Some more details regarding *parallelism* vs *concurrency* can be found on
[slide
8](
https://speakerdeck.com/trent/parallelism-and-concurrency-with-python?slide=8)
of a deck I put together many years ago titled
[Parallelism and Concurrency in Python](
https://speakerdeck.com/trent/parallelism-and-concurrency-with-python).

:::

So how do we try this out?  I guess we could spin up some threads and have
them all call `model.generate()`, but that's a little boring.

Why not try implement a pure Python, multi-threaded `asyncio`-based HTTP
server, expose a `/generate`-esque style `GET` endpoint, and wire that up
to the model generation code we saw above, allowing us to serve web requests
for generation in parallel, ideally in an `asyncio`-friendly manner, where
we can stream individual tokens back one-by-one via HTTP's chunked-encoding
transfer protocol, giving each thread's event loop the ability to service
multiple clients *concurrently* in a reasonably fair manner, whilst also
servicing many clients in *parallel* across all threads, and for kicks, get
AI to whip up a janky little React Bootstrap UI that we can slap in front of
it all to test it?

## Pure Python Multi-threaded HTTP Server

First thing we need is a nice and simple `asyncio`-based HTTP server, that
also happens to work with multi-threading now that we have a free-threaded
Python at our disposal.

Luckily, I have one of those laying around already!  In support of another
article I'm actively working on (which was meant to get published before
this post), I ported the [HTTP Server](
https://github.com/pyparallel/pyparallel/blob/branches/3.3-px/Lib/async/http/server.py)
I wrote many years ago for [PyParallel](https://pyparallel.org)^[And I'm
sure I used the existing Python stdlib `http.server` code at the time as the
basis; ain't nobody got time to be writing new web servers from scratch.] to
use the new `asyncio` facilities available with Python, and then slapped
multiple threads on it, where each thread gets its own `asyncio` event loop.

Turns out, thankfully, that this Just Works, at least on
Linux^[Unfortunately, it doesn't appear to work on Windows as-is; using the
exact same code, only one thread can be seen running when the server is
loaded.  It's not doing a round-robin between *all* threads, like you'd
expect to see with the GIL enabled, there's just a single sole thread
attempting to service all incoming requests, with all other threads sitting
idle.  I don't know if it's because of something quirky with regards to
additional, non-main-thread threads not getting their own event loop
(hopefully easy to fix), or something more insidious related to how we're
misuing I/O completion ports behind the scenes in `IocpProactor()` now that
we have free-threading (much harder to fix).  I haven't had time to
investigate in more detail.]---we can now have a pure Python HTTP server,
running in a single Python process, that'll happily saturate every CPU core
under load.

The server code lives in [`parallelopedia.http.server`](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/http/server.py),
and it includes a super janky little notion of *"HTTP Apps"*, the purpose of
which can be best demonstrated with a simple example:

```python
class PlaintextApp(HttpApp):
    @route
    def plaintext(self, request):
        response = text_response(request, 'Hello, World!')
        self.server.send_response(response)

class SleeperApp(HttpApp):
    @route
    def sleep(self, request, seconds):
        time.sleep(int(seconds))
        msg = f'Slept for {seconds} seconds.')
        response = text_response(request, msg)
        self.server.send_response(response)

# Create a server with the two apps.
app_classes=[PlaintextApp, SleeperApp]
server = HttpServer(app_classes=app_classes)
```

In the above example, the HTTP server will route requests for the
`/plaintext` endpoint to an instance of the `PlaintextApp`'s
`plaintext()` routine, and `/sleep` requests get routed to the
`SleeperApp`'s `sleep()` routine.

The *"slapped multiple threads on it"* activity I refered to earlier is
handled by some new [async and threading scaffolding](
https://github.com/tpn/parallelopedia/blob/main/src/parallelopedia/http/server.py#L1344)
added to the bottom of that module, with the pertinent pieces reproduced
below:

```python

async def main_async(
    args: argparse.Namespace,
    protocol_class: type,
    *protocol_args: Tuple
) -> None:
    """
    This is the main function for the server when it is running in
    asynchronous mode.  It will create a server instance and then
    call serve_forever() on it.

    Arguments:

        args (argparse.Namespace): Supplies the command-line arguments.

        protocol_class (type): Supplies the protocol class to use.

        protocol_args (tuple): Supplies the arguments to pass to the
            protocol class constructor.

    """
    loop = asyncio.get_running_loop()

    if os.name in ('nt', 'cygwin'):
        reuse_port = False
    else:
        reuse_port = True

    reuse_address = True

    server = await loop.create_server(
        lambda: protocol_class(*protocol_args),
        args.ip,
        args.port,
        backlog=args.listen_backlog,
        reuse_address=reuse_address,
        reuse_port=reuse_port,
    )

    async with server:
        await server.serve_forever()


def start_event_loop(
    args: argparse.Namespace,
    protocol_class: type,
    *protocol_args: Tuple
) -> None:
    """
    This function will start the asyncio event loop and run
    the main_async() function.  It is intended to be the
    target of a threading.Thread.

    Arguments:

        args (argparse.Namespace): Supplies the command-line arguments.

        protocol_class (type): Supplies the protocol class to use.

        protocol_args (tuple): Supplies the arguments to pass to the
            protocol class constructor.
    """
    asyncio.run(
        main_async(
            args,
            protocol_class,
            *protocol_args,
        ),
        debug=args.debug,
    )


def main_threaded_multi_accept(
    args: argparse.Namespace,
    protocol_class: type,
    *protocol_args: Tuple
) -> None:
    """
    This is the main function for the server when it is running in
    multi-threaded mode with multiple accept sockets.  Each thread
    will have its own asyncio loop issue a create_server() call for
    the given host/port and protocol.

    Arguments:

        args (argparse.Namespace): Supplies the command-line arguments.

        protocol_class (type): Supplies the protocol class to use.

        protocol_args (tuple): Supplies the arguments to pass to the
            protocol class constructor.
    """
    import threading

    threads = []
    for _ in range(args.threads):
        thread = threading.Thread(
            target=start_event_loop,
            args=(args, protocol_class, *protocol_args),
        )
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()


def main(args: Optional[argparse.Namespace] = None):
    """
    Main entry point for parallelopedia.http.server module.
    """
    args = parse_arguments()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

    # Use multiple threads to load the application classes.
    app_classes = get_classes_from_strings_parallel(
        args.app_classes,
    )

    protocol_class = get_class_from_string(args.protocol_class)
    protocol_args = (app_classes,)

    if args.threads == 1:
        asyncio.run(
            main_async(
                args,
                protocol_class,
                *protocol_args,
            ),
            debug=args.debug,
        )
    else:
        main_threaded_multi_accept(
            args,
            protocol_class,
            *protocol_args,
        )

if __name__ == '__main__':
    main()
```

## GPT2 HTTP App

With the HTTP server scaffolding in place, we can now whip up a little
`Gpt2App` class that has a `generate()` method.  Incoming requests to
the `/generate` endpoint will be routed to that routine by the server.

### Synchronous Up-Front Generation

Now, we could take the simple approach, where the `Gpt2App.generate()`
call goes off and calls `model.generate()` and then patiently waits for the
entire response to be generated before sending anything back to the user.

That code would look something like this:

```python
class Gpt2App(HttpApp):
    @route
    def generate(self, request: Request,
                 *args: List,
                 **kwds: Dict) -> None:
        prompt = args[0]
        model = get_model()
        result = model.generate(prompt=prompt)
        respose = text_response(request, result)
        self.server.send_response(response)
```

But when have you ever interacted with an LLM via a web interface where it
waits until it generates *all* of the response up-front before sending it
back to you?  Never; you can see it generate the response in real time, and
that's what we want to mimic here in this experiment.

### Our Goals

The high-level goals for our solution are thus:

1. We want to send a generated token back to the user as soon as it becomes
   available.

2. We want to ensure the client receiving the token can display it as soon
   as they receive it---so we need to be cognizant of what HTTP transfer
   protocol we use to send bytes back.  If we just used normal HTTP transfer
   encoding, the client would wait until we've sent *everything* before the
   user sees it, despite the fact that we've been trickling individual
   tokens to them the entire time.

3. We want to play nicely with the `asyncio` ecosystem upon which our
   hosting HTTP server is based---so we need to be cognizant of the current
   thread's event loop, and make sure we don't impede that thread's ability
   to *concurrently* serve other clients that are being handled by the event
   loop.

Thankfully, as we saw earlier with the implementation of the
`GPT.generate()` routine, generating tokens in response to a prompt is
inherently a *token-by-token* process.  So the algorithm at least provides
us with the means to obtain a single token at a time, which takes care of
the first point.

Second, HTTP's chunked-encoding transfer protocol will allow a HTTP client
to immediately *"see"* the tokens we send back to it as soon as we send
them, provided we enable `TCP_NODELAY` on the underlying socket to ensure
the operating system sends the bytes out to the client as soon as we send
them.

::: {.callout-note}

If we didn't do this, the default behavior of [Nagle's algorithm](
https://en.wikipedia.org/wiki/Nagle%27s_algorithm) would apply, and the
operating system would delay sending individual bytes back when we request,
in the hope that it can accumulate more bytes to send all at once at a
slightly later point in time.  This is advantageous for maximizing
throughput, but it impedes latency, and in our case, we want the lower
latency afforded by immediately sending the bytes back to the client as soon
as we generate them.

:::

[Chunked-encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding)
works by setting an HTTP response header `Transfer-Encoding: chunked`, and
then in the body, each chunk is transmitted by its length and then the chunk
itself.  The server communicates to the client that the transfer has
completed once a zero-length chunk is received.

So, as long as we send our tokens back via chunked-encoding, any HTTP/1.1
client will be able to reassemble them back into the generated text, giving
the visual appearance of real time model generation.  That will take care
of the second point.

Lastly, in order to play nice within the greater `asyncio` ecosystem, we
need to give control back to the underlying thread's `asyncio` event loop
after we generate a token and yield a decoded text fragment, which can
thankfully be done via a simple call to `await asyncio.sleep(0)`, provided
we're generating text from the model from within an `async` callback.

This ensures multiple *concurrent* clients being handled by our thread's
event loop will be handled fairly; they'll all receive generated tokens
at the same rate.

### Asynchronous Token-by-Token Generation

The first thing we need to do is to change our `Gpt2App.generate()` call
into something that is `async` compatible, in anticipation of some later
code that we write needing to issue an `await asyncio.sleep(0)`, which
can only be done within a call frame of an asynchronous method.

When our `Gpt2App.generate()` routine is called, we're still within the
context of the `asyncio` protocol's `data_received()` routine, which is a
normal, synchronous method, *not* an enlightened `async` method that can
participate in an `asyncio` event loop.

So, in order to transition from a synchronous callback to an asynchronous
one, we can use the current event loop's `create_task()` routine to enqueue
an `async` method for execution.

#### Step 1: Have generate() enqueue an async generate_response().

Thus, our `Gpt2App.generate()` call will look something like this:

```python

class Gpt2App(HttpApp):

    ...

    @route
    def generate(self, request: Request,
                 *args: List,
                 **kwds: Dict) -> None:

        # Extract the "prompt" provided in the incoming request.
        text = args[0]

        # Obtain the event loop and schedule the response
        # generation via our async generation coroutine.
        # We have to do it like this as at this point we're
        # still within the call frame of the data_received()
        # protocol callback, which isn't an async function.
        loop = asyncio.get_running_loop()
        async_task = self.generate_response(request, text, **kwds)
        loop.create_task(async_task)
```

#### Step 2: Implement an async generate_response()

Our asynchronous `generate_response()` routine will be the bridge between
generating tokens from the model, and sending those tokens back to the
client via chunked-encoding.

It is responsible for preparing the response to use chunked-encoding,
and then enabling `TCP_NODELAY` on the socket.

Then, assuming that our model has an `async_generate_for()` routine,
which we'll implement in the next step, we perform an `async for` loop
over that routine in order to obtain individual *decoded* tokens.  As
soon as we receive a token, we can send it back to the client via the
`response` object's `send_chunk()` routine.

Once we've exhausted the async generator (i.e. it either generated the
maximum number of requested tokens, or it encountered a stop token), we
can re-enable `TCP_NODELAY`, and then return.

A simplified version of the Python code is presented below.  I have omitted
most of the error handling and query parameter parsing code for simplicity;
see the expandable code block at the end for the full version.

```python

class Gpt2App(HttpApp):

    ...

    async def generate_response(
        self, request: Request,
        prompt: str,
        **kwds: Dict
    ) -> None:

        # Prepare a chunked-encoding response.
        response = request.response
        response.code = 200
        response.message = 'OK'
        response.chunked_response = True
        response.content_type = 'text/plain'

        # Obtain the model.
        model = get_model()

        # We want to enable TCP_NODELAY for the duration of
        # the response.  This ensures packets are sent
        # immediately without any internal buffering.
        try:
            response.enable_tcp_nodelay()
            enabled_nodelay = True
        except Exception as e:
            logging.error(f'Error enabling TCP_NODELAY: {e}')
            enabled_nodelay = False

        # Write the chunked header immediately.
        response_bytes = bytes(response)
        if not self.write(response_bytes):
            # Encountered a disconnect, return.
            return

        # N.B. From herein, all data must be transferred to
        #      the client via chunked encoding with the
        #      `response.send_chunk()` routine.

        # Send the initial prompt text.
        response.send_chunk(prompt)

        # Obtain decoded tokens from the model one at a time
        # via an `async for` loop, sending the token back to
        # the client as soon as it's available.
        async for decoded_token in model.generate_async_for(prompt):
            response.send_chunk(decoded_token)

        # Terminate the chunked-encoding response.
        response.end_chunks()

        # Disable TCP_NODELAY now that the response is complete.
        # The reasoning behind this is that the client may have
        # issued the HTTP request with a keep-alive header, and
        # plans on reusing this connection for a different request
        # next, which won't necessarily want `TCP_NODELAY` active.
        if enabled_nodelay:
            response.disable_tcp_nodelay()
```

::: {.callout-note #gpt2app-generate-response-full collapse="true" appearance="simple" icon="false" title="Full Code for async Gpt2App.generate_response()"}

The actual code has more robust error-handling facilities and support for
extracting the query string parameters from the incoming request URI and
converting them into keyword arguments suitable for passing to the model.

Additionally, we haven't touched on how we initialize or obtain instances of
our models yet, so the model-related code won't make much sense until later
in the article.

```python

class Gpt2App(HttpApp):
    routes = make_routes()
    route = router(routes)

    def __init__(self, server: HttpServer) -> None:
        super().__init__(server)
        self.printable = PRINTABLE

    def is_connected(self):
        # server.transport will be severed when the client
        # disconnects, so we can use this to determine if
        # the client is still connected.
        server = self.server
        transport = None
        try:
            transport = server.transport
        except AttributeError:
            pass
        return transport is not None

    def write(self, response_bytes):
        server = self.server
        transport = None
        try:
            transport = server.transport
        except AttributeError:
            pass
        if transport is not None:
            transport.write(response_bytes)
            return True
        else:
            return False

    async def generate_response(
        self, request: Request, prompt: str, **kwds: Dict
    ) -> None:

        response = request.response

        response.code = 200
        response.message = 'OK'
        response.chunked_response = True
        response.content_type = 'text/plain'

        if kwds is None:
            kwds = {}
        max_length = min(int(kwds.get('max_length', 100) or 100), 1024)
        top_k = min(int(kwds.get('top_k', 50) or 50), 50)
        seed = kwds.get('seed', None)
        if seed:
            try:
                seed = int(seed)
            except ValueError:
                seed = None
        if not seed:
            seed = random.randint(0, 2**32 - 1)

        device = kwds.get('device', None)

        model_name = kwds.get('model', None)
        if model_name == 'gpt2-xl':
            models = PRETRAINED_MODELS
            get_next = get_next_pretrained_model
        else:
            model_name = 'gpt2'
            models = MODELS
            get_next = get_next_model

        model = None
        if device is not None:
            if device == 'cpu':
                model = models[-1]
            elif device.startswith('cuda:'):
                try:
                    index = int(device[5:])
                except ValueError:
                    index = -1
                if index < 0 or index >= NUM_GPUS:
                    index = -1
                if index != -1:
                    model = models[index]
            elif device == 'cuda':
                model = models[random.randint(0, NUM_GPUS - 1)]

        if not model:
            # Get a model.  If there are multiple models available, e.g. if we
            # have multiple GPUs, this will balance the load a bit.
            model = get_next()

        expose_headers = (
            'Access-Control-Expose-Headers: '
            'X-Max-Length, '
            'X-Top-K, '
            'X-Seed, '
            'X-Model-Name, '
            'X-Model-Device'
        )
        response.other_headers.extend([
            expose_headers,
            f'X-Max-Length: {max_length}',
            f'X-Top-K: {top_k}',
            f'X-Seed: {seed}',
            f'X-Model-Name: {model_name}',
            f'X-Model-Device: {model.device}',
        ])

        # We want to enable TCP_NODELAY for the duration of
        # the response.  This ensures packets are sent
        # immediately without any internal buffering.
        try:
            response.enable_tcp_nodelay()
            enabled_nodelay = True
        except Exception as e:
            logging.error(f'Error enabling TCP_NODELAY: {e}')
            enabled_nodelay = False

        # Write the chunked header immediately.
        response_bytes = bytes(response)
        if not self.write(response_bytes):
            # Encountered a disconnect, return.
            return

        # N.B. From herein, all data must be transferred to
        #      the client via chunked encoding with the
        #      `response.send_chunk()` routine.

        # Send the initial prompt text.
        response.send_chunk(prompt)

        # Obtain an async generator instance to the model's
        # new async token generation routine.
        generate_tokens = model.generate_async_for(
            prompt,
            max_length=max_length,
            top_k=top_k,
            seed=seed,
        )
        async for decoded_token in generate_tokens:
            if decoded_token == -1:
                # A non-printable token was generated,
                # terminating generation.
                response.send_chunk(
                    OOPS_NON_PRINTABLE_ENCOUNTERED
                )
                break

            # If the client has forcibly disconnected,
            # terminate generation.
            if not self.is_connected():
                break

            # Otherwise, send the decoded token to the client
            # via chunked encoding.
            response.send_chunk(decoded_token)

        # Send the termination chunk.  This may fail at the
        # socket.send() level if the client has already
        # disconnected, which is harmless.
        response.end_chunks()

        # Disable TCP_NODELAY now that the response is complete.
        # Again, this may fail at the socket level if the client
        # has already disconnected, which is harmless.
        if enabled_nodelay:
            try:
                response.disable_tcp_nodelay()
            except Exception as e:
                msg = f'Error disabling TCP_NODELAY: {e}'
                logging.error(msg)

```

:::


#### Step 3: Implement an async GPT.async_generate_for()

In the code example above, we assumed the `GPT` model we've been using had
grown a new `async` routine named `async_generate_for()`, which we'll cover
now.

This routine is essentially an `asyncio`-friendly version of the original
`generate()` routine we wrote.  It shares a lot of the same code, with a few
notable tweaks in order to support the fact that it is being called from a
callback that was enqueued on a thread's `asyncio` event loop, and it is
expected to yield a token as soon as it is available, and then pass control
back to the event loop in order for it to service other clients before it
continues with generating the next token.

It also has the notion of checking for *"printable"* characters.  This came
about when I was initially testing this code via `curl` which would
sometimes balk and exit in the middle of streaming the response, citing that
it encountered corrupted data or something like that.

After investigation, it turned out that sometimes, for whatever reason, the
model just generates a junk, nonsensical token (like 65534, which is well
outside the highest token number of 50384).  I have no idea why it happens,
although I'll note it happens on the OpenAI GPT2 XL model available on
HuggingFace (which we'll discuss later) too, so, eh.

I deal with this by checking if we've generated a non-printable token after
decoding it, and, if so, return -1 and terminate the loop.  The
[full version](#gpt2app-generate-response-full) of the
`Gpt2App.generate_response()` routine that we introduced above checks if
we return -1, and if so, terminates generation with an oopsie message, e.g.:

```python
OOPS_NON_PRINTABLE_ENCOUNTERED = (
    'Oops! Non-printable token encountered.  Generation terminated.'
)
...

async for decoded_token in generate_tokens:
    if decoded_token == -1:
        # A non-printable token was generated,
        # terminating generation.
        response.send_chunk(
            OOPS_NON_PRINTABLE_ENCOUNTERED
        )
        break
```

After yielding a valid decoded token, we issue an `await asyncio.sleep(0)`
call, which returns control back to the event loop for it to potentially
handle other concurrent clients.  If there are no other clients, or after
it has handled all other enqueued work, generation resumes.

The full code follows, it is simple enough as-is that I didn't feel the need
to omit any details like in the prior example.

```python

class GPT:

    ...

    async def generate_async_for(
        self, text: str, max_length: int = 1024, top_k: int = 50,
        seed: int = None,
    ):
        """
        Asynchronously generate text from the model, yielding tokens
        one at a time as soon as they are available.

        Args:

            text (str): Supplies the prompt.

            max_length (int): Supplies the maximum total length,
                including prompt.

            top_k (int): Supplies the number of tokens to consider
                at each generation step.

            seed (int): Optionally supplies the manual seed to use
                for the generator.  If None, the model's manual
                seed will be used.

        Yields:

            byte: The newly generated decoded token.  If -1, a
            non-printable token was generated, and generation
            was terminated.
        """

        enc = self.enc
        stop_token = self.stop_token

        # Encode the prompt -> tensor of shape (1, T)
        tokens = enc.encode(text)
        x = torch.tensor(
            tokens, dtype=torch.long, device=self.device
        ).unsqueeze(0)

        sample_rng = torch.Generator(device=self.device)
        if seed is None:
            seed = self.manual_seed
        sample_rng.manual_seed(seed)

        logging.debug(
            f'[generate_async_for] Starting generation loop for {text} '
            f'with seed {seed}.'
        )

        start_time = time.perf_counter()
        count = 0
        while x.size(1) < max_length:
            count += 1
            with torch.no_grad():
                # Forward pass, ignoring the returned loss.
                (logits, _) = self(x)

            # Take the logits at the last time-step (shape:
            # (1, vocab_size)).
            logits = logits[:, -1, :]

            # Convert to probabilities.
            probs = F.softmax(logits, dim=-1)

            # Top-k sampling.
            topk_probs, topk_indices = torch.topk(
                probs, k=top_k, dim=-1,
            )

            # Sample the next token.
            next_idx = torch.multinomial(
                topk_probs,
                num_samples=1,
                generator=sample_rng,
            )
            next_token = torch.gather(topk_indices, -1, next_idx)

            # If the next token is the stop token, we're done.
            next_token_item = next_token.item()
            if next_token_item == stop_token:
                break

            # Append token to current sequence.  Although we only
            # yield a singular decoded token below, we still need
            # to keep track of the entire sequence for subsequent
            # generation steps.
            x = torch.cat((x, next_token), dim=1)

            # Decode the newly-generated token.  Note that a single
            # token will often be decoded to multiple characters.
            new_text_fragment = enc.decode([next_token.item()])

            # If any of the characters in the decoded text
            # representation aren't printable, terminate
            # generation.
            if not all(c in self.printable for c in new_text_fragment):
                yield -1
                break

            yield new_text_fragment

            # Yield control back to the event loop before continuing
            # generation.  If we didn't do this, this client would
            # hog the thread's event loop, preventing other clients
            # associated with the loop from getting serviced.  (As
            # we're now running multiple threads in parallel, other
            # clients associated with event loops on other threads
            # would not be impacted.)
            await asyncio.sleep(0)

        elapsed = time.perf_counter() - start_time
        logging.debug(
            f"[generate_async_for] Generated {count} tokens in "
            f"{elapsed:.2f} seconds (~{count / elapsed:.2f} tok/s)"
        )
```

This routine was the last piece we needed to implement to satisfy
[our three goals](#our-goals) captured earlier, so, we're now ready to test
it out!

## Test Drive!

### Launching the HTTP Server

We can launch an instance of our multi-threaded HTTP web server with our new
`Gpt2App` HTTP application via the command line as follows:

```bash
% python -Xgil=0 -m parallelopedia.http.server  \
    --threads 40 --ip 0.0.0.0 --port 4444       \
    --app-classes parallelopedia.gpt2.Gpt2App   \
                  parallelopedia.http.server.PlaintextApp
```

This will start up a multi-threaded HTTP server listening on all interfaces
on port 4444, with 40 threads, and two HTTP applications: our `Gpt2App`,
which will service requests to the `/generate` endpoint, and a
`PlaintextApp` that just returns "Hello, World!" to any incoming request
received for the `/plaintext` endpoint.

### Visualizing Chunked-Encoding

Let's visualize the generation response in a way that shows us the raw
chunked-encoding, without doing any client-side reassembly.  We can achieve
that with `echo` and `netcat` (`nc`):

```bash
% echo -en \
 'GET /generate/The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42 ' \
 'HTTP/1.1\r\nConnection: close\r\nHost: dgx\r\n\r\n' | nc dgx 4444

```

That should yield the following raw output:

```
HTTP/1.1 200 OK
Server: Parallelopedia Web Server v1.0
Date: Fri, 07 Feb 2025 23:32:02 GMT
Accept-Ranges: bytes
Content-Type: text/plain
Access-Control-Allow-Origin: *
Connection: close
Transfer-Encoding: chunked
Access-Control-Expose-Headers: X-Max-Length, X-Top-K, X-Seed, X-Model-Name, X-Model-Device
X-Max-Length: 20
X-Top-K: 50
X-Seed: 42
X-Model-Name: gpt2
X-Model-Device: cuda:0

13
The quick brown fox
3
 is
2
 a
4
 sub
7
species
5
 that
B
 originated
3
 in
9
 southern
9
 Scotland
3
 as
2
 a
8
 variety
3
 of
4
 fox
1
.
5
 This
0

```

As you can see, we've enabled chunked-encoding by way of the
`Transfer-Encoding: chunked` header.  And the body of the response is
comprised of these *"chunks"*; specifically, each bit of decoded text is
preceded by its length, in bytes, then followed by `\r\n`, then followed by
the text itself.

The zero-length chunk at the end indicates completion of the transfer, and
as we requested `Connection: close` in our headers, our HTTP server closes
the connection once the generation has completed.

::: {.callout-note}

In HTTP/1.1, *"keep-alive"* is the default behavior---i.e., a server won't
close a connection unless the client specifically requests it.  This is
the opposite of HTTP/1.0 behavior, where the server will close a connection
by default unless a client furnishes a `Connection: keep-alive` header.

:::

### Verifying via Curl

If we switch over to `curl` and run the same generation request, we'll see
the *reassembled* text, and, provided we supply the `--no-buffer` argument,
`curl` will also display decoded text as soon as it receives it.

```bash
% curl --no-buffer --verbose \
    'http://dgx:4444/generate/The%20quick%20brown%20fox?' \
        'max_length=20&seed=42&device=cuda'
*   Trying 10.0.132.48:4444...
* Connected to dgx (10.0.132.48) port 4444 (#0)
> GET /generate/The%20quick%20brown%20fox?max_length=20&seed=42&device=cuda HTTP/1.1
> Host: dgx:4444
> User-Agent: curl/7.81.0
> Accept: */*
> 
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: Parallelopedia Web Server v1.0
< Date: Fri, 07 Feb 2025 23:05:34 GMT
< Accept-Ranges: bytes
< Content-Type: text/plain
< Access-Control-Allow-Origin: *
< Transfer-Encoding: chunked
< Access-Control-Expose-Headers: X-Max-Length, X-Top-K, X-Seed, X-Model-Name, X-Model-Device
< X-Max-Length: 20
< X-Top-K: 50
< X-Seed: 42
< X-Model-Name: gpt2
< X-Model-Device: cuda:2
< 
The quick brown fox is a subspecies that originated in southern Scotland as a variety of fox. This* Connection #0 to host dgx left intact
```

### Parallel Generation

#### Netcat Example

To test drive the parallel generation---at least in a very rudimentary
fashion---I'll enlist the help of `tmux` and the `:setw synchronize-panes`
command, which sends keystrokes to all active panes within a given `tmux`
session window.

I captured a 15 frames-per-second GIF of this in action, which you can view
below if you expand the callout.  It shows terminal sessions to six
machines, arranged vertically, all executing the same `echo ... | nc`
command depicted above.  It's a bit janky, but hopefully you can see that
the requests are being serviced in parallel by the HTTP server.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Parallel Netcat Generation GIF"}

![Parallel Netcat Generation](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-nc.gif)

:::

I extracted the last frame of the GIF, which you can see below, where you
can see that at least there was some variance between which GPUs were
selected:

![Parallel Netcat Generation - Last Frame](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-nc-last-frame-2.png)

#### Curl Example

I did the same thing with the panes arranged horizontally, using `curl`
instead and a `max_length=1000` and no seed, which helps in visualizing
the parallel generation, as you can clearly see different clients are
receiving completely different outputs.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Parallel Curl Generation GIF"}

![Parallel Curl Generation](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-curl-1000.gif)

:::

## Let's Load Test This Sucker!

These tests are pretty positive!  Looks like we can absolutely do PyTorch
model generation in parallel via multiple threads thanks to free-threaded
Python.

Let's ramp it up a notch and see what happens if we attempt to load test
the `/generate` endpoint from a different server.  The `leopard` server
you see in the next example is an Intel(R) Xeon(R) W-2275 CPU @ 3.30GHz
(14 cores, 28 threads) and is connected to the `dgx` box via a 10G Ethernet
link.

Using the fork of [`wrk`](https://github.com/tpn/wrk) I hacked together some
ten years ago or so whilst load testing PyParallel, let's kick off a run
with 14 threads for 30 seconds.  There will be one connection per thread,
and HTTP/1.1 will be used, so the implicit *keep-alive* behavior means that
as soon as one `/generate` request completes, another one is immediately
dispatched.

For posterity, the console command being used is as follows:

```bash
% time ./wrk -v --latency -c14 -t14 -d30s \
    'http://dgx:4444/generate/'
    'The%20quick%20brown%20fox?max_length=20&device=cuda&seed=42'
```

If you expand the callout below, the GIF you'll see depicts two terminal
windows.  The smaller foreground window on the left is `leopard`, the
session doing the `wrk` load test.  The background window that takes up the
rest of the screen is a GPU-enabled build of [`btop`](
https://github.com/aristocratos/btop), which is a beautiful console app for
displaying resource usage.

The 40 CPU cores can be seen in the top pane, and, as expected, about 14 of
them whirr into life as soon as the load test begins, which tracks, as there
are now 14 clients attempting to hammer our `/generate` end point.

Below that, you can see the four Tesla V100-DGXS-32GB GPUs also whirr into
action, handling the generation between them with relative ease.

On the bottom right, I've filtered the process tree to just display our
`python` HTTP server invocation.  I would love it if `btop` showed the
individual threads and their load, as it beautifully depicts Python
saturating cores in parallel now that there's no GIL impeding execution.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Parallel Generation Load Test GIF: No GIL"}

![Parallel Generation Load Test - No GIL](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14.gif)

:::

I've extracted the last frame, below, to allow closer inspection at the end
of the run, without the annoyance of the GIF looping.

![Parallel Generation Load Test No GIL - Last Frame](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-last-frame-optimized.png)

The console output from `wrk` is captured in the callout below.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Console Output of wrk: No GIL"}

```
{{< include wrk-leopard-01-no-gil.txt >}}
```

:::

A visualization of the latencies is presented below.

{{< embed wrk-analysis.ipynb#wrk-01-no-gil >}}

### Ablation Test: Re-enable the GIL

Let's see what happens if we re-enable the GIL.  We should see poor resource
utilization on the server side, as only one Python thread can run at a time,
and the statistics reported on the client side should be equally dismal.

Expand the callout below to view the GIF.  I have used another terminal
window to launch the server with the `-Xgil=1` argument, which re-enables
the GIL.  I then switch back over to `leopard` and perform the `wrk` load
test like before.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Parallel Generation Load Test GIF: GIL Enabled"}

![Parallel Generation Load Test - GIL Enabled](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-GIL-enabled.gif)

:::

As with before, I've extracted the last frame, below, to allow closer
inspection at the end of the run, without the annoyance of the GIF looping.

![Parallel Generation Load Test - GIL Enabled - Last Frame](
images/parallelopedia-multi-threaded-gpu-inference-gpt2-wrk-14-GIL-enabled-last-frame-optimized.png)

As we expected: re-enabling the GIL absolutely murders resource utilization.

The console output from `wrk` follows in the next callout.

::: {.callout-note collapse="true" appearance="simple" icon="false" title="Console Output of wrk: GIL Enabled"}

```
{{< include wrk-leopard-01-gil.txt >}}
```

:::

And a visualization of the latencies follows.  Note that there were 60
socket timeouts in this case, whereas no timeouts were encountered in the
prior run with the GIL disabled.

{{< embed wrk-analysis.ipynb#wrk-01-gil >}}

Viewing them side by side:

{{< embed wrk-analysis.ipynb#wrk-01-combined >}}

Remember to keep in mind that the No GIL vs GIL requests/sec was 70.80 vs
7.35, nearly a ten-fold increase:

{{< embed wrk-analysis.ipynb#wrk-01-rps-combined >}}

::: {.hidden}

## Launching the React Bootstrap UI

The React Bootstrap UI can be launched as follows:

```bash
% cd $PARALLELOPEDIA_UI # i.e. root of gh:tpn/parallelopedia-ui
% conda activate py313t # or whatever has recent nodejs/npm
% npm start run
```

::: {.callout-note}

Full disclaimer: I'm not a web developer.  I don't know JavaScript, React,
or Bootstrap, or anything else in the modern web stack.  So like any good
developer in 2025 when confronted with a task they have absolutely no
business doing... I palmed it off to AI---either ChatGPT, local LLMs via
[LM Studio](https://lmstudio.ai/), or, more recently, [Aider](
https://aider.chat).

TL;DR: the web interface code probably sucks.

:::

:::

<!-- vim:set ts=8 sw=2 sts=2 expandtab textwidth=78 -->
